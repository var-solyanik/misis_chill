{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cfccd41-bc39-4548-8ca0-cb622267c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sobel_as_fourth_channel(image):\n",
    "    \"\"\"\n",
    "    Adds the Sobel operator (edge detection) result as the fourth channel to an input image.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image (H, W, 3) read by cv2.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Image with the Sobel operator as the fourth channel (H, W, 4).\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "        print(image)\n",
    "        raise ValueError(\"Input image is None. Please provide a valid image.\")\n",
    "    \n",
    "    if image.shape[-1] != 3:\n",
    "        raise ValueError(\"Input image must have 3 channels (H, W, 3).\")\n",
    "\n",
    "    # Convert to grayscale for edge detection\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Sobel operator (X and Y gradients)\n",
    "    sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)  # Sobel in X direction\n",
    "    sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)  # Sobel in Y direction\n",
    "\n",
    "    # Compute the gradient magnitude\n",
    "    sobel_magnitude = cv2.magnitude(sobel_x, sobel_y)\n",
    "\n",
    "    # Normalize to 8-bit (0-255)\n",
    "    sobel_normalized = cv2.normalize(sobel_magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    sobel_normalized = sobel_normalized.astype(np.uint8)\n",
    "\n",
    "    # Add Sobel as the fourth channel\n",
    "    sobel_channel = np.expand_dims(sobel_normalized, axis=-1)  # Shape (H, W, 1)\n",
    "    image_with_sobel = np.concatenate((image, sobel_channel), axis=-1)  # Shape (H, W, 4)\n",
    "\n",
    "    return image_with_sobel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e89357de-6d2d-4006-983d-8c821181e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_photos_with_sobel(input_photos_dir, output_photos_dir):\n",
    "    try:\n",
    "        files = os.listdir(input_photos_dir)  # Lists all items in the directory\n",
    "        files = [f for f in files if os.path.isfile(os.path.join(input_photos_dir, f))]  # Filter files only with asbolute path with it\n",
    "        print(\"files\", files)\n",
    "        for input_photo_name in files:\n",
    "            if input_photo_name != '.DS_Store':\n",
    "                image = cv2.imread(os.path.join(input_photos_dir, input_photo_name))\n",
    "                # print(input_photo_name)\n",
    "                image_with_sobel = add_sobel_as_fourth_channel(image)\n",
    "                cv2.imwrite(os.path.join(output_photos_dir, input_photo_name.replace(\".jpg\", \".png\").replace('.png', '.tiff')), image_with_sobel)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory not found: {input_photos_dir}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# print(process_photos_with_sobel('/Users/z.gabdrakhmanov/Downloads/train_dataset/cv_open_dataset/open_img', '/Users/z.gabdrakhmanov/vscode/hack/misis2024s-23-01-gabdrakhmanov-z-i/nazar'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f5ec2f4a-ddc4-44cc-851d-803247dcb534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files ['.DS_Store', '1710275253_0.jpg', '1710277054_0.jpg', '1710322071_0.jpg', '1710370672_0.jpg', '1710381477_0.jpg', '1710390473_0.jpg', '1710399478_0.jpg', '1710430072_0.jpg', '1710458871_0.jpg', '1710584874_0.jpg', '1710604665_0.jpg', '1710809853_0.jpg', '1710811652_0.jpg', '1710813452_0.jpg', '1710815253_0.jpg', '1710835053_0.jpg', '1710847680_0.jpg', '1710917853_0.jpg', '1711009681_0.jpg', '1711090673_0.jpg', '1711119479_0.jpg', '1711177052_0.jpg', '1711178852_0.jpg', '1711178872_0.jpg', '1711180652_0.jpg', '1711182453_0.jpg', '1711184253_0.jpg', '1711186053_0.jpg', '1711187868_0.jpg', '1711256271_0.jpg', '1711268872_0.jpg', '1711268876_0.jpg', '1711330053_0.jpg', '1711393084_0.jpg', '1711412853_0.jpg', '1711421860_0.jpg', '1711430853_0.jpg', '1711448853_0.jpg', '1711450664_0.jpg', '1711456067_0.jpg', '1711459680_0.jpg', '1711510053_0.jpg', '1712555862_0.jpg', '1712564893_0.jpg', '1712577488_0.jpg', '1712741269_0.jpg', '1712788070_0.jpg', '1713302111_0.jpg', '1713350757_0.jpg', '1713767248_0.jpg', '1715041648_0.jpg', '1715074048_0.jpg', '1715112967_0.jpg', '1715158095_0.jpg', '1716017248_0.jpg', '1717022950_0.jpg', '1717058869_0.jpg', '1717059450_0.jpg', '1717060391_0.jpg', '1717061248_0.jpg', '1717063048_0.jpg', '1717068448_0.jpg', '1717390039_0.jpg', 'F1_1_1_1.ts_f_1000.jpg', 'F1_1_1_1.ts_f_500.jpg', 'F1_1_1_2.ts_f_1000.jpg', 'F1_1_1_2.ts_f_500.jpg', 'F1_1_2_1.ts_f_1000.jpg', 'F1_1_2_1.ts_f_500.jpg', 'F1_1_2_2.ts_f_1000.jpg', 'F1_1_2_2.ts_f_500.jpg', 'F1_1_3_1.ts_f_1000.jpg', 'F1_1_3_1.ts_f_500.jpg', 'F1_1_4_2.ts_f_1000.jpg', 'F1_1_4_2.ts_f_500.jpg', 'F1_1_5_1.ts_f_1000.jpg', 'F1_1_5_1.ts_f_500.jpg', 'F1_1_5_2.ts_f_1000.jpg', 'F1_1_5_2.ts_f_500.jpg', 'F1_2_2_1.ts_f_1000.jpg', 'F1_2_2_1.ts_f_500.jpg', 'F1_2_2_2.ts_f_1000.jpg', 'F1_2_2_2.ts_f_500.jpg', 'F1_2_3_1.ts_f_1000.jpg', 'F1_2_3_1.ts_f_500.jpg', 'F1_2_3_2.ts_f_1000.jpg', 'F1_2_3_2.ts_f_500.jpg', 'F1_2_4_1.ts_f_1000.jpg', 'F1_2_4_1.ts_f_500.jpg', 'F1_2_4_2.ts_f_1000.jpg', 'F1_2_4_2.ts_f_500.jpg', 'F1_2_5_1.ts_f_1000.jpg', 'F1_2_5_1.ts_f_500.jpg', 'F1_2_5_2.ts_f_1000.jpg', 'F1_2_5_2.ts_f_500.jpg', 'F2_1_1_1.ts_f_1000.jpg', 'F2_1_1_1.ts_f_500.jpg', 'F2_1_1_2.ts_f_1000.jpg', 'F2_1_1_2.ts_f_500.jpg', 'F2_1_2_2.ts_f_1000.jpg', 'F2_1_2_2.ts_f_500.jpg', 'F2_2_1_1.ts_f_1000.jpg', 'F2_2_1_1.ts_f_500.jpg', 'F2_2_1_2.ts_f_1000.jpg', 'F2_2_1_2.ts_f_500.jpg', 'F2_2_2_1.ts_f_1000.jpg', 'F2_2_2_1.ts_f_500.jpg', 'F2_2_2_2.ts_f_1000.jpg', 'F2_2_2_2.ts_f_500.jpg', 'F2_2_3_1.ts_f_1000.jpg', 'F2_2_3_1.ts_f_500.jpg', 'F2_2_3_2.ts_f_1000.jpg', 'F2_2_3_2.ts_f_500.jpg', 'F4_1_1_1.ts_f_1000.jpg', 'F4_1_1_1.ts_f_500.jpg', 'F4_1_1_2.ts_f_1000.jpg', 'F4_1_1_2.ts_f_500.jpg', 'F4_1_2_1.ts_f_1000.jpg', 'F4_1_2_1.ts_f_500.jpg', 'F4_1_2_2.ts_f_1000.jpg', 'F4_1_2_2.ts_f_500.jpg', 'F4_1_3_1.ts_f_1000.jpg', 'F4_1_3_1.ts_f_500.jpg', 'F4_1_3_2.ts_f_1000.jpg', 'F4_1_3_2.ts_f_500.jpg', 'F4_2_2_1.ts_f_1000.jpg', 'F4_2_2_1.ts_f_500.jpg', 'F4_2_2_2.ts_f_1000.jpg', 'F4_2_2_2.ts_f_500.jpg', 'F4_2_3_1.ts_f_1000.jpg', 'F4_2_3_1.ts_f_500.jpg', 'F4_2_3_2.ts_f_1000.jpg', 'F4_2_3_2.ts_f_500.jpg', 'F5_1_1_1.ts_f_1000.jpg', 'F5_1_1_1.ts_f_500.jpg', 'F5_1_1_2.ts_f_1000.jpg', 'F5_1_1_2.ts_f_500.jpg', 'F5_1_2_1.ts_f_1000.jpg', 'F5_1_2_1.ts_f_500.jpg', 'F5_1_2_2.ts_f_1000.jpg', 'F5_1_2_2.ts_f_500.jpg', 'F5_1_3_1.ts_f_1000.jpg', 'F5_1_3_1.ts_f_500.jpg', 'F5_1_3_2.ts_f_1000.jpg', 'F5_1_3_2.ts_f_500.jpg', 'F5_2_1_1.ts_f_1000.jpg', 'F5_2_1_1.ts_f_500.jpg', 'F5_2_1_2.ts_f_1000.jpg', 'F5_2_1_2.ts_f_500.jpg', 'F5_2_2_1.ts_f_1000.jpg', 'F5_2_2_1.ts_f_500.jpg', 'F5_2_2_2.ts_f_1000.jpg', 'F5_2_2_2.ts_f_500.jpg', 'F5_2_3_1.ts_f_1000.jpg', 'F5_2_3_1.ts_f_500.jpg', 'F5_2_3_2.ts_f_1000.jpg', 'F5_2_3_2.ts_f_500.jpg', 'F7_1_1_1.ts_f_1000.jpg', 'F7_1_1_1.ts_f_500.jpg', 'F7_1_1_2.ts_f_1000.jpg', 'F7_1_1_2.ts_f_500.jpg', 'F7_1_2_1.ts_f_1000.jpg', 'F7_1_2_1.ts_f_500.jpg', 'F7_1_2_2.ts_f_1000.jpg', 'F7_1_2_2.ts_f_500.jpg', 'F7_2_1_1.ts_f_1000.jpg', 'F7_2_1_1.ts_f_500.jpg', 'F7_2_1_2.ts_f_1000.jpg', 'F7_2_1_2.ts_f_500.jpg', 'gr1.jpg']\n"
     ]
    }
   ],
   "source": [
    "process_photos_with_sobel('/home/jovyan/nazar/123/123/misis_chill/train_dataset/cv_open_dataset/open_img', '/home/jovyan/nazar/123/123/misis_chill/train_dataset/cv_open_dataset/open_img_tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "b38bf9d8-bf5a-46b5-89a3-5d6aa2baa2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files ['100_1710813452_0.png', '101_1710835053_0.png', '103_1711186053_0.png', '105_1712788070_0.png', '106_1715112967_0.png', '107_1717060391_0.png', '10_1709255042_0.png', '11_1709260082_0.png', '12_1709260681_0.png', '13_1709261462_0.png', '14_1709262422_0.png', '15_1709265902_0.png', '16_1709265962_0.png', '17_1709276582_0.png', '18_1709310482_0.png', '1_1709104681_0.png', '20_1709344621_0.png', '21_1709388062_0.png', '22_1709859454_0.png', '23_1709859466_0.png', '256_1709104681_0.png', '257_1709109361_0.png', '258_1709116441_0.png', '259_1709186522_0.png', '25_1709888254_0.png', '260_1709187722_0.png', '261_1709191202_0.png', '262_1709232001_0.png', '263_1709253121_0.png', '264_1709254201_0.png', '265_1709255042_0.png', '266_1709260082_0.png', '267_1709260681_0.png', '268_1709261462_0.png', '269_1709262422_0.png', '26_1710079053_0.png', '270_1709265902_0.png', '271_1709265962_0.png', '272_1709276582_0.png', '273_1709310482_0.png', '274_1709339881_0.png', '275_1709344621_0.png', '276_1709388062_0.png', '277_1709859454_0.png', '278_1709859466_0.png', '27_1710253681_0.png', '280_1709888254_0.png', '282_1710253681_0.png', '283_1710260853_0.png', '284_1710262654_0.png', '285_1710266253_0.png', '287_1710809853_0.png', '288_1710813452_0.png', '28_1710260853_0.png', '291_1711186053_0.png', '293_1712788070_0.png', '294_1715112967_0.png', '295_1717060391_0.png', '296_dirty1.png', '29_1710262654_0.png', '2_1709109361_0.png', '30_1710266253_0.png', '31_1710458871_0.png', '32_1710809853_0.png', '33_1710813452_0.png', '34_1710835053_0.png', '36_1711186053_0.png', '38_1712788070_0.png', '39_1715112967_0.png', '3_1709116441_0.png', '401_1709104681_0.png', '402_1709109361_0.png', '403_1709116441_0.png', '404_1709186522_0.png', '405_1709187722_0.png', '406_1709191202_0.png', '407_1709232001_0.png', '408_1709253121_0.png', '409_1709254201_0.png', '40_1717060391_0.png', '410_1709255042_0.png', '411_1709260082_0.png', '412_1709260681_0.png', '413_1709261462_0.png', '414_1709262422_0.png', '415_1709265902_0.png', '416_1709265962_0.png', '417_1709276582_0.png', '418_1709310482_0.png', '419_1709339881_0.png', '41_dirty1.png', '420_1709344621_0.png', '421_1709388062_0.png', '422_1709859454_0.png', '423_1709859466_0.png', '425_1709888254_0.png', '426_1710079053_0.png', '427_1710253681_0.png', '428_1710260853_0.png', '429_1710262654_0.png', '430_1710266253_0.png', '431_1710458871_0.png', '432_1710809853_0.png', '433_1710813452_0.png', '434_1710835053_0.png', '436_1711186053_0.png', '438_1712788070_0.png', '439_1715112967_0.png', '440_1717060391_0.png', '441_dirty1.png', '4_1709186522_0.png', '5_1709187722_0.png', '68_1709104681_0.png', '69_1709109361_0.png', '6_1709191202_0.png', '70_1709116441_0.png', '71_1709186522_0.png', '72_1709187722_0.png', '73_1709191202_0.png', '74_1709232001_0.png', '75_1709253121_0.png', '76_1709254201_0.png', '77_1709255042_0.png', '78_1709260082_0.png', '79_1709260681_0.png', '7_1709232001_0.png', '80_1709261462_0.png', '81_1709262422_0.png', '82_1709265902_0.png', '83_1709265962_0.png', '84_1709276582_0.png', '85_1709310482_0.png', '86_1709339881_0.png', '87_1709344621_0.png', '88_1709388062_0.png', '89_1709859454_0.png', '90_1709859466_0.png', '91_1709886453_0.png', '92_1709888254_0.png', '93_1710079053_0.png', '94_1710253681_0.png', '95_1710260853_0.png', '96_1710262654_0.png', '97_1710266253_0.png', '98_1710458871_0.png', '99_1710809853_0.png', '9_1709254201_0.png']\n"
     ]
    }
   ],
   "source": [
    "process_photos_with_sobel('/home/jovyan/nazar/123/123/misis_chill/train_dataset/cv_synt_dataset/synt_img', '/home/jovyan/nazar/123/123/misis_chill/train_dataset/cv_synt_dataset/synt_img_tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac56f6e6ea79ce6e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "IMAGES_DIR = '/home/jovyan/nazar/123/123/misis_chill/new_photos/cv_open_dataset/open_img'  # Путь к вашему датасету с изображениями\n",
    "MASKS_DIR = '/home/jovyan/nazar/123/123/misis_chill/new_photos/cv_open_dataset/open_msk'  # Путь к вашему датасету с масками\n",
    "OUTPUT_DIR = './datasets/train_data'  # Путь к выходной директории\n",
    "TRAIN_SIZE = 0.8  # Процент обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "406044194f9535d5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(OUTPUT_DIR, 'images/train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'images/val'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'labels/train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'labels/val'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54fd5dbf-cf56-4d81-b088-ce747ab63f85",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mimage_files\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_files' is not defined"
     ]
    }
   ],
   "source": [
    "image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61ba29c30ff4a2dc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_files = [f for f in os.listdir(IMAGES_DIR) if f.endswith(('.jpg', '.png', '.tiff'))]\n",
    "mask_files = [f for f in os.listdir(MASKS_DIR) if f.endswith('.png')]\n",
    "if len(image_files) != len(mask_files):\n",
    "    print(len(image_files))\n",
    "    print(len(mask_files))\n",
    "    print(\"Количество изображений и масок не совпадает.\")\n",
    "    for mask in mask_files:\n",
    "        if mask.replace(\".png\", \".jpg\") not in image_files:\n",
    "            print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "835ee47d-c89d-409a-a645-266ab030cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_DIR2 = '/home/jovyan/nazar/123/123/misis_chill/new_photos/cv_synt_dataset/synt_img'  # Путь к вашему датасету с изображениями\n",
    "MASKS_DIR2 = '/home/jovyan/nazar/123/123/misis_chill/new_photos/cv_synt_dataset/synt_msk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b85c9706-c257-4469-832c-5998629422df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "397\n",
      "Количество изображений и масок не совпадает.\n",
      "17_1.png\n",
      "17_2.png\n",
      "29_1.png\n",
      "29_2.png\n",
      "100_1710813452_0.png\n",
      "101_1710835053_0.png\n",
      "103_1711186053_0.png\n",
      "105_1712788070_0.png\n",
      "106_1715112967_0.png\n",
      "107_1717060391_0.png\n",
      "10_1709255042_0.png\n",
      "11_1709260082_0.png\n",
      "12_1709260681_0.png\n",
      "13_1709261462_0.png\n",
      "14_1709262422_0.png\n",
      "15_1709265902_0.png\n",
      "16_1709265962_0.png\n",
      "17_1709276582_0.png\n",
      "18_1709310482_0.png\n",
      "1_1709104681_0.png\n",
      "20_1709344621_0.png\n",
      "21_1709388062_0.png\n",
      "22_1709859454_0.png\n",
      "23_1709859466_0.png\n",
      "256_1709104681_0.png\n",
      "257_1709109361_0.png\n",
      "258_1709116441_0.png\n",
      "259_1709186522_0.png\n",
      "25_1709888254_0.png\n",
      "260_1709187722_0.png\n",
      "261_1709191202_0.png\n",
      "262_1709232001_0.png\n",
      "263_1709253121_0.png\n",
      "264_1709254201_0.png\n",
      "265_1709255042_0.png\n",
      "266_1709260082_0.png\n",
      "267_1709260681_0.png\n",
      "268_1709261462_0.png\n",
      "269_1709262422_0.png\n",
      "26_1710079053_0.png\n",
      "270_1709265902_0.png\n",
      "271_1709265962_0.png\n",
      "272_1709276582_0.png\n",
      "273_1709310482_0.png\n",
      "274_1709339881_0.png\n",
      "275_1709344621_0.png\n",
      "276_1709388062_0.png\n",
      "277_1709859454_0.png\n",
      "278_1709859466_0.png\n",
      "27_1710253681_0.png\n",
      "280_1709888254_0.png\n",
      "282_1710253681_0.png\n",
      "283_1710260853_0.png\n",
      "284_1710262654_0.png\n",
      "285_1710266253_0.png\n",
      "287_1710809853_0.png\n",
      "288_1710813452_0.png\n",
      "28_1710260853_0.png\n",
      "291_1711186053_0.png\n",
      "293_1712788070_0.png\n",
      "294_1715112967_0.png\n",
      "295_1717060391_0.png\n",
      "296_dirty1.png\n",
      "29_1710262654_0.png\n",
      "2_1709109361_0.png\n",
      "30_1710266253_0.png\n",
      "31_1710458871_0.png\n",
      "32_1710809853_0.png\n",
      "33_1710813452_0.png\n",
      "34_1710835053_0.png\n",
      "36_1711186053_0.png\n",
      "38_1712788070_0.png\n",
      "39_1715112967_0.png\n",
      "3_1709116441_0.png\n",
      "401_1709104681_0.png\n",
      "402_1709109361_0.png\n",
      "403_1709116441_0.png\n",
      "404_1709186522_0.png\n",
      "405_1709187722_0.png\n",
      "406_1709191202_0.png\n",
      "407_1709232001_0.png\n",
      "408_1709253121_0.png\n",
      "409_1709254201_0.png\n",
      "40_1717060391_0.png\n",
      "410_1709255042_0.png\n",
      "411_1709260082_0.png\n",
      "412_1709260681_0.png\n",
      "413_1709261462_0.png\n",
      "414_1709262422_0.png\n",
      "415_1709265902_0.png\n",
      "416_1709265962_0.png\n",
      "417_1709276582_0.png\n",
      "418_1709310482_0.png\n",
      "419_1709339881_0.png\n",
      "41_dirty1.png\n",
      "420_1709344621_0.png\n",
      "421_1709388062_0.png\n",
      "422_1709859454_0.png\n",
      "423_1709859466_0.png\n",
      "425_1709888254_0.png\n",
      "426_1710079053_0.png\n",
      "427_1710253681_0.png\n",
      "428_1710260853_0.png\n",
      "429_1710262654_0.png\n",
      "430_1710266253_0.png\n",
      "431_1710458871_0.png\n",
      "432_1710809853_0.png\n",
      "433_1710813452_0.png\n",
      "434_1710835053_0.png\n",
      "436_1711186053_0.png\n",
      "438_1712788070_0.png\n",
      "439_1715112967_0.png\n",
      "440_1717060391_0.png\n",
      "441_dirty1.png\n",
      "4_1709186522_0.png\n",
      "5_1709187722_0.png\n",
      "68_1709104681_0.png\n",
      "69_1709109361_0.png\n",
      "6_1709191202_0.png\n",
      "70_1709116441_0.png\n",
      "71_1709186522_0.png\n",
      "72_1709187722_0.png\n",
      "73_1709191202_0.png\n",
      "74_1709232001_0.png\n",
      "75_1709253121_0.png\n",
      "76_1709254201_0.png\n",
      "77_1709255042_0.png\n",
      "78_1709260082_0.png\n",
      "79_1709260681_0.png\n",
      "7_1709232001_0.png\n",
      "80_1709261462_0.png\n",
      "81_1709262422_0.png\n",
      "82_1709265902_0.png\n",
      "83_1709265962_0.png\n",
      "84_1709276582_0.png\n",
      "85_1709310482_0.png\n",
      "86_1709339881_0.png\n",
      "87_1709344621_0.png\n",
      "88_1709388062_0.png\n",
      "89_1709859454_0.png\n",
      "90_1709859466_0.png\n",
      "91_1709886453_0.png\n",
      "92_1709888254_0.png\n",
      "93_1710079053_0.png\n",
      "94_1710253681_0.png\n",
      "95_1710260853_0.png\n",
      "96_1710262654_0.png\n",
      "97_1710266253_0.png\n",
      "98_1710458871_0.png\n",
      "99_1710809853_0.png\n",
      "9_1709254201_0.png\n"
     ]
    }
   ],
   "source": [
    "train_images += [\"synt\" + f for f in os.listdir(IMAGES_DIR2) if f.endswith(('.jpg', '.png', '.tiff'))]\n",
    "mask_files += [f for f in os.listdir(MASKS_DIR2) if f.endswith('.png')]\n",
    "if len(image_files) != len(mask_files):\n",
    "    print(len(image_files))\n",
    "    print(len(mask_files))\n",
    "    print(\"Количество изображений и масок не совпадает.\")\n",
    "    for mask in mask_files:\n",
    "        if mask.replace(\".png\", \".jpg\") not in image_files:\n",
    "            print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6e9828bb-10b2-4bcf-ad6a-43b0d7bb5882",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1709046321_0.jpg',\n",
       " '1709046334_0.jpg',\n",
       " '1709080522_0.jpg',\n",
       " '1709101222_0.jpg',\n",
       " '1709103022_0.jpg',\n",
       " '1709104681_0.jpg',\n",
       " '1709109361_0.jpg',\n",
       " '1709116441_0.jpg',\n",
       " '1709117582_0.jpg',\n",
       " '1709143887_0.jpg',\n",
       " '1709186522_0.jpg',\n",
       " '1709187722_0.jpg',\n",
       " '1709191202_0.jpg',\n",
       " '1709232001_0.jpg',\n",
       " '1709253121_0.jpg',\n",
       " '1709254201_0.jpg',\n",
       " '1709255042_0.jpg',\n",
       " '1709255402_0.jpg',\n",
       " '1709260082_0.jpg',\n",
       " '1709260681_0.jpg',\n",
       " '1709261462_0.jpg',\n",
       " '1709262422_0.jpg',\n",
       " '1709263081_0.jpg',\n",
       " '1709265902_0.jpg',\n",
       " '1709265962_0.jpg',\n",
       " '1709269442_0.jpg',\n",
       " '1709271842_0.jpg',\n",
       " '1709276582_0.jpg',\n",
       " '1709278599_0.jpg',\n",
       " '1709278661_0.jpg',\n",
       " '1709283396_0.jpg',\n",
       " '1709310482_0.jpg',\n",
       " '1709339881_0.jpg',\n",
       " '1709344621_0.jpg',\n",
       " '1709388062_0.jpg',\n",
       " '1709520200_0.jpg',\n",
       " '1709550918_0.jpg',\n",
       " '1709587678_0.jpg',\n",
       " '1709807287_0.jpg',\n",
       " '1709827069_0.jpg',\n",
       " '1709855872_0.jpg',\n",
       " '1709857666_0.jpg',\n",
       " '1709859454_0.jpg',\n",
       " '1709859466_0.jpg',\n",
       " '1709863065_0.jpg',\n",
       " '1709864860_0.jpg',\n",
       " '1709864863_0.jpg',\n",
       " '1709866664_0.jpg',\n",
       " '1709868465_0.jpg',\n",
       " '1709870253_0.jpg',\n",
       " '1709870265_0.jpg',\n",
       " '1709872064_0.jpg',\n",
       " '1709875661_0.jpg',\n",
       " '1709875666_0.jpg',\n",
       " '1709877464_0.jpg',\n",
       " '1709879265_0.jpg',\n",
       " '1709881065_0.jpg',\n",
       " '1709882867_0.jpg',\n",
       " '1709884664_0.jpg',\n",
       " '1709886453_0.jpg',\n",
       " '1709888254_0.jpg',\n",
       " '1709893685_0.jpg',\n",
       " '1710079053_0.jpg',\n",
       " '1710147478_0.jpg',\n",
       " '1710253681_0.jpg',\n",
       " '1710260853_0.jpg',\n",
       " '1710262654_0.jpg',\n",
       " '1710262674_0.jpg',\n",
       " '1710264453_0.jpg',\n",
       " '1710264471_0.jpg',\n",
       " '1710266253_0.jpg',\n",
       " '1710268053_0.jpg',\n",
       " '1710269853_0.jpg',\n",
       " '1710271653_0.jpg',\n",
       " '1710273453_0.jpg',\n",
       " '1710273470_0.jpg',\n",
       " '1710275253_0.jpg',\n",
       " '1710277054_0.jpg',\n",
       " '1710322071_0.jpg',\n",
       " '1710370672_0.jpg',\n",
       " '1710381477_0.jpg',\n",
       " '1710390473_0.jpg',\n",
       " '1710399478_0.jpg',\n",
       " '1710430072_0.jpg',\n",
       " '1710458871_0.jpg',\n",
       " '1710584874_0.jpg',\n",
       " '1710604665_0.jpg',\n",
       " '1710809853_0.jpg',\n",
       " '1710811652_0.jpg',\n",
       " '1710813452_0.jpg',\n",
       " '1710815253_0.jpg',\n",
       " '1710835053_0.jpg',\n",
       " '1710847680_0.jpg',\n",
       " '1710917853_0.jpg',\n",
       " '1711009681_0.jpg',\n",
       " '1711090673_0.jpg',\n",
       " '1711119479_0.jpg',\n",
       " '1711177052_0.jpg',\n",
       " '1711178852_0.jpg',\n",
       " '1711178872_0.jpg',\n",
       " '1711180652_0.jpg',\n",
       " '1711182453_0.jpg',\n",
       " '1711184253_0.jpg',\n",
       " '1711186053_0.jpg',\n",
       " '1711187868_0.jpg',\n",
       " '1711256271_0.jpg',\n",
       " '1711268872_0.jpg',\n",
       " '1711268876_0.jpg',\n",
       " '1711330053_0.jpg',\n",
       " '1711393084_0.jpg',\n",
       " '1711412853_0.jpg',\n",
       " '1711421860_0.jpg',\n",
       " '1711430853_0.jpg',\n",
       " '1711448853_0.jpg',\n",
       " '1711450664_0.jpg',\n",
       " '1711456067_0.jpg',\n",
       " '1711459680_0.jpg',\n",
       " '1711510053_0.jpg',\n",
       " '1712555862_0.jpg',\n",
       " '1712564893_0.jpg',\n",
       " '1712577488_0.jpg',\n",
       " '1712741269_0.jpg',\n",
       " '1712788070_0.jpg',\n",
       " '1713302111_0.jpg',\n",
       " '1713350757_0.jpg',\n",
       " '1713767248_0.jpg',\n",
       " '1715041648_0.jpg',\n",
       " '1715074048_0.jpg',\n",
       " '1715112967_0.jpg',\n",
       " '1715158095_0.jpg',\n",
       " '1716017248_0.jpg',\n",
       " '1717022950_0.jpg',\n",
       " '1717058869_0.jpg',\n",
       " '1717059450_0.jpg',\n",
       " '1717060391_0.jpg',\n",
       " '1717061248_0.jpg',\n",
       " '1717063048_0.jpg',\n",
       " '1717068448_0.jpg',\n",
       " '1717390039_0.jpg',\n",
       " '17_1.png',\n",
       " '17_2.png',\n",
       " '29_1.png',\n",
       " '29_2.png',\n",
       " 'F1_1_1_1.ts_f_1000.jpg',\n",
       " 'F1_1_1_1.ts_f_500.jpg',\n",
       " 'F1_1_1_2.ts_f_1000.jpg',\n",
       " 'F1_1_1_2.ts_f_500.jpg',\n",
       " 'F1_1_2_1.ts_f_1000.jpg',\n",
       " 'F1_1_2_1.ts_f_500.jpg',\n",
       " 'F1_1_2_2.ts_f_1000.jpg',\n",
       " 'F1_1_2_2.ts_f_500.jpg',\n",
       " 'F1_1_3_1.ts_f_1000.jpg',\n",
       " 'F1_1_3_1.ts_f_500.jpg',\n",
       " 'F1_1_4_2.ts_f_1000.jpg',\n",
       " 'F1_1_4_2.ts_f_500.jpg',\n",
       " 'F1_1_5_1.ts_f_1000.jpg',\n",
       " 'F1_1_5_1.ts_f_500.jpg',\n",
       " 'F1_1_5_2.ts_f_1000.jpg',\n",
       " 'F1_1_5_2.ts_f_500.jpg',\n",
       " 'F1_2_2_1.ts_f_1000.jpg',\n",
       " 'F1_2_2_1.ts_f_500.jpg',\n",
       " 'F1_2_2_2.ts_f_1000.jpg',\n",
       " 'F1_2_2_2.ts_f_500.jpg',\n",
       " 'F1_2_3_1.ts_f_1000.jpg',\n",
       " 'F1_2_3_1.ts_f_500.jpg',\n",
       " 'F1_2_3_2.ts_f_1000.jpg',\n",
       " 'F1_2_3_2.ts_f_500.jpg',\n",
       " 'F1_2_4_1.ts_f_1000.jpg',\n",
       " 'F1_2_4_1.ts_f_500.jpg',\n",
       " 'F1_2_4_2.ts_f_1000.jpg',\n",
       " 'F1_2_4_2.ts_f_500.jpg',\n",
       " 'F1_2_5_1.ts_f_1000.jpg',\n",
       " 'F1_2_5_1.ts_f_500.jpg',\n",
       " 'F1_2_5_2.ts_f_1000.jpg',\n",
       " 'F1_2_5_2.ts_f_500.jpg',\n",
       " 'F2_1_1_1.ts_f_1000.jpg',\n",
       " 'F2_1_1_1.ts_f_500.jpg',\n",
       " 'F2_1_1_2.ts_f_1000.jpg',\n",
       " 'F2_1_1_2.ts_f_500.jpg',\n",
       " 'F2_1_2_2.ts_f_1000.jpg',\n",
       " 'F2_1_2_2.ts_f_500.jpg',\n",
       " 'F2_2_1_1.ts_f_1000.jpg',\n",
       " 'F2_2_1_1.ts_f_500.jpg',\n",
       " 'F2_2_1_2.ts_f_1000.jpg',\n",
       " 'F2_2_1_2.ts_f_500.jpg',\n",
       " 'F2_2_2_1.ts_f_1000.jpg',\n",
       " 'F2_2_2_1.ts_f_500.jpg',\n",
       " 'F2_2_2_2.ts_f_1000.jpg',\n",
       " 'F2_2_2_2.ts_f_500.jpg',\n",
       " 'F2_2_3_1.ts_f_1000.jpg',\n",
       " 'F2_2_3_1.ts_f_500.jpg',\n",
       " 'F2_2_3_2.ts_f_1000.jpg',\n",
       " 'F2_2_3_2.ts_f_500.jpg',\n",
       " 'F4_1_1_1.ts_f_1000.jpg',\n",
       " 'F4_1_1_1.ts_f_500.jpg',\n",
       " 'F4_1_1_2.ts_f_1000.jpg',\n",
       " 'F4_1_1_2.ts_f_500.jpg',\n",
       " 'F4_1_2_1.ts_f_1000.jpg',\n",
       " 'F4_1_2_1.ts_f_500.jpg',\n",
       " 'F4_1_2_2.ts_f_1000.jpg',\n",
       " 'F4_1_2_2.ts_f_500.jpg',\n",
       " 'F4_1_3_1.ts_f_1000.jpg',\n",
       " 'F4_1_3_1.ts_f_500.jpg',\n",
       " 'F4_1_3_2.ts_f_1000.jpg',\n",
       " 'F4_1_3_2.ts_f_500.jpg',\n",
       " 'F4_2_2_1.ts_f_1000.jpg',\n",
       " 'F4_2_2_1.ts_f_500.jpg',\n",
       " 'F4_2_2_2.ts_f_1000.jpg',\n",
       " 'F4_2_2_2.ts_f_500.jpg',\n",
       " 'F4_2_3_1.ts_f_1000.jpg',\n",
       " 'F4_2_3_1.ts_f_500.jpg',\n",
       " 'F4_2_3_2.ts_f_1000.jpg',\n",
       " 'F4_2_3_2.ts_f_500.jpg',\n",
       " 'F5_1_1_1.ts_f_1000.jpg',\n",
       " 'F5_1_1_1.ts_f_500.jpg',\n",
       " 'F5_1_1_2.ts_f_1000.jpg',\n",
       " 'F5_1_1_2.ts_f_500.jpg',\n",
       " 'F5_1_2_1.ts_f_1000.jpg',\n",
       " 'F5_1_2_1.ts_f_500.jpg',\n",
       " 'F5_1_2_2.ts_f_1000.jpg',\n",
       " 'F5_1_2_2.ts_f_500.jpg',\n",
       " 'F5_1_3_1.ts_f_1000.jpg',\n",
       " 'F5_1_3_1.ts_f_500.jpg',\n",
       " 'F5_1_3_2.ts_f_1000.jpg',\n",
       " 'F5_1_3_2.ts_f_500.jpg',\n",
       " 'F5_2_1_1.ts_f_1000.jpg',\n",
       " 'F5_2_1_1.ts_f_500.jpg',\n",
       " 'F5_2_1_2.ts_f_1000.jpg',\n",
       " 'F5_2_1_2.ts_f_500.jpg',\n",
       " 'F5_2_2_1.ts_f_1000.jpg',\n",
       " 'F5_2_2_1.ts_f_500.jpg',\n",
       " 'F5_2_2_2.ts_f_1000.jpg',\n",
       " 'F5_2_2_2.ts_f_500.jpg',\n",
       " 'F5_2_3_1.ts_f_1000.jpg',\n",
       " 'F5_2_3_1.ts_f_500.jpg',\n",
       " 'F5_2_3_2.ts_f_1000.jpg',\n",
       " 'F5_2_3_2.ts_f_500.jpg',\n",
       " 'F7_1_1_1.ts_f_1000.jpg',\n",
       " 'F7_1_1_1.ts_f_500.jpg',\n",
       " 'F7_1_1_2.ts_f_1000.jpg',\n",
       " 'F7_1_1_2.ts_f_500.jpg',\n",
       " 'F7_1_2_1.ts_f_1000.jpg',\n",
       " 'F7_1_2_1.ts_f_500.jpg',\n",
       " 'F7_1_2_2.ts_f_1000.jpg',\n",
       " 'F7_1_2_2.ts_f_500.jpg',\n",
       " 'F7_2_1_1.ts_f_1000.jpg',\n",
       " 'F7_2_1_1.ts_f_500.jpg',\n",
       " 'F7_2_1_2.ts_f_1000.jpg',\n",
       " 'F7_2_1_2.ts_f_500.jpg',\n",
       " 'gr1.jpg']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1e867c40-f98c-4e4a-bfee-50e643b15433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1710275253_0.png',\n",
       " '1710277054_0.png',\n",
       " '1710322071_0.png',\n",
       " '1710370672_0.png',\n",
       " '1710381477_0.png',\n",
       " '1710390473_0.png',\n",
       " '1710399478_0.png',\n",
       " '1710430072_0.png',\n",
       " '1710458871_0.png',\n",
       " '1710584874_0.png',\n",
       " '1710604665_0.png',\n",
       " '1710809853_0.png',\n",
       " '1710811652_0.png',\n",
       " '1710813452_0.png',\n",
       " '1710815253_0.png',\n",
       " '1710835053_0.png',\n",
       " '1710847680_0.png',\n",
       " '1710917853_0.png',\n",
       " '1711009681_0.png',\n",
       " '1711090673_0.png',\n",
       " '1711119479_0.png',\n",
       " '1711177052_0.png',\n",
       " '1711178852_0.png',\n",
       " '1711178872_0.png',\n",
       " '1711180652_0.png',\n",
       " '1711182453_0.png',\n",
       " '1711184253_0.png',\n",
       " '1711186053_0.png',\n",
       " '1711187868_0.png',\n",
       " '1711256271_0.png',\n",
       " '1711268872_0.png',\n",
       " '1711268876_0.png',\n",
       " '1711330053_0.png',\n",
       " '1711393084_0.png',\n",
       " '1711412853_0.png',\n",
       " '1711421860_0.png',\n",
       " '1711430853_0.png',\n",
       " '1711448853_0.png',\n",
       " '1711450664_0.png',\n",
       " '1711456067_0.png',\n",
       " '1711459680_0.png',\n",
       " '1711510053_0.png',\n",
       " '1712555862_0.png',\n",
       " '1712564893_0.png',\n",
       " '1712577488_0.png',\n",
       " '1712741269_0.png',\n",
       " '1712788070_0.png',\n",
       " '1713302111_0.png',\n",
       " '1713350757_0.png',\n",
       " '1713767248_0.png',\n",
       " '1715041648_0.png',\n",
       " '1715074048_0.png',\n",
       " '1715112967_0.png',\n",
       " '1715158095_0.png',\n",
       " '1716017248_0.png',\n",
       " '1717022950_0.png',\n",
       " '1717058869_0.png',\n",
       " '1717059450_0.png',\n",
       " '1717060391_0.png',\n",
       " '1717061248_0.png',\n",
       " '1717063048_0.png',\n",
       " '1717068448_0.png',\n",
       " '1717390039_0.png',\n",
       " 'F1_1_1_1.ts_f_1000.png',\n",
       " 'F1_1_1_1.ts_f_500.png',\n",
       " 'F1_1_1_2.ts_f_1000.png',\n",
       " 'F1_1_1_2.ts_f_500.png',\n",
       " 'F1_1_2_1.ts_f_1000.png',\n",
       " 'F1_1_2_1.ts_f_500.png',\n",
       " 'F1_1_2_2.ts_f_1000.png',\n",
       " 'F1_1_2_2.ts_f_500.png',\n",
       " 'F1_1_3_1.ts_f_1000.png',\n",
       " 'F1_1_3_1.ts_f_500.png',\n",
       " 'F1_1_4_2.ts_f_1000.png',\n",
       " 'F1_1_4_2.ts_f_500.png',\n",
       " 'F1_1_5_1.ts_f_1000.png',\n",
       " 'F1_1_5_1.ts_f_500.png',\n",
       " 'F1_1_5_2.ts_f_1000.png',\n",
       " 'F1_1_5_2.ts_f_500.png',\n",
       " 'F1_2_2_1.ts_f_1000.png',\n",
       " 'F1_2_2_1.ts_f_500.png',\n",
       " 'F1_2_2_2.ts_f_1000.png',\n",
       " 'F1_2_2_2.ts_f_500.png',\n",
       " 'F1_2_3_1.ts_f_1000.png',\n",
       " 'F1_2_3_1.ts_f_500.png',\n",
       " 'F1_2_3_2.ts_f_1000.png',\n",
       " 'F1_2_3_2.ts_f_500.png',\n",
       " 'F1_2_4_1.ts_f_1000.png',\n",
       " 'F1_2_4_1.ts_f_500.png',\n",
       " 'F1_2_4_2.ts_f_1000.png',\n",
       " 'F1_2_4_2.ts_f_500.png',\n",
       " 'F1_2_5_1.ts_f_1000.png',\n",
       " 'F1_2_5_1.ts_f_500.png',\n",
       " 'F1_2_5_2.ts_f_1000.png',\n",
       " 'F1_2_5_2.ts_f_500.png',\n",
       " 'F2_1_1_1.ts_f_1000.png',\n",
       " 'F2_1_1_1.ts_f_500.png',\n",
       " 'F2_1_1_2.ts_f_1000.png',\n",
       " 'F2_1_1_2.ts_f_500.png',\n",
       " 'F2_1_2_2.ts_f_1000.png',\n",
       " 'F2_1_2_2.ts_f_500.png',\n",
       " 'F2_2_1_1.ts_f_1000.png',\n",
       " 'F2_2_1_1.ts_f_500.png',\n",
       " 'F2_2_1_2.ts_f_1000.png',\n",
       " 'F2_2_1_2.ts_f_500.png',\n",
       " 'F2_2_2_1.ts_f_1000.png',\n",
       " 'F2_2_2_1.ts_f_500.png',\n",
       " 'F2_2_2_2.ts_f_1000.png',\n",
       " 'F2_2_2_2.ts_f_500.png',\n",
       " 'F2_2_3_1.ts_f_1000.png',\n",
       " 'F2_2_3_1.ts_f_500.png',\n",
       " 'F2_2_3_2.ts_f_1000.png',\n",
       " 'F2_2_3_2.ts_f_500.png',\n",
       " 'F4_1_1_1.ts_f_1000.png',\n",
       " 'F4_1_1_1.ts_f_500.png',\n",
       " 'F4_1_1_2.ts_f_1000.png',\n",
       " 'F4_1_1_2.ts_f_500.png',\n",
       " 'F4_1_2_1.ts_f_1000.png',\n",
       " 'F4_1_2_1.ts_f_500.png',\n",
       " 'F4_1_2_2.ts_f_1000.png',\n",
       " 'F4_1_2_2.ts_f_500.png',\n",
       " 'F4_1_3_1.ts_f_1000.png',\n",
       " 'F4_1_3_1.ts_f_500.png',\n",
       " 'F4_1_3_2.ts_f_1000.png',\n",
       " 'F4_1_3_2.ts_f_500.png',\n",
       " 'F4_2_2_1.ts_f_1000.png',\n",
       " 'F4_2_2_1.ts_f_500.png',\n",
       " 'F4_2_2_2.ts_f_1000.png',\n",
       " 'F4_2_2_2.ts_f_500.png',\n",
       " 'F4_2_3_1.ts_f_1000.png',\n",
       " 'F4_2_3_1.ts_f_500.png',\n",
       " 'F4_2_3_2.ts_f_1000.png',\n",
       " 'F4_2_3_2.ts_f_500.png',\n",
       " 'F5_1_1_1.ts_f_1000.png',\n",
       " 'F5_1_1_1.ts_f_500.png',\n",
       " 'F5_1_1_2.ts_f_1000.png',\n",
       " 'F5_1_1_2.ts_f_500.png',\n",
       " 'F5_1_2_1.ts_f_1000.png',\n",
       " 'F5_1_2_1.ts_f_500.png',\n",
       " 'F5_1_2_2.ts_f_1000.png',\n",
       " 'F5_1_2_2.ts_f_500.png',\n",
       " 'F5_1_3_1.ts_f_1000.png',\n",
       " 'F5_1_3_1.ts_f_500.png',\n",
       " 'F5_1_3_2.ts_f_1000.png',\n",
       " 'F5_1_3_2.ts_f_500.png',\n",
       " 'F5_2_1_1.ts_f_1000.png',\n",
       " 'F5_2_1_1.ts_f_500.png',\n",
       " 'F5_2_1_2.ts_f_1000.png',\n",
       " 'F5_2_1_2.ts_f_500.png',\n",
       " 'F5_2_2_1.ts_f_1000.png',\n",
       " 'F5_2_2_1.ts_f_500.png',\n",
       " 'F5_2_2_2.ts_f_1000.png',\n",
       " 'F5_2_2_2.ts_f_500.png',\n",
       " 'F5_2_3_1.ts_f_1000.png',\n",
       " 'F5_2_3_1.ts_f_500.png',\n",
       " 'F5_2_3_2.ts_f_1000.png',\n",
       " 'F5_2_3_2.ts_f_500.png',\n",
       " 'F7_1_1_1.ts_f_1000.png',\n",
       " 'F7_1_1_1.ts_f_500.png',\n",
       " 'F7_1_1_2.ts_f_1000.png',\n",
       " 'F7_1_1_2.ts_f_500.png',\n",
       " 'F7_1_2_1.ts_f_1000.png',\n",
       " 'F7_1_2_1.ts_f_500.png',\n",
       " 'F7_1_2_2.ts_f_1000.png',\n",
       " 'F7_1_2_2.ts_f_500.png',\n",
       " 'F7_2_1_1.ts_f_1000.png',\n",
       " 'F7_2_1_1.ts_f_500.png',\n",
       " 'F7_2_1_2.ts_f_1000.png',\n",
       " 'F7_2_1_2.ts_f_500.png',\n",
       " 'gr1.png',\n",
       " '100_1710813452_0.png',\n",
       " '101_1710835053_0.png',\n",
       " '103_1711186053_0.png',\n",
       " '105_1712788070_0.png',\n",
       " '106_1715112967_0.png',\n",
       " '107_1717060391_0.png',\n",
       " '10_1709255042_0.png',\n",
       " '11_1709260082_0.png',\n",
       " '12_1709260681_0.png',\n",
       " '13_1709261462_0.png',\n",
       " '14_1709262422_0.png',\n",
       " '15_1709265902_0.png',\n",
       " '16_1709265962_0.png',\n",
       " '17_1709276582_0.png',\n",
       " '18_1709310482_0.png',\n",
       " '1_1709104681_0.png',\n",
       " '20_1709344621_0.png',\n",
       " '21_1709388062_0.png',\n",
       " '22_1709859454_0.png',\n",
       " '23_1709859466_0.png',\n",
       " '256_1709104681_0.png',\n",
       " '257_1709109361_0.png',\n",
       " '258_1709116441_0.png',\n",
       " '259_1709186522_0.png',\n",
       " '25_1709888254_0.png',\n",
       " '260_1709187722_0.png',\n",
       " '261_1709191202_0.png',\n",
       " '262_1709232001_0.png',\n",
       " '263_1709253121_0.png',\n",
       " '264_1709254201_0.png',\n",
       " '265_1709255042_0.png',\n",
       " '266_1709260082_0.png',\n",
       " '267_1709260681_0.png',\n",
       " '268_1709261462_0.png',\n",
       " '269_1709262422_0.png',\n",
       " '26_1710079053_0.png',\n",
       " '270_1709265902_0.png',\n",
       " '271_1709265962_0.png',\n",
       " '272_1709276582_0.png',\n",
       " '273_1709310482_0.png',\n",
       " '274_1709339881_0.png',\n",
       " '275_1709344621_0.png',\n",
       " '276_1709388062_0.png',\n",
       " '277_1709859454_0.png',\n",
       " '278_1709859466_0.png',\n",
       " '27_1710253681_0.png',\n",
       " '280_1709888254_0.png',\n",
       " '282_1710253681_0.png',\n",
       " '283_1710260853_0.png',\n",
       " '284_1710262654_0.png',\n",
       " '285_1710266253_0.png',\n",
       " '287_1710809853_0.png',\n",
       " '288_1710813452_0.png',\n",
       " '28_1710260853_0.png',\n",
       " '291_1711186053_0.png',\n",
       " '293_1712788070_0.png',\n",
       " '294_1715112967_0.png',\n",
       " '295_1717060391_0.png',\n",
       " '296_dirty1.png',\n",
       " '29_1710262654_0.png',\n",
       " '2_1709109361_0.png',\n",
       " '30_1710266253_0.png',\n",
       " '31_1710458871_0.png',\n",
       " '32_1710809853_0.png',\n",
       " '33_1710813452_0.png',\n",
       " '34_1710835053_0.png',\n",
       " '36_1711186053_0.png',\n",
       " '38_1712788070_0.png',\n",
       " '39_1715112967_0.png',\n",
       " '3_1709116441_0.png',\n",
       " '401_1709104681_0.png',\n",
       " '402_1709109361_0.png',\n",
       " '403_1709116441_0.png',\n",
       " '404_1709186522_0.png',\n",
       " '405_1709187722_0.png',\n",
       " '406_1709191202_0.png',\n",
       " '407_1709232001_0.png',\n",
       " '408_1709253121_0.png',\n",
       " '409_1709254201_0.png',\n",
       " '40_1717060391_0.png',\n",
       " '410_1709255042_0.png',\n",
       " '411_1709260082_0.png',\n",
       " '412_1709260681_0.png',\n",
       " '413_1709261462_0.png',\n",
       " '414_1709262422_0.png',\n",
       " '415_1709265902_0.png',\n",
       " '416_1709265962_0.png',\n",
       " '417_1709276582_0.png',\n",
       " '418_1709310482_0.png',\n",
       " '419_1709339881_0.png',\n",
       " '41_dirty1.png',\n",
       " '420_1709344621_0.png',\n",
       " '421_1709388062_0.png',\n",
       " '422_1709859454_0.png',\n",
       " '423_1709859466_0.png',\n",
       " '425_1709888254_0.png',\n",
       " '426_1710079053_0.png',\n",
       " '427_1710253681_0.png',\n",
       " '428_1710260853_0.png',\n",
       " '429_1710262654_0.png',\n",
       " '430_1710266253_0.png',\n",
       " '431_1710458871_0.png',\n",
       " '432_1710809853_0.png',\n",
       " '433_1710813452_0.png',\n",
       " '434_1710835053_0.png',\n",
       " '436_1711186053_0.png',\n",
       " '438_1712788070_0.png',\n",
       " '439_1715112967_0.png',\n",
       " '440_1717060391_0.png',\n",
       " '441_dirty1.png',\n",
       " '4_1709186522_0.png',\n",
       " '5_1709187722_0.png',\n",
       " '68_1709104681_0.png',\n",
       " '69_1709109361_0.png',\n",
       " '6_1709191202_0.png',\n",
       " '70_1709116441_0.png',\n",
       " '71_1709186522_0.png',\n",
       " '72_1709187722_0.png',\n",
       " '73_1709191202_0.png',\n",
       " '74_1709232001_0.png',\n",
       " '75_1709253121_0.png',\n",
       " '76_1709254201_0.png',\n",
       " '77_1709255042_0.png',\n",
       " '78_1709260082_0.png',\n",
       " '79_1709260681_0.png',\n",
       " '7_1709232001_0.png',\n",
       " '80_1709261462_0.png',\n",
       " '81_1709262422_0.png',\n",
       " '82_1709265902_0.png',\n",
       " '83_1709265962_0.png',\n",
       " '84_1709276582_0.png',\n",
       " '85_1709310482_0.png',\n",
       " '86_1709339881_0.png',\n",
       " '87_1709344621_0.png',\n",
       " '88_1709388062_0.png',\n",
       " '89_1709859454_0.png',\n",
       " '90_1709859466_0.png',\n",
       " '91_1709886453_0.png',\n",
       " '92_1709888254_0.png',\n",
       " '93_1710079053_0.png',\n",
       " '94_1710253681_0.png',\n",
       " '95_1710260853_0.png',\n",
       " '96_1710262654_0.png',\n",
       " '97_1710266253_0.png',\n",
       " '98_1710458871_0.png',\n",
       " '99_1710809853_0.png',\n",
       " '9_1709254201_0.png']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fd72f0f-c6d4-4fc6-b015-07601a47a9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len((mask_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d00ab007653eda31",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_images, val_images = train_test_split(image_files, train_size=TRAIN_SIZE, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "93940f22-5132-4897-ac3e-3bbd2af60618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1717058869_0.jpg',\n",
       " 'F5_2_1_1.ts_f_1000.jpg',\n",
       " 'F7_1_1_1.ts_f_500.jpg',\n",
       " '1712564893_0.jpg',\n",
       " '1717063048_0.jpg',\n",
       " '1710262654_0.jpg',\n",
       " 'F5_1_2_1.ts_f_1000.jpg',\n",
       " 'F1_1_4_2.ts_f_500.jpg',\n",
       " 'F1_2_4_1.ts_f_500.jpg',\n",
       " 'F5_2_2_1.ts_f_1000.jpg',\n",
       " 'F5_2_3_1.ts_f_1000.jpg',\n",
       " 'F1_1_5_1.ts_f_500.jpg',\n",
       " '1711510053_0.jpg',\n",
       " '1710458871_0.jpg',\n",
       " '1713767248_0.jpg',\n",
       " 'F1_2_4_1.ts_f_1000.jpg',\n",
       " 'F7_2_1_1.ts_f_1000.jpg',\n",
       " '1711393084_0.jpg',\n",
       " 'F1_2_5_1.ts_f_500.jpg',\n",
       " 'F5_2_3_1.ts_f_500.jpg',\n",
       " '1710271653_0.jpg',\n",
       " 'F1_1_5_2.ts_f_500.jpg',\n",
       " '1711456067_0.jpg',\n",
       " '1710273470_0.jpg',\n",
       " 'F7_1_1_1.ts_f_1000.jpg',\n",
       " '1710399478_0.jpg',\n",
       " '1709278661_0.jpg',\n",
       " '1715041648_0.jpg',\n",
       " '1710370672_0.jpg',\n",
       " '1710604665_0.jpg',\n",
       " '1709104681_0.jpg',\n",
       " '1709881065_0.jpg',\n",
       " 'F2_2_2_1.ts_f_500.jpg',\n",
       " 'F5_2_2_2.ts_f_1000.jpg',\n",
       " 'F1_2_4_2.ts_f_500.jpg',\n",
       " '1710260853_0.jpg',\n",
       " '17_2.png',\n",
       " '1713350757_0.jpg',\n",
       " 'F5_1_1_1.ts_f_1000.jpg',\n",
       " 'F1_1_2_1.ts_f_500.jpg',\n",
       " '1709310482_0.jpg',\n",
       " '1709191202_0.jpg',\n",
       " '1709520200_0.jpg',\n",
       " '1709278599_0.jpg',\n",
       " '1709859454_0.jpg',\n",
       " 'F5_1_2_1.ts_f_500.jpg',\n",
       " '1709263081_0.jpg',\n",
       " '1710917853_0.jpg',\n",
       " 'F4_2_3_2.ts_f_500.jpg',\n",
       " '1711182453_0.jpg',\n",
       " '1709872064_0.jpg',\n",
       " '1711090673_0.jpg',\n",
       " 'F1_1_1_1.ts_f_1000.jpg',\n",
       " 'F4_1_2_2.ts_f_1000.jpg',\n",
       " '1710275253_0.jpg',\n",
       " '1709857666_0.jpg',\n",
       " '1711448853_0.jpg',\n",
       " 'F1_1_3_1.ts_f_500.jpg',\n",
       " '1710322071_0.jpg',\n",
       " 'F1_1_2_1.ts_f_1000.jpg',\n",
       " '1709271842_0.jpg',\n",
       " 'F7_1_2_1.ts_f_1000.jpg',\n",
       " 'F1_2_3_1.ts_f_500.jpg',\n",
       " 'F7_1_2_2.ts_f_1000.jpg',\n",
       " '1717068448_0.jpg',\n",
       " '1709046321_0.jpg',\n",
       " '1709080522_0.jpg',\n",
       " '1710277054_0.jpg',\n",
       " '1709864863_0.jpg',\n",
       " '1711180652_0.jpg',\n",
       " '1715074048_0.jpg',\n",
       " 'F1_1_2_2.ts_f_500.jpg',\n",
       " 'F7_2_1_2.ts_f_500.jpg',\n",
       " '1710815253_0.jpg',\n",
       " '1710584874_0.jpg',\n",
       " 'F2_1_1_1.ts_f_1000.jpg',\n",
       " 'F4_1_1_2.ts_f_1000.jpg',\n",
       " '1711178852_0.jpg',\n",
       " '1709550918_0.jpg',\n",
       " '1709893685_0.jpg',\n",
       " 'F1_2_2_2.ts_f_500.jpg',\n",
       " '17_1.png',\n",
       " '1709187722_0.jpg',\n",
       " 'F7_1_2_2.ts_f_500.jpg',\n",
       " 'F4_1_2_1.ts_f_500.jpg',\n",
       " '1709276582_0.jpg',\n",
       " 'F1_1_5_1.ts_f_1000.jpg',\n",
       " '1709103022_0.jpg',\n",
       " '1712788070_0.jpg',\n",
       " '1709339881_0.jpg',\n",
       " 'F1_1_1_1.ts_f_500.jpg',\n",
       " 'F2_1_1_2.ts_f_500.jpg',\n",
       " '1717390039_0.jpg',\n",
       " '1710079053_0.jpg',\n",
       " '1717061248_0.jpg',\n",
       " '1715112967_0.jpg',\n",
       " 'F5_2_1_1.ts_f_500.jpg',\n",
       " 'F4_2_2_1.ts_f_1000.jpg',\n",
       " '1710266253_0.jpg',\n",
       " 'F2_2_3_2.ts_f_500.jpg',\n",
       " '1710253681_0.jpg',\n",
       " '1709863065_0.jpg',\n",
       " 'F1_2_3_2.ts_f_1000.jpg',\n",
       " '1709855872_0.jpg',\n",
       " '1713302111_0.jpg',\n",
       " 'F1_1_4_2.ts_f_1000.jpg',\n",
       " '1709265902_0.jpg',\n",
       " 'F4_2_3_2.ts_f_1000.jpg',\n",
       " 'F2_2_1_2.ts_f_1000.jpg',\n",
       " '1710390473_0.jpg',\n",
       " '1709827069_0.jpg',\n",
       " 'F4_2_3_1.ts_f_1000.jpg',\n",
       " '1709866664_0.jpg',\n",
       " '1711009681_0.jpg',\n",
       " 'F2_2_3_1.ts_f_500.jpg',\n",
       " 'F1_2_2_2.ts_f_1000.jpg',\n",
       " '1709859466_0.jpg',\n",
       " 'F1_1_1_2.ts_f_1000.jpg',\n",
       " 'F2_2_1_1.ts_f_500.jpg',\n",
       " '1709101222_0.jpg',\n",
       " '1711256271_0.jpg',\n",
       " '1709875666_0.jpg',\n",
       " '1717059450_0.jpg',\n",
       " 'F5_2_1_2.ts_f_1000.jpg',\n",
       " 'F4_1_1_1.ts_f_1000.jpg',\n",
       " 'F5_2_2_2.ts_f_500.jpg',\n",
       " '1709870253_0.jpg',\n",
       " 'F1_2_3_1.ts_f_1000.jpg',\n",
       " '1710381477_0.jpg',\n",
       " '1709388062_0.jpg',\n",
       " 'F4_2_2_1.ts_f_500.jpg',\n",
       " '1709116441_0.jpg',\n",
       " 'F1_2_5_1.ts_f_1000.jpg',\n",
       " '1711412853_0.jpg',\n",
       " '1710835053_0.jpg',\n",
       " '1710430072_0.jpg',\n",
       " 'F5_1_3_2.ts_f_1000.jpg',\n",
       " 'F5_2_1_2.ts_f_500.jpg',\n",
       " '1710813452_0.jpg',\n",
       " '1709117582_0.jpg',\n",
       " '1709232001_0.jpg',\n",
       " '1709886453_0.jpg',\n",
       " 'F5_1_1_2.ts_f_1000.jpg',\n",
       " '1717022950_0.jpg',\n",
       " '1709255402_0.jpg',\n",
       " 'F1_2_3_2.ts_f_500.jpg',\n",
       " '1710269853_0.jpg',\n",
       " 'F5_1_2_2.ts_f_500.jpg',\n",
       " '1717060391_0.jpg',\n",
       " 'F4_1_3_2.ts_f_500.jpg',\n",
       " 'F5_2_2_1.ts_f_500.jpg',\n",
       " '1710147478_0.jpg',\n",
       " '1709877464_0.jpg',\n",
       " '1711268876_0.jpg',\n",
       " '1709870265_0.jpg',\n",
       " 'F7_1_1_2.ts_f_1000.jpg',\n",
       " 'F1_2_5_2.ts_f_500.jpg',\n",
       " 'F4_2_2_2.ts_f_500.jpg',\n",
       " 'F2_2_3_1.ts_f_1000.jpg',\n",
       " 'F7_2_1_1.ts_f_500.jpg',\n",
       " 'F4_2_2_2.ts_f_1000.jpg',\n",
       " 'F5_1_3_1.ts_f_1000.jpg',\n",
       " 'F1_2_4_2.ts_f_1000.jpg',\n",
       " '1709884664_0.jpg',\n",
       " '1709868465_0.jpg',\n",
       " '1710811652_0.jpg',\n",
       " '1709262422_0.jpg',\n",
       " '1709882867_0.jpg',\n",
       " 'F4_1_3_2.ts_f_1000.jpg',\n",
       " 'F1_2_2_1.ts_f_500.jpg',\n",
       " 'F7_1_2_1.ts_f_500.jpg',\n",
       " 'F2_2_2_2.ts_f_1000.jpg',\n",
       " 'F2_2_3_2.ts_f_1000.jpg',\n",
       " '1715158095_0.jpg',\n",
       " '1709587678_0.jpg',\n",
       " 'F1_1_5_2.ts_f_1000.jpg',\n",
       " 'F5_2_3_2.ts_f_1000.jpg',\n",
       " '1709046334_0.jpg',\n",
       " '1709875661_0.jpg',\n",
       " 'F1_1_2_2.ts_f_1000.jpg',\n",
       " '1716017248_0.jpg',\n",
       " 'F1_1_3_1.ts_f_1000.jpg',\n",
       " '1711186053_0.jpg',\n",
       " '1711178872_0.jpg',\n",
       " '1711459680_0.jpg',\n",
       " '1710809853_0.jpg',\n",
       " 'F4_1_3_1.ts_f_500.jpg',\n",
       " '1710273453_0.jpg',\n",
       " 'F5_1_1_1.ts_f_500.jpg',\n",
       " 'F4_2_3_1.ts_f_500.jpg',\n",
       " '1712741269_0.jpg',\n",
       " 'gr1.jpg',\n",
       " '1709261462_0.jpg',\n",
       " 'F2_2_2_2.ts_f_500.jpg',\n",
       " '1710268053_0.jpg',\n",
       " '1711268872_0.jpg',\n",
       " '1709253121_0.jpg',\n",
       " '1710847680_0.jpg',\n",
       " 'F2_1_2_2.ts_f_1000.jpg',\n",
       " '1711184253_0.jpg']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "196bfff8-12f5-4a31-b3f5-56bd458d2788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['29_2.png',\n",
       " '1709109361_0.jpg',\n",
       " '1711177052_0.jpg',\n",
       " '1709888254_0.jpg',\n",
       " '1711430853_0.jpg',\n",
       " 'F2_2_1_1.ts_f_1000.jpg',\n",
       " 'F4_1_2_1.ts_f_1000.jpg',\n",
       " 'F2_2_1_2.ts_f_500.jpg',\n",
       " '1709143887_0.jpg',\n",
       " '1711187868_0.jpg',\n",
       " 'F4_1_2_2.ts_f_500.jpg',\n",
       " 'F5_1_1_2.ts_f_500.jpg',\n",
       " 'F5_2_3_2.ts_f_500.jpg',\n",
       " 'F7_1_1_2.ts_f_500.jpg',\n",
       " '1710262674_0.jpg',\n",
       " 'F5_1_3_2.ts_f_500.jpg',\n",
       " 'F4_1_1_1.ts_f_500.jpg',\n",
       " '1709254201_0.jpg',\n",
       " 'F2_1_1_2.ts_f_1000.jpg',\n",
       " '1709265962_0.jpg',\n",
       " 'F7_2_1_2.ts_f_1000.jpg',\n",
       " '1709260681_0.jpg',\n",
       " '1711450664_0.jpg',\n",
       " '1710264453_0.jpg',\n",
       " 'F2_1_1_1.ts_f_500.jpg',\n",
       " '1709864860_0.jpg',\n",
       " '1709879265_0.jpg',\n",
       " '1709186522_0.jpg',\n",
       " '1709283396_0.jpg',\n",
       " '1712577488_0.jpg',\n",
       " 'F4_1_3_1.ts_f_1000.jpg',\n",
       " '1709344621_0.jpg',\n",
       " '29_1.png',\n",
       " 'F5_1_2_2.ts_f_1000.jpg',\n",
       " '1709269442_0.jpg',\n",
       " '1711421860_0.jpg',\n",
       " '1709260082_0.jpg',\n",
       " 'F4_1_1_2.ts_f_500.jpg',\n",
       " 'F2_1_2_2.ts_f_500.jpg',\n",
       " 'F1_2_2_1.ts_f_1000.jpg',\n",
       " '1712555862_0.jpg',\n",
       " '1711119479_0.jpg',\n",
       " '1711330053_0.jpg',\n",
       " 'F2_2_2_1.ts_f_1000.jpg',\n",
       " 'F5_1_3_1.ts_f_500.jpg',\n",
       " '1709255042_0.jpg',\n",
       " 'F1_1_1_2.ts_f_500.jpg',\n",
       " '1710264471_0.jpg',\n",
       " 'F1_2_5_2.ts_f_1000.jpg',\n",
       " '1709807287_0.jpg']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4cd15221eb3f864",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def convert_mask_to_yolo(mask_path, out):\n",
    "    # Открываем изображение\n",
    "    image = cv2.imread(mask_path)\n",
    "\n",
    "    if image is None:\n",
    "        print(f\"Не удалось открыть изображение: {mask_path}\")\n",
    "        return\n",
    "    \n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Создаем маску для черного цвета\n",
    "    black_mask = cv2.inRange(image, (0, 0, 0), (50, 50, 50))\n",
    "\n",
    "    # Создаем новое изображение, где черный цвет остается, а остальные цвета становятся белыми\n",
    "    new_image = np.ones_like(image) * 255  # Начинаем с белого изображения\n",
    "    new_image[black_mask > 0] = [0, 0, 0]  # Заменяем черные пиксели\n",
    "\n",
    "    # Преобразуем в градации серого для нахождения контуров\n",
    "    gray_image = cv2.cvtColor(new_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Находим контуры\n",
    "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Сохраняем контуры в текстовом формате\n",
    "    output_file_name = os.path.splitext(os.path.basename(mask_path))[0] + '.txt'\n",
    "    output_file_path = os.path.join(OUTPUT_DIR, out, output_file_name)\n",
    "\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        for index, contour in enumerate(contours):\n",
    "            # Получаем координаты всех точек контура\n",
    "            contour_points = contour.reshape(-1, 2)\n",
    "            # Нормализуем координаты\n",
    "            normalized_points = [(x / width, y / height) for x, y in contour_points]\n",
    "            points_str = ' '.join(f\"{x:.3f} {y:.3f}\" for x, y in normalized_points)\n",
    "            f.write(f\"0 {points_str}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c0b4e5c-e87c-4798-80b6-ab9cc4bcf6cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7922a992-3e79-43dd-af5c-40c236e8f5dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1717058869_0.jpg',\n",
       " 'F5_2_1_1.ts_f_1000.jpg',\n",
       " 'F7_1_1_1.ts_f_500.jpg',\n",
       " '1712564893_0.jpg',\n",
       " '1717063048_0.jpg',\n",
       " '1710262654_0.jpg',\n",
       " 'F5_1_2_1.ts_f_1000.jpg',\n",
       " 'F1_1_4_2.ts_f_500.jpg',\n",
       " 'F1_2_4_1.ts_f_500.jpg',\n",
       " 'F5_2_2_1.ts_f_1000.jpg',\n",
       " 'F5_2_3_1.ts_f_1000.jpg',\n",
       " 'F1_1_5_1.ts_f_500.jpg',\n",
       " '1711510053_0.jpg',\n",
       " '1710458871_0.jpg',\n",
       " '1713767248_0.jpg',\n",
       " 'F1_2_4_1.ts_f_1000.jpg',\n",
       " 'F7_2_1_1.ts_f_1000.jpg',\n",
       " '1711393084_0.jpg',\n",
       " 'F1_2_5_1.ts_f_500.jpg',\n",
       " 'F5_2_3_1.ts_f_500.jpg',\n",
       " '1710271653_0.jpg',\n",
       " 'F1_1_5_2.ts_f_500.jpg',\n",
       " '1711456067_0.jpg',\n",
       " '1710273470_0.jpg',\n",
       " 'F7_1_1_1.ts_f_1000.jpg',\n",
       " '1710399478_0.jpg',\n",
       " '1709278661_0.jpg',\n",
       " '1715041648_0.jpg',\n",
       " '1710370672_0.jpg',\n",
       " '1710604665_0.jpg',\n",
       " '1709104681_0.jpg',\n",
       " '1709881065_0.jpg',\n",
       " 'F2_2_2_1.ts_f_500.jpg',\n",
       " 'F5_2_2_2.ts_f_1000.jpg',\n",
       " 'F1_2_4_2.ts_f_500.jpg',\n",
       " '1710260853_0.jpg',\n",
       " '17_2.png',\n",
       " '1713350757_0.jpg',\n",
       " 'F5_1_1_1.ts_f_1000.jpg',\n",
       " 'F1_1_2_1.ts_f_500.jpg',\n",
       " '1709310482_0.jpg',\n",
       " '1709191202_0.jpg',\n",
       " '1709520200_0.jpg',\n",
       " '1709278599_0.jpg',\n",
       " '1709859454_0.jpg',\n",
       " 'F5_1_2_1.ts_f_500.jpg',\n",
       " '1709263081_0.jpg',\n",
       " '1710917853_0.jpg',\n",
       " 'F4_2_3_2.ts_f_500.jpg',\n",
       " '1711182453_0.jpg',\n",
       " '1709872064_0.jpg',\n",
       " '1711090673_0.jpg',\n",
       " 'F1_1_1_1.ts_f_1000.jpg',\n",
       " 'F4_1_2_2.ts_f_1000.jpg',\n",
       " '1710275253_0.jpg',\n",
       " '1709857666_0.jpg',\n",
       " '1711448853_0.jpg',\n",
       " 'F1_1_3_1.ts_f_500.jpg',\n",
       " '1710322071_0.jpg',\n",
       " 'F1_1_2_1.ts_f_1000.jpg',\n",
       " '1709271842_0.jpg',\n",
       " 'F7_1_2_1.ts_f_1000.jpg',\n",
       " 'F1_2_3_1.ts_f_500.jpg',\n",
       " 'F7_1_2_2.ts_f_1000.jpg',\n",
       " '1717068448_0.jpg',\n",
       " '1709046321_0.jpg',\n",
       " '1709080522_0.jpg',\n",
       " '1710277054_0.jpg',\n",
       " '1709864863_0.jpg',\n",
       " '1711180652_0.jpg',\n",
       " '1715074048_0.jpg',\n",
       " 'F1_1_2_2.ts_f_500.jpg',\n",
       " 'F7_2_1_2.ts_f_500.jpg',\n",
       " '1710815253_0.jpg',\n",
       " '1710584874_0.jpg',\n",
       " 'F2_1_1_1.ts_f_1000.jpg',\n",
       " 'F4_1_1_2.ts_f_1000.jpg',\n",
       " '1711178852_0.jpg',\n",
       " '1709550918_0.jpg',\n",
       " '1709893685_0.jpg',\n",
       " 'F1_2_2_2.ts_f_500.jpg',\n",
       " '17_1.png',\n",
       " '1709187722_0.jpg',\n",
       " 'F7_1_2_2.ts_f_500.jpg',\n",
       " 'F4_1_2_1.ts_f_500.jpg',\n",
       " '1709276582_0.jpg',\n",
       " 'F1_1_5_1.ts_f_1000.jpg',\n",
       " '1709103022_0.jpg',\n",
       " '1712788070_0.jpg',\n",
       " '1709339881_0.jpg',\n",
       " 'F1_1_1_1.ts_f_500.jpg',\n",
       " 'F2_1_1_2.ts_f_500.jpg',\n",
       " '1717390039_0.jpg',\n",
       " '1710079053_0.jpg',\n",
       " '1717061248_0.jpg',\n",
       " '1715112967_0.jpg',\n",
       " 'F5_2_1_1.ts_f_500.jpg',\n",
       " 'F4_2_2_1.ts_f_1000.jpg',\n",
       " '1710266253_0.jpg',\n",
       " 'F2_2_3_2.ts_f_500.jpg',\n",
       " '1710253681_0.jpg',\n",
       " '1709863065_0.jpg',\n",
       " 'F1_2_3_2.ts_f_1000.jpg',\n",
       " '1709855872_0.jpg',\n",
       " '1713302111_0.jpg',\n",
       " 'F1_1_4_2.ts_f_1000.jpg',\n",
       " '1709265902_0.jpg',\n",
       " 'F4_2_3_2.ts_f_1000.jpg',\n",
       " 'F2_2_1_2.ts_f_1000.jpg',\n",
       " '1710390473_0.jpg',\n",
       " '1709827069_0.jpg',\n",
       " 'F4_2_3_1.ts_f_1000.jpg',\n",
       " '1709866664_0.jpg',\n",
       " '1711009681_0.jpg',\n",
       " 'F2_2_3_1.ts_f_500.jpg',\n",
       " 'F1_2_2_2.ts_f_1000.jpg',\n",
       " '1709859466_0.jpg',\n",
       " 'F1_1_1_2.ts_f_1000.jpg',\n",
       " 'F2_2_1_1.ts_f_500.jpg',\n",
       " '1709101222_0.jpg',\n",
       " '1711256271_0.jpg',\n",
       " '1709875666_0.jpg',\n",
       " '1717059450_0.jpg',\n",
       " 'F5_2_1_2.ts_f_1000.jpg',\n",
       " 'F4_1_1_1.ts_f_1000.jpg',\n",
       " 'F5_2_2_2.ts_f_500.jpg',\n",
       " '1709870253_0.jpg',\n",
       " 'F1_2_3_1.ts_f_1000.jpg',\n",
       " '1710381477_0.jpg',\n",
       " '1709388062_0.jpg',\n",
       " 'F4_2_2_1.ts_f_500.jpg',\n",
       " '1709116441_0.jpg',\n",
       " 'F1_2_5_1.ts_f_1000.jpg',\n",
       " '1711412853_0.jpg',\n",
       " '1710835053_0.jpg',\n",
       " '1710430072_0.jpg',\n",
       " 'F5_1_3_2.ts_f_1000.jpg',\n",
       " 'F5_2_1_2.ts_f_500.jpg',\n",
       " '1710813452_0.jpg',\n",
       " '1709117582_0.jpg',\n",
       " '1709232001_0.jpg',\n",
       " '1709886453_0.jpg',\n",
       " 'F5_1_1_2.ts_f_1000.jpg',\n",
       " '1717022950_0.jpg',\n",
       " '1709255402_0.jpg',\n",
       " 'F1_2_3_2.ts_f_500.jpg',\n",
       " '1710269853_0.jpg',\n",
       " 'F5_1_2_2.ts_f_500.jpg',\n",
       " '1717060391_0.jpg',\n",
       " 'F4_1_3_2.ts_f_500.jpg',\n",
       " 'F5_2_2_1.ts_f_500.jpg',\n",
       " '1710147478_0.jpg',\n",
       " '1709877464_0.jpg',\n",
       " '1711268876_0.jpg',\n",
       " '1709870265_0.jpg',\n",
       " 'F7_1_1_2.ts_f_1000.jpg',\n",
       " 'F1_2_5_2.ts_f_500.jpg',\n",
       " 'F4_2_2_2.ts_f_500.jpg',\n",
       " 'F2_2_3_1.ts_f_1000.jpg',\n",
       " 'F7_2_1_1.ts_f_500.jpg',\n",
       " 'F4_2_2_2.ts_f_1000.jpg',\n",
       " 'F5_1_3_1.ts_f_1000.jpg',\n",
       " 'F1_2_4_2.ts_f_1000.jpg',\n",
       " '1709884664_0.jpg',\n",
       " '1709868465_0.jpg',\n",
       " '1710811652_0.jpg',\n",
       " '1709262422_0.jpg',\n",
       " '1709882867_0.jpg',\n",
       " 'F4_1_3_2.ts_f_1000.jpg',\n",
       " 'F1_2_2_1.ts_f_500.jpg',\n",
       " 'F7_1_2_1.ts_f_500.jpg',\n",
       " 'F2_2_2_2.ts_f_1000.jpg',\n",
       " 'F2_2_3_2.ts_f_1000.jpg',\n",
       " '1715158095_0.jpg',\n",
       " '1709587678_0.jpg',\n",
       " 'F1_1_5_2.ts_f_1000.jpg',\n",
       " 'F5_2_3_2.ts_f_1000.jpg',\n",
       " '1709046334_0.jpg',\n",
       " '1709875661_0.jpg',\n",
       " 'F1_1_2_2.ts_f_1000.jpg',\n",
       " '1716017248_0.jpg',\n",
       " 'F1_1_3_1.ts_f_1000.jpg',\n",
       " '1711186053_0.jpg',\n",
       " '1711178872_0.jpg',\n",
       " '1711459680_0.jpg',\n",
       " '1710809853_0.jpg',\n",
       " 'F4_1_3_1.ts_f_500.jpg',\n",
       " '1710273453_0.jpg',\n",
       " 'F5_1_1_1.ts_f_500.jpg',\n",
       " 'F4_2_3_1.ts_f_500.jpg',\n",
       " '1712741269_0.jpg',\n",
       " 'gr1.jpg',\n",
       " '1709261462_0.jpg',\n",
       " 'F2_2_2_2.ts_f_500.jpg',\n",
       " '1710268053_0.jpg',\n",
       " '1711268872_0.jpg',\n",
       " '1709253121_0.jpg',\n",
       " '1710847680_0.jpg',\n",
       " 'F2_1_2_2.ts_f_1000.jpg',\n",
       " '1711184253_0.jpg',\n",
       " 'synt100_1710813452_0.png',\n",
       " 'synt101_1710835053_0.png',\n",
       " 'synt103_1711186053_0.png',\n",
       " 'synt105_1712788070_0.png',\n",
       " 'synt106_1715112967_0.png',\n",
       " 'synt107_1717060391_0.png',\n",
       " 'synt10_1709255042_0.png',\n",
       " 'synt11_1709260082_0.png',\n",
       " 'synt12_1709260681_0.png',\n",
       " 'synt13_1709261462_0.png',\n",
       " 'synt14_1709262422_0.png',\n",
       " 'synt15_1709265902_0.png',\n",
       " 'synt16_1709265962_0.png',\n",
       " 'synt17_1709276582_0.png',\n",
       " 'synt18_1709310482_0.png',\n",
       " 'synt1_1709104681_0.png',\n",
       " 'synt20_1709344621_0.png',\n",
       " 'synt21_1709388062_0.png',\n",
       " 'synt22_1709859454_0.png',\n",
       " 'synt23_1709859466_0.png',\n",
       " 'synt256_1709104681_0.png',\n",
       " 'synt257_1709109361_0.png',\n",
       " 'synt258_1709116441_0.png',\n",
       " 'synt259_1709186522_0.png',\n",
       " 'synt25_1709888254_0.png',\n",
       " 'synt260_1709187722_0.png',\n",
       " 'synt261_1709191202_0.png',\n",
       " 'synt262_1709232001_0.png',\n",
       " 'synt263_1709253121_0.png',\n",
       " 'synt264_1709254201_0.png',\n",
       " 'synt265_1709255042_0.png',\n",
       " 'synt266_1709260082_0.png',\n",
       " 'synt267_1709260681_0.png',\n",
       " 'synt268_1709261462_0.png',\n",
       " 'synt269_1709262422_0.png',\n",
       " 'synt26_1710079053_0.png',\n",
       " 'synt270_1709265902_0.png',\n",
       " 'synt271_1709265962_0.png',\n",
       " 'synt272_1709276582_0.png',\n",
       " 'synt273_1709310482_0.png',\n",
       " 'synt274_1709339881_0.png',\n",
       " 'synt275_1709344621_0.png',\n",
       " 'synt276_1709388062_0.png',\n",
       " 'synt277_1709859454_0.png',\n",
       " 'synt278_1709859466_0.png',\n",
       " 'synt27_1710253681_0.png',\n",
       " 'synt280_1709888254_0.png',\n",
       " 'synt282_1710253681_0.png',\n",
       " 'synt283_1710260853_0.png',\n",
       " 'synt284_1710262654_0.png',\n",
       " 'synt285_1710266253_0.png',\n",
       " 'synt287_1710809853_0.png',\n",
       " 'synt288_1710813452_0.png',\n",
       " 'synt28_1710260853_0.png',\n",
       " 'synt291_1711186053_0.png',\n",
       " 'synt293_1712788070_0.png',\n",
       " 'synt294_1715112967_0.png',\n",
       " 'synt295_1717060391_0.png',\n",
       " 'synt296_dirty1.png',\n",
       " 'synt29_1710262654_0.png',\n",
       " 'synt2_1709109361_0.png',\n",
       " 'synt30_1710266253_0.png',\n",
       " 'synt31_1710458871_0.png',\n",
       " 'synt32_1710809853_0.png',\n",
       " 'synt33_1710813452_0.png',\n",
       " 'synt34_1710835053_0.png',\n",
       " 'synt36_1711186053_0.png',\n",
       " 'synt38_1712788070_0.png',\n",
       " 'synt39_1715112967_0.png',\n",
       " 'synt3_1709116441_0.png',\n",
       " 'synt401_1709104681_0.png',\n",
       " 'synt402_1709109361_0.png',\n",
       " 'synt403_1709116441_0.png',\n",
       " 'synt404_1709186522_0.png',\n",
       " 'synt405_1709187722_0.png',\n",
       " 'synt406_1709191202_0.png',\n",
       " 'synt407_1709232001_0.png',\n",
       " 'synt408_1709253121_0.png',\n",
       " 'synt409_1709254201_0.png',\n",
       " 'synt40_1717060391_0.png',\n",
       " 'synt410_1709255042_0.png',\n",
       " 'synt411_1709260082_0.png',\n",
       " 'synt412_1709260681_0.png',\n",
       " 'synt413_1709261462_0.png',\n",
       " 'synt414_1709262422_0.png',\n",
       " 'synt415_1709265902_0.png',\n",
       " 'synt416_1709265962_0.png',\n",
       " 'synt417_1709276582_0.png',\n",
       " 'synt418_1709310482_0.png',\n",
       " 'synt419_1709339881_0.png',\n",
       " 'synt41_dirty1.png',\n",
       " 'synt420_1709344621_0.png',\n",
       " 'synt421_1709388062_0.png',\n",
       " 'synt422_1709859454_0.png',\n",
       " 'synt423_1709859466_0.png',\n",
       " 'synt425_1709888254_0.png',\n",
       " 'synt426_1710079053_0.png',\n",
       " 'synt427_1710253681_0.png',\n",
       " 'synt428_1710260853_0.png',\n",
       " 'synt429_1710262654_0.png',\n",
       " 'synt430_1710266253_0.png',\n",
       " 'synt431_1710458871_0.png',\n",
       " 'synt432_1710809853_0.png',\n",
       " 'synt433_1710813452_0.png',\n",
       " 'synt434_1710835053_0.png',\n",
       " 'synt436_1711186053_0.png',\n",
       " 'synt438_1712788070_0.png',\n",
       " 'synt439_1715112967_0.png',\n",
       " 'synt440_1717060391_0.png',\n",
       " 'synt441_dirty1.png',\n",
       " 'synt4_1709186522_0.png',\n",
       " 'synt5_1709187722_0.png',\n",
       " 'synt68_1709104681_0.png',\n",
       " 'synt69_1709109361_0.png',\n",
       " 'synt6_1709191202_0.png',\n",
       " 'synt70_1709116441_0.png',\n",
       " 'synt71_1709186522_0.png',\n",
       " 'synt72_1709187722_0.png',\n",
       " 'synt73_1709191202_0.png',\n",
       " 'synt74_1709232001_0.png',\n",
       " 'synt75_1709253121_0.png',\n",
       " 'synt76_1709254201_0.png',\n",
       " 'synt77_1709255042_0.png',\n",
       " 'synt78_1709260082_0.png',\n",
       " 'synt79_1709260681_0.png',\n",
       " 'synt7_1709232001_0.png',\n",
       " 'synt80_1709261462_0.png',\n",
       " 'synt81_1709262422_0.png',\n",
       " 'synt82_1709265902_0.png',\n",
       " 'synt83_1709265962_0.png',\n",
       " 'synt84_1709276582_0.png',\n",
       " 'synt85_1709310482_0.png',\n",
       " 'synt86_1709339881_0.png',\n",
       " 'synt87_1709344621_0.png',\n",
       " 'synt88_1709388062_0.png',\n",
       " 'synt89_1709859454_0.png',\n",
       " 'synt90_1709859466_0.png',\n",
       " 'synt91_1709886453_0.png',\n",
       " 'synt92_1709888254_0.png',\n",
       " 'synt93_1710079053_0.png',\n",
       " 'synt94_1710253681_0.png',\n",
       " 'synt95_1710260853_0.png',\n",
       " 'synt96_1710262654_0.png',\n",
       " 'synt97_1710266253_0.png',\n",
       " 'synt98_1710458871_0.png',\n",
       " 'synt99_1710809853_0.png',\n",
       " 'synt9_1709254201_0.png']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd28d575-1bca-45f1-97c4-0caff4c1e432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdf99ded756441b8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Копирование изображений и масок в соответствующие папки\n",
    "for img in train_images:\n",
    "    if 'synt' not in img:\n",
    "        shutil.copy(os.path.join(IMAGES_DIR, img), os.path.join(OUTPUT_DIR, 'images/train', img))\n",
    "        mask_name = img.replace('.jpg', '.png')\n",
    "        convert_mask_to_yolo(os.path.join(MASKS_DIR, mask_name), 'labels/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a80bd15-a3ef-4e36-8e81-ba2c1c2f0155",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F1_1_1_2.ts_f_500.jpg',\n",
       " 'F4_1_1_1.ts_f_500.jpg',\n",
       " '1710430072_0.jpg',\n",
       " 'F5_1_2_1.ts_f_1000.jpg',\n",
       " 'syntF7_1_1_1.ts_f_500.jpg',\n",
       " 'syntF4_2_3_2.ts_f_500.jpg',\n",
       " 'syntF4_2_2_2.ts_f_500.jpg',\n",
       " 'F1_1_5_2.ts_f_1000.jpg',\n",
       " 'F5_2_1_1.ts_f_1000.jpg',\n",
       " 'synt1711187868_0.jpg',\n",
       " 'syntF1_2_3_1.ts_f_500.jpg',\n",
       " '1717063048_0.jpg',\n",
       " 'F2_2_1_1.ts_f_1000.jpg',\n",
       " 'syntF2_2_1_1.ts_f_500.jpg',\n",
       " 'F1_2_5_2.ts_f_500.jpg',\n",
       " 'synt1711256271_0.jpg',\n",
       " 'F5_2_1_2.ts_f_1000.jpg',\n",
       " '1710917853_0.jpg',\n",
       " '1711180652_0.jpg',\n",
       " 'syntF2_2_2_1.ts_f_1000.jpg',\n",
       " 'F5_1_3_2.ts_f_1000.jpg',\n",
       " 'synt1715112967_0.jpg',\n",
       " 'syntF1_2_2_1.ts_f_500.jpg',\n",
       " 'syntF2_1_1_2.ts_f_500.jpg',\n",
       " 'synt1711510053_0.jpg',\n",
       " 'syntF1_1_5_2.ts_f_1000.jpg',\n",
       " 'F5_2_2_2.ts_f_500.jpg',\n",
       " 'synt1711456067_0.jpg',\n",
       " '1711090673_0.jpg',\n",
       " 'synt1711180652_0.jpg',\n",
       " 'F1_2_2_1.ts_f_1000.jpg',\n",
       " 'syntF5_2_2_2.ts_f_1000.jpg',\n",
       " '1711009681_0.jpg',\n",
       " 'F5_1_2_2.ts_f_500.jpg',\n",
       " 'syntF5_2_2_2.ts_f_500.jpg',\n",
       " '1711268876_0.jpg',\n",
       " 'syntF1_1_3_1.ts_f_500.jpg',\n",
       " 'syntF1_1_1_1.ts_f_500.jpg',\n",
       " 'F7_1_2_2.ts_f_1000.jpg',\n",
       " 'syntF1_1_5_1.ts_f_500.jpg',\n",
       " 'F5_2_1_2.ts_f_500.jpg',\n",
       " 'synt1710430072_0.jpg',\n",
       " '1717061248_0.jpg',\n",
       " 'F2_2_3_2.ts_f_1000.jpg',\n",
       " 'F4_2_3_2.ts_f_500.jpg',\n",
       " '1710835053_0.jpg',\n",
       " '1710399478_0.jpg',\n",
       " 'syntF4_2_2_2.ts_f_1000.jpg',\n",
       " 'synt1713767248_0.jpg',\n",
       " 'F1_2_5_2.ts_f_1000.jpg',\n",
       " '1711268872_0.jpg',\n",
       " '1711178852_0.jpg',\n",
       " 'syntF4_1_2_2.ts_f_1000.jpg',\n",
       " 'syntF4_2_3_1.ts_f_1000.jpg',\n",
       " '1717058869_0.jpg',\n",
       " 'synt1710835053_0.jpg',\n",
       " '1710604665_0.jpg',\n",
       " 'synt1717061248_0.jpg',\n",
       " 'syntF1_1_2_2.ts_f_1000.jpg',\n",
       " 'F1_2_2_2.ts_f_1000.jpg',\n",
       " 'syntF5_2_3_1.ts_f_1000.jpg',\n",
       " 'synt1711330053_0.jpg',\n",
       " 'synt1717390039_0.jpg',\n",
       " 'F7_2_1_2.ts_f_1000.jpg',\n",
       " 'synt1715041648_0.jpg',\n",
       " 'synt1710584874_0.jpg',\n",
       " 'F1_2_5_1.ts_f_500.jpg',\n",
       " 'F1_1_2_2.ts_f_1000.jpg',\n",
       " 'syntF1_1_2_1.ts_f_1000.jpg',\n",
       " 'F2_1_1_1.ts_f_500.jpg',\n",
       " 'syntF4_1_1_2.ts_f_1000.jpg',\n",
       " 'F2_1_1_2.ts_f_1000.jpg',\n",
       " 'synt1710604665_0.jpg',\n",
       " 'F1_1_2_1.ts_f_500.jpg',\n",
       " '1711178872_0.jpg',\n",
       " '1711448853_0.jpg',\n",
       " 'F4_1_3_1.ts_f_500.jpg',\n",
       " 'F1_1_2_1.ts_f_1000.jpg',\n",
       " 'syntF2_2_1_1.ts_f_1000.jpg',\n",
       " 'F5_2_3_1.ts_f_1000.jpg',\n",
       " 'F4_2_2_1.ts_f_1000.jpg',\n",
       " 'F5_2_1_1.ts_f_500.jpg',\n",
       " 'synt1711182453_0.jpg',\n",
       " 'F1_2_3_2.ts_f_500.jpg',\n",
       " 'syntF4_1_2_1.ts_f_1000.jpg',\n",
       " 'syntF1_1_5_2.ts_f_500.jpg',\n",
       " 'syntF4_1_2_1.ts_f_500.jpg',\n",
       " 'F7_1_1_1.ts_f_1000.jpg',\n",
       " 'syntF1_2_4_1.ts_f_500.jpg',\n",
       " 'F7_2_1_2.ts_f_500.jpg',\n",
       " 'syntF1_1_1_1.ts_f_1000.jpg',\n",
       " 'syntF4_2_3_2.ts_f_1000.jpg',\n",
       " 'syntF1_2_3_2.ts_f_500.jpg',\n",
       " 'F4_2_3_1.ts_f_1000.jpg',\n",
       " '1711450664_0.jpg',\n",
       " 'syntF1_2_3_2.ts_f_1000.jpg',\n",
       " 'syntF1_1_2_1.ts_f_500.jpg',\n",
       " 'synt1710390473_0.jpg',\n",
       " 'F2_2_3_2.ts_f_500.jpg',\n",
       " 'syntF2_2_1_2.ts_f_500.jpg',\n",
       " 'synt1717059450_0.jpg',\n",
       " 'F4_1_2_1.ts_f_1000.jpg',\n",
       " 'syntF5_2_1_2.ts_f_500.jpg',\n",
       " 'syntF7_1_1_1.ts_f_1000.jpg',\n",
       " 'syntF1_1_2_2.ts_f_500.jpg',\n",
       " 'syntF5_1_2_1.ts_f_1000.jpg',\n",
       " 'F7_1_2_2.ts_f_500.jpg',\n",
       " 'synt1711184253_0.jpg',\n",
       " 'synt1715158095_0.jpg',\n",
       " 'F5_2_3_1.ts_f_500.jpg',\n",
       " 'synt1717022950_0.jpg',\n",
       " '1710322071_0.jpg',\n",
       " 'F4_1_1_2.ts_f_1000.jpg',\n",
       " 'synt1710813452_0.jpg',\n",
       " 'synt1716017248_0.jpg',\n",
       " 'F4_1_2_2.ts_f_500.jpg',\n",
       " 'syntF1_2_5_1.ts_f_500.jpg',\n",
       " 'syntF4_1_3_2.ts_f_500.jpg',\n",
       " 'F4_2_2_2.ts_f_1000.jpg',\n",
       " 'F1_1_4_2.ts_f_500.jpg',\n",
       " '1711256271_0.jpg',\n",
       " 'F1_2_3_1.ts_f_1000.jpg',\n",
       " 'syntF2_2_3_2.ts_f_500.jpg',\n",
       " 'F2_2_2_2.ts_f_1000.jpg',\n",
       " 'synt1711178872_0.jpg',\n",
       " 'syntF2_2_3_1.ts_f_1000.jpg',\n",
       " 'F5_1_1_1.ts_f_1000.jpg',\n",
       " 'synt1710815253_0.jpg',\n",
       " 'syntF1_1_4_2.ts_f_500.jpg',\n",
       " '1712577488_0.jpg',\n",
       " 'syntF5_1_3_1.ts_f_500.jpg',\n",
       " 'F1_1_1_2.ts_f_1000.jpg',\n",
       " 'syntF1_2_3_1.ts_f_1000.jpg',\n",
       " 'F1_2_3_2.ts_f_1000.jpg',\n",
       " 'synt1710847680_0.jpg',\n",
       " 'F1_1_2_2.ts_f_500.jpg',\n",
       " 'F7_1_1_2.ts_f_1000.jpg',\n",
       " '1710811652_0.jpg',\n",
       " '1711421860_0.jpg',\n",
       " '1711187868_0.jpg',\n",
       " 'synt1710275253_0.jpg',\n",
       " 'F5_1_3_1.ts_f_500.jpg',\n",
       " 'syntF1_1_5_1.ts_f_1000.jpg',\n",
       " 'F4_2_3_2.ts_f_1000.jpg',\n",
       " 'syntF2_2_2_2.ts_f_1000.jpg',\n",
       " '1715074048_0.jpg',\n",
       " 'F2_1_1_1.ts_f_1000.jpg',\n",
       " 'syntF1_2_4_2.ts_f_500.jpg',\n",
       " 'synt1710458871_0.jpg',\n",
       " 'synt1711412853_0.jpg',\n",
       " 'F4_1_3_2.ts_f_1000.jpg',\n",
       " '1711510053_0.jpg',\n",
       " 'F1_2_4_2.ts_f_1000.jpg',\n",
       " 'syntF2_1_1_1.ts_f_1000.jpg',\n",
       " 'synt1711430853_0.jpg',\n",
       " 'F5_1_1_2.ts_f_500.jpg',\n",
       " '1711184253_0.jpg',\n",
       " 'syntF7_1_2_2.ts_f_1000.jpg',\n",
       " 'F5_1_3_1.ts_f_1000.jpg',\n",
       " 'syntF4_1_1_1.ts_f_1000.jpg',\n",
       " '1710381477_0.jpg',\n",
       " '1710275253_0.jpg',\n",
       " 'syntF5_1_1_1.ts_f_500.jpg',\n",
       " 'syntF4_1_3_1.ts_f_1000.jpg',\n",
       " 'syntF5_2_3_1.ts_f_500.jpg',\n",
       " 'F2_1_2_2.ts_f_500.jpg',\n",
       " 'synt1717058869_0.jpg',\n",
       " 'synt1711178852_0.jpg',\n",
       " 'syntF4_1_3_1.ts_f_500.jpg',\n",
       " 'synt1713350757_0.jpg',\n",
       " 'synt1712564893_0.jpg',\n",
       " 'F2_2_1_2.ts_f_1000.jpg',\n",
       " 'synt1710277054_0.jpg',\n",
       " 'F2_1_1_2.ts_f_500.jpg',\n",
       " '1711430853_0.jpg',\n",
       " 'synt1712741269_0.jpg',\n",
       " '1717068448_0.jpg',\n",
       " 'F5_2_2_1.ts_f_500.jpg',\n",
       " '1713302111_0.jpg',\n",
       " '1711330053_0.jpg',\n",
       " 'syntF2_2_3_2.ts_f_1000.jpg',\n",
       " '1710809853_0.jpg',\n",
       " 'syntF7_2_1_1.ts_f_500.jpg',\n",
       " 'synt1711268872_0.jpg',\n",
       " 'syntF5_1_1_1.ts_f_1000.jpg',\n",
       " '1711186053_0.jpg',\n",
       " 'syntF2_1_1_2.ts_f_1000.jpg',\n",
       " 'syntF1_2_2_1.ts_f_1000.jpg',\n",
       " 'synt1717063048_0.jpg',\n",
       " 'F7_1_2_1.ts_f_500.jpg',\n",
       " 'syntF4_1_2_2.ts_f_500.jpg',\n",
       " 'F5_1_2_1.ts_f_500.jpg',\n",
       " '1717390039_0.jpg',\n",
       " 'F5_1_1_2.ts_f_1000.jpg',\n",
       " 'F4_2_2_2.ts_f_500.jpg',\n",
       " '1710458871_0.jpg',\n",
       " 'F1_1_1_1.ts_f_500.jpg',\n",
       " '1710815253_0.jpg',\n",
       " 'F5_2_3_2.ts_f_500.jpg',\n",
       " '1711459680_0.jpg',\n",
       " 'syntF5_1_3_1.ts_f_1000.jpg',\n",
       " 'syntF5_2_2_1.ts_f_500.jpg',\n",
       " 'synt1712788070_0.jpg',\n",
       " 'syntF4_2_2_1.ts_f_1000.jpg',\n",
       " 'syntF1_1_1_2.ts_f_500.jpg',\n",
       " 'synt1711448853_0.jpg',\n",
       " 'synt1712555862_0.jpg',\n",
       " 'syntF5_1_2_2.ts_f_1000.jpg',\n",
       " 'syntF1_2_2_2.ts_f_1000.jpg',\n",
       " 'syntF7_1_2_1.ts_f_1000.jpg',\n",
       " 'syntF5_1_2_2.ts_f_500.jpg',\n",
       " 'syntF2_1_2_2.ts_f_1000.jpg',\n",
       " 'synt1711268876_0.jpg',\n",
       " 'F7_1_2_1.ts_f_1000.jpg',\n",
       " '1712564893_0.jpg',\n",
       " 'synt1713302111_0.jpg',\n",
       " 'synt1711119479_0.jpg',\n",
       " 'syntF1_2_4_2.ts_f_1000.jpg',\n",
       " 'F2_2_2_1.ts_f_1000.jpg',\n",
       " '1715158095_0.jpg',\n",
       " '1710277054_0.jpg',\n",
       " '1713767248_0.jpg',\n",
       " 'F1_2_2_1.ts_f_500.jpg',\n",
       " 'synt1711421860_0.jpg',\n",
       " '1711412853_0.jpg',\n",
       " 'syntF1_2_5_2.ts_f_1000.jpg',\n",
       " 'F1_2_5_1.ts_f_1000.jpg',\n",
       " '1715112967_0.jpg',\n",
       " 'syntF1_2_5_2.ts_f_500.jpg',\n",
       " 'syntF1_1_3_1.ts_f_1000.jpg',\n",
       " '1710813452_0.jpg',\n",
       " 'F1_2_4_1.ts_f_500.jpg',\n",
       " 'syntF2_2_1_2.ts_f_1000.jpg',\n",
       " 'F7_2_1_1.ts_f_500.jpg',\n",
       " 'syntF7_1_2_2.ts_f_500.jpg',\n",
       " 'F5_1_1_1.ts_f_500.jpg',\n",
       " 'syntF5_2_3_2.ts_f_500.jpg',\n",
       " 'syntF1_1_4_2.ts_f_1000.jpg',\n",
       " '1716017248_0.jpg',\n",
       " '1715041648_0.jpg',\n",
       " 'synt1710381477_0.jpg',\n",
       " 'synt1711090673_0.jpg',\n",
       " 'syntF7_2_1_2.ts_f_500.jpg',\n",
       " 'synt1710917853_0.jpg',\n",
       " 'gr1.jpg',\n",
       " '1717060391_0.jpg',\n",
       " '1713350757_0.jpg',\n",
       " 'syntF1_1_1_2.ts_f_1000.jpg',\n",
       " 'syntF1_2_2_2.ts_f_500.jpg',\n",
       " '1711177052_0.jpg',\n",
       " 'syntF5_1_3_2.ts_f_1000.jpg',\n",
       " 'F7_1_1_2.ts_f_500.jpg',\n",
       " 'syntF2_2_2_1.ts_f_500.jpg',\n",
       " 'synt1711177052_0.jpg',\n",
       " 'syntF4_1_3_2.ts_f_1000.jpg',\n",
       " 'syntF1_2_4_1.ts_f_1000.jpg',\n",
       " 'syntF5_1_2_1.ts_f_500.jpg',\n",
       " 'F5_2_2_1.ts_f_1000.jpg',\n",
       " 'F4_2_3_1.ts_f_500.jpg',\n",
       " 'F5_2_2_2.ts_f_1000.jpg',\n",
       " 'F2_1_2_2.ts_f_1000.jpg',\n",
       " 'F1_2_4_1.ts_f_1000.jpg',\n",
       " 'syntF7_1_1_2.ts_f_500.jpg',\n",
       " 'synt1712577488_0.jpg',\n",
       " 'F4_1_3_1.ts_f_1000.jpg',\n",
       " 'syntgr1.jpg',\n",
       " '1711119479_0.jpg',\n",
       " 'synt1711009681_0.jpg',\n",
       " 'F1_1_3_1.ts_f_1000.jpg',\n",
       " 'F2_2_2_1.ts_f_500.jpg',\n",
       " 'syntF2_1_2_2.ts_f_500.jpg',\n",
       " 'F2_2_1_1.ts_f_500.jpg']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "089793a9-27c9-4801-833a-49bab279f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for synt_img in train_images:\n",
    "    img = synt_img[4:]\n",
    "    if 'synt' in synt_img:\n",
    "        shutil.copy(os.path.join(IMAGES_DIR2, img), os.path.join(OUTPUT_DIR, 'images/train', img))\n",
    "        mask_name = img.replace('.jpg', '.png')\n",
    "        convert_mask_to_yolo(os.path.join(MASKS_DIR2, mask_name), 'labels/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b2fd246643ce5cf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for img in val_images:\n",
    "    if 'synt' not in img:\n",
    "        shutil.copy(os.path.join(IMAGES_DIR, img), os.path.join(OUTPUT_DIR, 'images/val', img))\n",
    "        mask_name = img.replace('.jpg', '.png')\n",
    "        convert_mask_to_yolo(os.path.join(MASKS_DIR, mask_name), 'labels/val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f5f9c58b-e536-472d-ae27-7dfab27121e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F1_1_4_2.ts_f_1000.jpg',\n",
       " 'synt4_1709186522_0.png',\n",
       " '1711182453_0.jpg',\n",
       " 'synt415_1709265902_0.png',\n",
       " '1710584874_0.jpg',\n",
       " 'F2_2_1_1.ts_f_1000.jpg',\n",
       " 'synt10_1709255042_0.png',\n",
       " 'synt20_1709344621_0.png',\n",
       " 'F1_1_1_1.ts_f_1000.jpg',\n",
       " 'F4_1_1_2.ts_f_500.jpg',\n",
       " 'synt3_1709116441_0.png',\n",
       " 'F1_1_5_1.ts_f_1000.jpg',\n",
       " 'F7_1_2_2.ts_f_500.jpg',\n",
       " 'synt105_1712788070_0.png',\n",
       " 'synt274_1709339881_0.png',\n",
       " 'F1_2_3_1.ts_f_500.jpg',\n",
       " 'synt1_1709104681_0.png',\n",
       " 'F4_2_3_2.ts_f_500.jpg',\n",
       " 'synt96_1710262654_0.png',\n",
       " 'synt406_1709191202_0.png',\n",
       " 'synt267_1709260681_0.png',\n",
       " 'synt11_1709260082_0.png',\n",
       " 'synt262_1709232001_0.png',\n",
       " 'F2_2_2_2.ts_f_500.jpg',\n",
       " '1712555862_0.jpg',\n",
       " 'F4_1_2_1.ts_f_500.jpg',\n",
       " 'synt79_1709260681_0.png',\n",
       " '1711393084_0.jpg',\n",
       " 'F4_1_1_1.ts_f_500.jpg',\n",
       " 'synt16_1709265962_0.png',\n",
       " 'F4_2_2_1.ts_f_500.jpg',\n",
       " '1710390473_0.jpg',\n",
       " 'F7_2_1_2.ts_f_1000.jpg',\n",
       " 'synt40_1717060391_0.png',\n",
       " 'synt81_1709262422_0.png',\n",
       " 'F1_1_5_2.ts_f_1000.jpg',\n",
       " 'synt107_1717060391_0.png',\n",
       " 'synt25_1709888254_0.png',\n",
       " 'F5_1_2_1.ts_f_1000.jpg',\n",
       " 'F7_2_1_2.ts_f_500.jpg',\n",
       " '1710430072_0.jpg',\n",
       " 'F5_2_1_2.ts_f_1000.jpg',\n",
       " 'synt68_1709104681_0.png',\n",
       " 'synt440_1717060391_0.png',\n",
       " '1712788070_0.jpg',\n",
       " '1710370672_0.jpg',\n",
       " 'F5_1_2_2.ts_f_500.jpg',\n",
       " 'synt401_1709104681_0.png',\n",
       " 'synt28_1710260853_0.png',\n",
       " 'F1_2_2_2.ts_f_500.jpg',\n",
       " '1717059450_0.jpg',\n",
       " 'synt30_1710266253_0.png',\n",
       " 'F7_2_1_1.ts_f_1000.jpg',\n",
       " '1712741269_0.jpg',\n",
       " '1710917853_0.jpg',\n",
       " 'synt18_1709310482_0.png',\n",
       " 'F2_2_1_2.ts_f_500.jpg',\n",
       " '1717063048_0.jpg',\n",
       " '1711180652_0.jpg',\n",
       " 'synt284_1710262654_0.png',\n",
       " 'synt272_1709276582_0.png',\n",
       " 'F2_2_3_1.ts_f_1000.jpg',\n",
       " 'F1_2_5_2.ts_f_1000.jpg',\n",
       " 'F4_1_3_2.ts_f_500.jpg']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "557532ce-6081-42a4-8a1f-26aee2ea6d09",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1717058869_0.jpg',\n",
       " 'F5_2_1_1.ts_f_1000.jpg',\n",
       " 'F7_1_1_1.ts_f_500.jpg',\n",
       " '1712564893_0.jpg',\n",
       " '1717063048_0.jpg',\n",
       " '1710262654_0.jpg',\n",
       " 'F5_1_2_1.ts_f_1000.jpg',\n",
       " 'F1_1_4_2.ts_f_500.jpg',\n",
       " 'F1_2_4_1.ts_f_500.jpg',\n",
       " 'F5_2_2_1.ts_f_1000.jpg',\n",
       " 'F5_2_3_1.ts_f_1000.jpg',\n",
       " 'F1_1_5_1.ts_f_500.jpg',\n",
       " '1711510053_0.jpg',\n",
       " '1710458871_0.jpg',\n",
       " '1713767248_0.jpg',\n",
       " 'F1_2_4_1.ts_f_1000.jpg',\n",
       " 'F7_2_1_1.ts_f_1000.jpg',\n",
       " '1711393084_0.jpg',\n",
       " 'F1_2_5_1.ts_f_500.jpg',\n",
       " 'F5_2_3_1.ts_f_500.jpg',\n",
       " '1710271653_0.jpg',\n",
       " 'F1_1_5_2.ts_f_500.jpg',\n",
       " '1711456067_0.jpg',\n",
       " '1710273470_0.jpg',\n",
       " 'F7_1_1_1.ts_f_1000.jpg',\n",
       " '1710399478_0.jpg',\n",
       " '1709278661_0.jpg',\n",
       " '1715041648_0.jpg',\n",
       " '1710370672_0.jpg',\n",
       " '1710604665_0.jpg',\n",
       " '1709104681_0.jpg',\n",
       " '1709881065_0.jpg',\n",
       " 'F2_2_2_1.ts_f_500.jpg',\n",
       " 'F5_2_2_2.ts_f_1000.jpg',\n",
       " 'F1_2_4_2.ts_f_500.jpg',\n",
       " '1710260853_0.jpg',\n",
       " '17_2.png',\n",
       " '1713350757_0.jpg',\n",
       " 'F5_1_1_1.ts_f_1000.jpg',\n",
       " 'F1_1_2_1.ts_f_500.jpg',\n",
       " '1709310482_0.jpg',\n",
       " '1709191202_0.jpg',\n",
       " '1709520200_0.jpg',\n",
       " '1709278599_0.jpg',\n",
       " '1709859454_0.jpg',\n",
       " 'F5_1_2_1.ts_f_500.jpg',\n",
       " '1709263081_0.jpg',\n",
       " '1710917853_0.jpg',\n",
       " 'F4_2_3_2.ts_f_500.jpg',\n",
       " '1711182453_0.jpg',\n",
       " '1709872064_0.jpg',\n",
       " '1711090673_0.jpg',\n",
       " 'F1_1_1_1.ts_f_1000.jpg',\n",
       " 'F4_1_2_2.ts_f_1000.jpg',\n",
       " '1710275253_0.jpg',\n",
       " '1709857666_0.jpg',\n",
       " '1711448853_0.jpg',\n",
       " 'F1_1_3_1.ts_f_500.jpg',\n",
       " '1710322071_0.jpg',\n",
       " 'F1_1_2_1.ts_f_1000.jpg',\n",
       " '1709271842_0.jpg',\n",
       " 'F7_1_2_1.ts_f_1000.jpg',\n",
       " 'F1_2_3_1.ts_f_500.jpg',\n",
       " 'F7_1_2_2.ts_f_1000.jpg',\n",
       " '1717068448_0.jpg',\n",
       " '1709046321_0.jpg',\n",
       " '1709080522_0.jpg',\n",
       " '1710277054_0.jpg',\n",
       " '1709864863_0.jpg',\n",
       " '1711180652_0.jpg',\n",
       " '1715074048_0.jpg',\n",
       " 'F1_1_2_2.ts_f_500.jpg',\n",
       " 'F7_2_1_2.ts_f_500.jpg',\n",
       " '1710815253_0.jpg',\n",
       " '1710584874_0.jpg',\n",
       " 'F2_1_1_1.ts_f_1000.jpg',\n",
       " 'F4_1_1_2.ts_f_1000.jpg',\n",
       " '1711178852_0.jpg',\n",
       " '1709550918_0.jpg',\n",
       " '1709893685_0.jpg',\n",
       " 'F1_2_2_2.ts_f_500.jpg',\n",
       " '17_1.png',\n",
       " '1709187722_0.jpg',\n",
       " 'F7_1_2_2.ts_f_500.jpg',\n",
       " 'F4_1_2_1.ts_f_500.jpg',\n",
       " '1709276582_0.jpg',\n",
       " 'F1_1_5_1.ts_f_1000.jpg',\n",
       " '1709103022_0.jpg',\n",
       " '1712788070_0.jpg',\n",
       " '1709339881_0.jpg',\n",
       " 'F1_1_1_1.ts_f_500.jpg',\n",
       " 'F2_1_1_2.ts_f_500.jpg',\n",
       " '1717390039_0.jpg',\n",
       " '1710079053_0.jpg',\n",
       " '1717061248_0.jpg',\n",
       " '1715112967_0.jpg',\n",
       " 'F5_2_1_1.ts_f_500.jpg',\n",
       " 'F4_2_2_1.ts_f_1000.jpg',\n",
       " '1710266253_0.jpg',\n",
       " 'F2_2_3_2.ts_f_500.jpg',\n",
       " '1710253681_0.jpg',\n",
       " '1709863065_0.jpg',\n",
       " 'F1_2_3_2.ts_f_1000.jpg',\n",
       " '1709855872_0.jpg',\n",
       " '1713302111_0.jpg',\n",
       " 'F1_1_4_2.ts_f_1000.jpg',\n",
       " '1709265902_0.jpg',\n",
       " 'F4_2_3_2.ts_f_1000.jpg',\n",
       " 'F2_2_1_2.ts_f_1000.jpg',\n",
       " '1710390473_0.jpg',\n",
       " '1709827069_0.jpg',\n",
       " 'F4_2_3_1.ts_f_1000.jpg',\n",
       " '1709866664_0.jpg',\n",
       " '1711009681_0.jpg',\n",
       " 'F2_2_3_1.ts_f_500.jpg',\n",
       " 'F1_2_2_2.ts_f_1000.jpg',\n",
       " '1709859466_0.jpg',\n",
       " 'F1_1_1_2.ts_f_1000.jpg',\n",
       " 'F2_2_1_1.ts_f_500.jpg',\n",
       " '1709101222_0.jpg',\n",
       " '1711256271_0.jpg',\n",
       " '1709875666_0.jpg',\n",
       " '1717059450_0.jpg',\n",
       " 'F5_2_1_2.ts_f_1000.jpg',\n",
       " 'F4_1_1_1.ts_f_1000.jpg',\n",
       " 'F5_2_2_2.ts_f_500.jpg',\n",
       " '1709870253_0.jpg',\n",
       " 'F1_2_3_1.ts_f_1000.jpg',\n",
       " '1710381477_0.jpg',\n",
       " '1709388062_0.jpg',\n",
       " 'F4_2_2_1.ts_f_500.jpg',\n",
       " '1709116441_0.jpg',\n",
       " 'F1_2_5_1.ts_f_1000.jpg',\n",
       " '1711412853_0.jpg',\n",
       " '1710835053_0.jpg',\n",
       " '1710430072_0.jpg',\n",
       " 'F5_1_3_2.ts_f_1000.jpg',\n",
       " 'F5_2_1_2.ts_f_500.jpg',\n",
       " '1710813452_0.jpg',\n",
       " '1709117582_0.jpg',\n",
       " '1709232001_0.jpg',\n",
       " '1709886453_0.jpg',\n",
       " 'F5_1_1_2.ts_f_1000.jpg',\n",
       " '1717022950_0.jpg',\n",
       " '1709255402_0.jpg',\n",
       " 'F1_2_3_2.ts_f_500.jpg',\n",
       " '1710269853_0.jpg',\n",
       " 'F5_1_2_2.ts_f_500.jpg',\n",
       " '1717060391_0.jpg',\n",
       " 'F4_1_3_2.ts_f_500.jpg',\n",
       " 'F5_2_2_1.ts_f_500.jpg',\n",
       " '1710147478_0.jpg',\n",
       " '1709877464_0.jpg',\n",
       " '1711268876_0.jpg',\n",
       " '1709870265_0.jpg',\n",
       " 'F7_1_1_2.ts_f_1000.jpg',\n",
       " 'F1_2_5_2.ts_f_500.jpg',\n",
       " 'F4_2_2_2.ts_f_500.jpg',\n",
       " 'F2_2_3_1.ts_f_1000.jpg',\n",
       " 'F7_2_1_1.ts_f_500.jpg',\n",
       " 'F4_2_2_2.ts_f_1000.jpg',\n",
       " 'F5_1_3_1.ts_f_1000.jpg',\n",
       " 'F1_2_4_2.ts_f_1000.jpg',\n",
       " '1709884664_0.jpg',\n",
       " '1709868465_0.jpg',\n",
       " '1710811652_0.jpg',\n",
       " '1709262422_0.jpg',\n",
       " '1709882867_0.jpg',\n",
       " 'F4_1_3_2.ts_f_1000.jpg',\n",
       " 'F1_2_2_1.ts_f_500.jpg',\n",
       " 'F7_1_2_1.ts_f_500.jpg',\n",
       " 'F2_2_2_2.ts_f_1000.jpg',\n",
       " 'F2_2_3_2.ts_f_1000.jpg',\n",
       " '1715158095_0.jpg',\n",
       " '1709587678_0.jpg',\n",
       " 'F1_1_5_2.ts_f_1000.jpg',\n",
       " 'F5_2_3_2.ts_f_1000.jpg',\n",
       " '1709046334_0.jpg',\n",
       " '1709875661_0.jpg',\n",
       " 'F1_1_2_2.ts_f_1000.jpg',\n",
       " '1716017248_0.jpg',\n",
       " 'F1_1_3_1.ts_f_1000.jpg',\n",
       " '1711186053_0.jpg',\n",
       " '1711178872_0.jpg',\n",
       " '1711459680_0.jpg',\n",
       " '1710809853_0.jpg',\n",
       " 'F4_1_3_1.ts_f_500.jpg',\n",
       " '1710273453_0.jpg',\n",
       " 'F5_1_1_1.ts_f_500.jpg',\n",
       " 'F4_2_3_1.ts_f_500.jpg',\n",
       " '1712741269_0.jpg',\n",
       " 'gr1.jpg',\n",
       " '1709261462_0.jpg',\n",
       " 'F2_2_2_2.ts_f_500.jpg',\n",
       " '1710268053_0.jpg',\n",
       " '1711268872_0.jpg',\n",
       " '1709253121_0.jpg',\n",
       " '1710847680_0.jpg',\n",
       " 'F2_1_2_2.ts_f_1000.jpg',\n",
       " '1711184253_0.jpg',\n",
       " 'synt100_1710813452_0.png',\n",
       " 'synt101_1710835053_0.png',\n",
       " 'synt103_1711186053_0.png',\n",
       " 'synt105_1712788070_0.png',\n",
       " 'synt106_1715112967_0.png',\n",
       " 'synt107_1717060391_0.png',\n",
       " 'synt10_1709255042_0.png',\n",
       " 'synt11_1709260082_0.png',\n",
       " 'synt12_1709260681_0.png',\n",
       " 'synt13_1709261462_0.png',\n",
       " 'synt14_1709262422_0.png',\n",
       " 'synt15_1709265902_0.png',\n",
       " 'synt16_1709265962_0.png',\n",
       " 'synt17_1709276582_0.png',\n",
       " 'synt18_1709310482_0.png',\n",
       " 'synt1_1709104681_0.png',\n",
       " 'synt20_1709344621_0.png',\n",
       " 'synt21_1709388062_0.png',\n",
       " 'synt22_1709859454_0.png',\n",
       " 'synt23_1709859466_0.png',\n",
       " 'synt256_1709104681_0.png',\n",
       " 'synt257_1709109361_0.png',\n",
       " 'synt258_1709116441_0.png',\n",
       " 'synt259_1709186522_0.png',\n",
       " 'synt25_1709888254_0.png',\n",
       " 'synt260_1709187722_0.png',\n",
       " 'synt261_1709191202_0.png',\n",
       " 'synt262_1709232001_0.png',\n",
       " 'synt263_1709253121_0.png',\n",
       " 'synt264_1709254201_0.png',\n",
       " 'synt265_1709255042_0.png',\n",
       " 'synt266_1709260082_0.png',\n",
       " 'synt267_1709260681_0.png',\n",
       " 'synt268_1709261462_0.png',\n",
       " 'synt269_1709262422_0.png',\n",
       " 'synt26_1710079053_0.png',\n",
       " 'synt270_1709265902_0.png',\n",
       " 'synt271_1709265962_0.png',\n",
       " 'synt272_1709276582_0.png',\n",
       " 'synt273_1709310482_0.png',\n",
       " 'synt274_1709339881_0.png',\n",
       " 'synt275_1709344621_0.png',\n",
       " 'synt276_1709388062_0.png',\n",
       " 'synt277_1709859454_0.png',\n",
       " 'synt278_1709859466_0.png',\n",
       " 'synt27_1710253681_0.png',\n",
       " 'synt280_1709888254_0.png',\n",
       " 'synt282_1710253681_0.png',\n",
       " 'synt283_1710260853_0.png',\n",
       " 'synt284_1710262654_0.png',\n",
       " 'synt285_1710266253_0.png',\n",
       " 'synt287_1710809853_0.png',\n",
       " 'synt288_1710813452_0.png',\n",
       " 'synt28_1710260853_0.png',\n",
       " 'synt291_1711186053_0.png',\n",
       " 'synt293_1712788070_0.png',\n",
       " 'synt294_1715112967_0.png',\n",
       " 'synt295_1717060391_0.png',\n",
       " 'synt296_dirty1.png',\n",
       " 'synt29_1710262654_0.png',\n",
       " 'synt2_1709109361_0.png',\n",
       " 'synt30_1710266253_0.png',\n",
       " 'synt31_1710458871_0.png',\n",
       " 'synt32_1710809853_0.png',\n",
       " 'synt33_1710813452_0.png',\n",
       " 'synt34_1710835053_0.png',\n",
       " 'synt36_1711186053_0.png',\n",
       " 'synt38_1712788070_0.png',\n",
       " 'synt39_1715112967_0.png',\n",
       " 'synt3_1709116441_0.png',\n",
       " 'synt401_1709104681_0.png',\n",
       " 'synt402_1709109361_0.png',\n",
       " 'synt403_1709116441_0.png',\n",
       " 'synt404_1709186522_0.png',\n",
       " 'synt405_1709187722_0.png',\n",
       " 'synt406_1709191202_0.png',\n",
       " 'synt407_1709232001_0.png',\n",
       " 'synt408_1709253121_0.png',\n",
       " 'synt409_1709254201_0.png',\n",
       " 'synt40_1717060391_0.png',\n",
       " 'synt410_1709255042_0.png',\n",
       " 'synt411_1709260082_0.png',\n",
       " 'synt412_1709260681_0.png',\n",
       " 'synt413_1709261462_0.png',\n",
       " 'synt414_1709262422_0.png',\n",
       " 'synt415_1709265902_0.png',\n",
       " 'synt416_1709265962_0.png',\n",
       " 'synt417_1709276582_0.png',\n",
       " 'synt418_1709310482_0.png',\n",
       " 'synt419_1709339881_0.png',\n",
       " 'synt41_dirty1.png',\n",
       " 'synt420_1709344621_0.png',\n",
       " 'synt421_1709388062_0.png',\n",
       " 'synt422_1709859454_0.png',\n",
       " 'synt423_1709859466_0.png',\n",
       " 'synt425_1709888254_0.png',\n",
       " 'synt426_1710079053_0.png',\n",
       " 'synt427_1710253681_0.png',\n",
       " 'synt428_1710260853_0.png',\n",
       " 'synt429_1710262654_0.png',\n",
       " 'synt430_1710266253_0.png',\n",
       " 'synt431_1710458871_0.png',\n",
       " 'synt432_1710809853_0.png',\n",
       " 'synt433_1710813452_0.png',\n",
       " 'synt434_1710835053_0.png',\n",
       " 'synt436_1711186053_0.png',\n",
       " 'synt438_1712788070_0.png',\n",
       " 'synt439_1715112967_0.png',\n",
       " 'synt440_1717060391_0.png',\n",
       " 'synt441_dirty1.png',\n",
       " 'synt4_1709186522_0.png',\n",
       " 'synt5_1709187722_0.png',\n",
       " 'synt68_1709104681_0.png',\n",
       " 'synt69_1709109361_0.png',\n",
       " 'synt6_1709191202_0.png',\n",
       " 'synt70_1709116441_0.png',\n",
       " 'synt71_1709186522_0.png',\n",
       " 'synt72_1709187722_0.png',\n",
       " 'synt73_1709191202_0.png',\n",
       " 'synt74_1709232001_0.png',\n",
       " 'synt75_1709253121_0.png',\n",
       " 'synt76_1709254201_0.png',\n",
       " 'synt77_1709255042_0.png',\n",
       " 'synt78_1709260082_0.png',\n",
       " 'synt79_1709260681_0.png',\n",
       " 'synt7_1709232001_0.png',\n",
       " 'synt80_1709261462_0.png',\n",
       " 'synt81_1709262422_0.png',\n",
       " 'synt82_1709265902_0.png',\n",
       " 'synt83_1709265962_0.png',\n",
       " 'synt84_1709276582_0.png',\n",
       " 'synt85_1709310482_0.png',\n",
       " 'synt86_1709339881_0.png',\n",
       " 'synt87_1709344621_0.png',\n",
       " 'synt88_1709388062_0.png',\n",
       " 'synt89_1709859454_0.png',\n",
       " 'synt90_1709859466_0.png',\n",
       " 'synt91_1709886453_0.png',\n",
       " 'synt92_1709888254_0.png',\n",
       " 'synt93_1710079053_0.png',\n",
       " 'synt94_1710253681_0.png',\n",
       " 'synt95_1710260853_0.png',\n",
       " 'synt96_1710262654_0.png',\n",
       " 'synt97_1710266253_0.png',\n",
       " 'synt98_1710458871_0.png',\n",
       " 'synt99_1710809853_0.png',\n",
       " 'synt9_1709254201_0.png']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "728b00f2-960b-4a91-9ea3-30c9ab79cc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for synt_img in val_images:\n",
    "    img = synt_img[4:]\n",
    "    if 'synt' in synt_img:\n",
    "        shutil.copy(os.path.join(IMAGES_DIR2, img), os.path.join(OUTPUT_DIR, 'images/val', img))\n",
    "        mask_name = img.replace('.jpg', '.png')\n",
    "        convert_mask_to_yolo(os.path.join(MASKS_DIR2, mask_name), 'labels/val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d7767e7853d7b98",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасет успешно разбит и сохранен в структуре проекта.\n"
     ]
    }
   ],
   "source": [
    "data_yaml_content = f\"\"\"\n",
    "train: train_data/images/train\n",
    "val: train_data/images/val\n",
    "\n",
    "nc: 1  # Обновите количество классов (1 для загрязнения)\n",
    "names: ['contaminated']  # Обновите названия классов\n",
    "\"\"\"\n",
    "\n",
    "with open('data.yaml', 'w') as f:\n",
    "    f.write(data_yaml_content)\n",
    "\n",
    "print(\"Датасет успешно разбит и сохранен в структуре проекта.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8c29dc47-fcb0-4904-9692-162a2eb95f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images_names, masks_names, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (string): Directory with all the images.\n",
    "            mask_dir (string): Directory with all the masks.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        # self.image_dir = image_dir\n",
    "        # self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = images_names\n",
    "        self.masks = masks_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask\n",
    "        img = self.images[idx]\n",
    "        if 'synt' not in img:\n",
    "            img_path = os.path.join('/home/jovyan/nazar/123/123/misis_chill/new_photos/cv_open_dataset/open_img', self.images[idx])\n",
    "            mask_path = os.path.join('/home/jovyan/nazar/123/123/misis_chill/new_photos/cv_open_dataset/open_msk', self.images[idx].replace('.jpg', '.png'))\n",
    "        else:\n",
    "            img_path = os.path.join('/home/jovyan/nazar/123/123/misis_chill/new_photos/cv_synt_dataset/synt_img', self.images[idx][4:])\n",
    "            mask_path = os.path.join('/home/jovyan/nazar/123/123/misis_chill/new_photos/cv_synt_dataset/synt_msk', self.images[idx][4:].replace('.jpg', '.png'))\n",
    "            \n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # Assuming masks are grayscale\n",
    "\n",
    "        # Convert mask to numpy array and ensure it's binary (0 or 1)\n",
    "        mask = np.array(mask)\n",
    "        mask = np.where(mask > 0, 1, 0).astype(np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = Image.fromarray(mask) \n",
    "            mask = self.transform(mask)\n",
    "            # mask = mask.unsqueeze(0)  # Add channel dimension\n",
    "        if mask.dim() == 2:  # If mask is 2D, add a channel dimension\n",
    "            mask = mask.unsqueeze(0)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9b74dc9-23b4-433b-95d7-fb78803c9892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82d237c3-5cf9-4150-9466-b599e383846b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1717058869_0.jpg',\n",
       " 'F5_2_1_1.ts_f_1000.jpg',\n",
       " 'F7_1_1_1.ts_f_500.jpg',\n",
       " '1712564893_0.jpg',\n",
       " '1717063048_0.jpg',\n",
       " '1710262654_0.jpg',\n",
       " 'F5_1_2_1.ts_f_1000.jpg',\n",
       " 'F1_1_4_2.ts_f_500.jpg',\n",
       " 'F1_2_4_1.ts_f_500.jpg',\n",
       " 'F5_2_2_1.ts_f_1000.jpg',\n",
       " 'F5_2_3_1.ts_f_1000.jpg',\n",
       " 'F1_1_5_1.ts_f_500.jpg',\n",
       " '1711510053_0.jpg',\n",
       " '1710458871_0.jpg',\n",
       " '1713767248_0.jpg',\n",
       " 'F1_2_4_1.ts_f_1000.jpg',\n",
       " 'F7_2_1_1.ts_f_1000.jpg',\n",
       " '1711393084_0.jpg',\n",
       " 'F1_2_5_1.ts_f_500.jpg',\n",
       " 'F5_2_3_1.ts_f_500.jpg',\n",
       " '1710271653_0.jpg',\n",
       " 'F1_1_5_2.ts_f_500.jpg',\n",
       " '1711456067_0.jpg',\n",
       " '1710273470_0.jpg',\n",
       " 'F7_1_1_1.ts_f_1000.jpg',\n",
       " '1710399478_0.jpg',\n",
       " '1709278661_0.jpg',\n",
       " '1715041648_0.jpg',\n",
       " '1710370672_0.jpg',\n",
       " '1710604665_0.jpg',\n",
       " '1709104681_0.jpg',\n",
       " '1709881065_0.jpg',\n",
       " 'F2_2_2_1.ts_f_500.jpg',\n",
       " 'F5_2_2_2.ts_f_1000.jpg',\n",
       " 'F1_2_4_2.ts_f_500.jpg',\n",
       " '1710260853_0.jpg',\n",
       " '17_2.png',\n",
       " '1713350757_0.jpg',\n",
       " 'F5_1_1_1.ts_f_1000.jpg',\n",
       " 'F1_1_2_1.ts_f_500.jpg',\n",
       " '1709310482_0.jpg',\n",
       " '1709191202_0.jpg',\n",
       " '1709520200_0.jpg',\n",
       " '1709278599_0.jpg',\n",
       " '1709859454_0.jpg',\n",
       " 'F5_1_2_1.ts_f_500.jpg',\n",
       " '1709263081_0.jpg',\n",
       " '1710917853_0.jpg',\n",
       " 'F4_2_3_2.ts_f_500.jpg',\n",
       " '1711182453_0.jpg',\n",
       " '1709872064_0.jpg',\n",
       " '1711090673_0.jpg',\n",
       " 'F1_1_1_1.ts_f_1000.jpg',\n",
       " 'F4_1_2_2.ts_f_1000.jpg',\n",
       " '1710275253_0.jpg',\n",
       " '1709857666_0.jpg',\n",
       " '1711448853_0.jpg',\n",
       " 'F1_1_3_1.ts_f_500.jpg',\n",
       " '1710322071_0.jpg',\n",
       " 'F1_1_2_1.ts_f_1000.jpg',\n",
       " '1709271842_0.jpg',\n",
       " 'F7_1_2_1.ts_f_1000.jpg',\n",
       " 'F1_2_3_1.ts_f_500.jpg',\n",
       " 'F7_1_2_2.ts_f_1000.jpg',\n",
       " '1717068448_0.jpg',\n",
       " '1709046321_0.jpg',\n",
       " '1709080522_0.jpg',\n",
       " '1710277054_0.jpg',\n",
       " '1709864863_0.jpg',\n",
       " '1711180652_0.jpg',\n",
       " '1715074048_0.jpg',\n",
       " 'F1_1_2_2.ts_f_500.jpg',\n",
       " 'F7_2_1_2.ts_f_500.jpg',\n",
       " '1710815253_0.jpg',\n",
       " '1710584874_0.jpg',\n",
       " 'F2_1_1_1.ts_f_1000.jpg',\n",
       " 'F4_1_1_2.ts_f_1000.jpg',\n",
       " '1711178852_0.jpg',\n",
       " '1709550918_0.jpg',\n",
       " '1709893685_0.jpg',\n",
       " 'F1_2_2_2.ts_f_500.jpg',\n",
       " '17_1.png',\n",
       " '1709187722_0.jpg',\n",
       " 'F7_1_2_2.ts_f_500.jpg',\n",
       " 'F4_1_2_1.ts_f_500.jpg',\n",
       " '1709276582_0.jpg',\n",
       " 'F1_1_5_1.ts_f_1000.jpg',\n",
       " '1709103022_0.jpg',\n",
       " '1712788070_0.jpg',\n",
       " '1709339881_0.jpg',\n",
       " 'F1_1_1_1.ts_f_500.jpg',\n",
       " 'F2_1_1_2.ts_f_500.jpg',\n",
       " '1717390039_0.jpg',\n",
       " '1710079053_0.jpg',\n",
       " '1717061248_0.jpg',\n",
       " '1715112967_0.jpg',\n",
       " 'F5_2_1_1.ts_f_500.jpg',\n",
       " 'F4_2_2_1.ts_f_1000.jpg',\n",
       " '1710266253_0.jpg',\n",
       " 'F2_2_3_2.ts_f_500.jpg',\n",
       " '1710253681_0.jpg',\n",
       " '1709863065_0.jpg',\n",
       " 'F1_2_3_2.ts_f_1000.jpg',\n",
       " '1709855872_0.jpg',\n",
       " '1713302111_0.jpg',\n",
       " 'F1_1_4_2.ts_f_1000.jpg',\n",
       " '1709265902_0.jpg',\n",
       " 'F4_2_3_2.ts_f_1000.jpg',\n",
       " 'F2_2_1_2.ts_f_1000.jpg',\n",
       " '1710390473_0.jpg',\n",
       " '1709827069_0.jpg',\n",
       " 'F4_2_3_1.ts_f_1000.jpg',\n",
       " '1709866664_0.jpg',\n",
       " '1711009681_0.jpg',\n",
       " 'F2_2_3_1.ts_f_500.jpg',\n",
       " 'F1_2_2_2.ts_f_1000.jpg',\n",
       " '1709859466_0.jpg',\n",
       " 'F1_1_1_2.ts_f_1000.jpg',\n",
       " 'F2_2_1_1.ts_f_500.jpg',\n",
       " '1709101222_0.jpg',\n",
       " '1711256271_0.jpg',\n",
       " '1709875666_0.jpg',\n",
       " '1717059450_0.jpg',\n",
       " 'F5_2_1_2.ts_f_1000.jpg',\n",
       " 'F4_1_1_1.ts_f_1000.jpg',\n",
       " 'F5_2_2_2.ts_f_500.jpg',\n",
       " '1709870253_0.jpg',\n",
       " 'F1_2_3_1.ts_f_1000.jpg',\n",
       " '1710381477_0.jpg',\n",
       " '1709388062_0.jpg',\n",
       " 'F4_2_2_1.ts_f_500.jpg',\n",
       " '1709116441_0.jpg',\n",
       " 'F1_2_5_1.ts_f_1000.jpg',\n",
       " '1711412853_0.jpg',\n",
       " '1710835053_0.jpg',\n",
       " '1710430072_0.jpg',\n",
       " 'F5_1_3_2.ts_f_1000.jpg',\n",
       " 'F5_2_1_2.ts_f_500.jpg',\n",
       " '1710813452_0.jpg',\n",
       " '1709117582_0.jpg',\n",
       " '1709232001_0.jpg',\n",
       " '1709886453_0.jpg',\n",
       " 'F5_1_1_2.ts_f_1000.jpg',\n",
       " '1717022950_0.jpg',\n",
       " '1709255402_0.jpg',\n",
       " 'F1_2_3_2.ts_f_500.jpg',\n",
       " '1710269853_0.jpg',\n",
       " 'F5_1_2_2.ts_f_500.jpg',\n",
       " '1717060391_0.jpg',\n",
       " 'F4_1_3_2.ts_f_500.jpg',\n",
       " 'F5_2_2_1.ts_f_500.jpg',\n",
       " '1710147478_0.jpg',\n",
       " '1709877464_0.jpg',\n",
       " '1711268876_0.jpg',\n",
       " '1709870265_0.jpg',\n",
       " 'F7_1_1_2.ts_f_1000.jpg',\n",
       " 'F1_2_5_2.ts_f_500.jpg',\n",
       " 'F4_2_2_2.ts_f_500.jpg',\n",
       " 'F2_2_3_1.ts_f_1000.jpg',\n",
       " 'F7_2_1_1.ts_f_500.jpg',\n",
       " 'F4_2_2_2.ts_f_1000.jpg',\n",
       " 'F5_1_3_1.ts_f_1000.jpg',\n",
       " 'F1_2_4_2.ts_f_1000.jpg',\n",
       " '1709884664_0.jpg',\n",
       " '1709868465_0.jpg',\n",
       " '1710811652_0.jpg',\n",
       " '1709262422_0.jpg',\n",
       " '1709882867_0.jpg',\n",
       " 'F4_1_3_2.ts_f_1000.jpg',\n",
       " 'F1_2_2_1.ts_f_500.jpg',\n",
       " 'F7_1_2_1.ts_f_500.jpg',\n",
       " 'F2_2_2_2.ts_f_1000.jpg',\n",
       " 'F2_2_3_2.ts_f_1000.jpg',\n",
       " '1715158095_0.jpg',\n",
       " '1709587678_0.jpg',\n",
       " 'F1_1_5_2.ts_f_1000.jpg',\n",
       " 'F5_2_3_2.ts_f_1000.jpg',\n",
       " '1709046334_0.jpg',\n",
       " '1709875661_0.jpg',\n",
       " 'F1_1_2_2.ts_f_1000.jpg',\n",
       " '1716017248_0.jpg',\n",
       " 'F1_1_3_1.ts_f_1000.jpg',\n",
       " '1711186053_0.jpg',\n",
       " '1711178872_0.jpg',\n",
       " '1711459680_0.jpg',\n",
       " '1710809853_0.jpg',\n",
       " 'F4_1_3_1.ts_f_500.jpg',\n",
       " '1710273453_0.jpg',\n",
       " 'F5_1_1_1.ts_f_500.jpg',\n",
       " 'F4_2_3_1.ts_f_500.jpg',\n",
       " '1712741269_0.jpg',\n",
       " 'gr1.jpg',\n",
       " '1709261462_0.jpg',\n",
       " 'F2_2_2_2.ts_f_500.jpg',\n",
       " '1710268053_0.jpg',\n",
       " '1711268872_0.jpg',\n",
       " '1709253121_0.jpg',\n",
       " '1710847680_0.jpg',\n",
       " 'F2_1_2_2.ts_f_1000.jpg',\n",
       " '1711184253_0.jpg',\n",
       " 'synt100_1710813452_0.png',\n",
       " 'synt101_1710835053_0.png',\n",
       " 'synt103_1711186053_0.png',\n",
       " 'synt105_1712788070_0.png',\n",
       " 'synt106_1715112967_0.png',\n",
       " 'synt107_1717060391_0.png',\n",
       " 'synt10_1709255042_0.png',\n",
       " 'synt11_1709260082_0.png',\n",
       " 'synt12_1709260681_0.png',\n",
       " 'synt13_1709261462_0.png',\n",
       " 'synt14_1709262422_0.png',\n",
       " 'synt15_1709265902_0.png',\n",
       " 'synt16_1709265962_0.png',\n",
       " 'synt17_1709276582_0.png',\n",
       " 'synt18_1709310482_0.png',\n",
       " 'synt1_1709104681_0.png',\n",
       " 'synt20_1709344621_0.png',\n",
       " 'synt21_1709388062_0.png',\n",
       " 'synt22_1709859454_0.png',\n",
       " 'synt23_1709859466_0.png',\n",
       " 'synt256_1709104681_0.png',\n",
       " 'synt257_1709109361_0.png',\n",
       " 'synt258_1709116441_0.png',\n",
       " 'synt259_1709186522_0.png',\n",
       " 'synt25_1709888254_0.png',\n",
       " 'synt260_1709187722_0.png',\n",
       " 'synt261_1709191202_0.png',\n",
       " 'synt262_1709232001_0.png',\n",
       " 'synt263_1709253121_0.png',\n",
       " 'synt264_1709254201_0.png',\n",
       " 'synt265_1709255042_0.png',\n",
       " 'synt266_1709260082_0.png',\n",
       " 'synt267_1709260681_0.png',\n",
       " 'synt268_1709261462_0.png',\n",
       " 'synt269_1709262422_0.png',\n",
       " 'synt26_1710079053_0.png',\n",
       " 'synt270_1709265902_0.png',\n",
       " 'synt271_1709265962_0.png',\n",
       " 'synt272_1709276582_0.png',\n",
       " 'synt273_1709310482_0.png',\n",
       " 'synt274_1709339881_0.png',\n",
       " 'synt275_1709344621_0.png',\n",
       " 'synt276_1709388062_0.png',\n",
       " 'synt277_1709859454_0.png',\n",
       " 'synt278_1709859466_0.png',\n",
       " 'synt27_1710253681_0.png',\n",
       " 'synt280_1709888254_0.png',\n",
       " 'synt282_1710253681_0.png',\n",
       " 'synt283_1710260853_0.png',\n",
       " 'synt284_1710262654_0.png',\n",
       " 'synt285_1710266253_0.png',\n",
       " 'synt287_1710809853_0.png',\n",
       " 'synt288_1710813452_0.png',\n",
       " 'synt28_1710260853_0.png',\n",
       " 'synt291_1711186053_0.png',\n",
       " 'synt293_1712788070_0.png',\n",
       " 'synt294_1715112967_0.png',\n",
       " 'synt295_1717060391_0.png',\n",
       " 'synt296_dirty1.png',\n",
       " 'synt29_1710262654_0.png',\n",
       " 'synt2_1709109361_0.png',\n",
       " 'synt30_1710266253_0.png',\n",
       " 'synt31_1710458871_0.png',\n",
       " 'synt32_1710809853_0.png',\n",
       " 'synt33_1710813452_0.png',\n",
       " 'synt34_1710835053_0.png',\n",
       " 'synt36_1711186053_0.png',\n",
       " 'synt38_1712788070_0.png',\n",
       " 'synt39_1715112967_0.png',\n",
       " 'synt3_1709116441_0.png',\n",
       " 'synt401_1709104681_0.png',\n",
       " 'synt402_1709109361_0.png',\n",
       " 'synt403_1709116441_0.png',\n",
       " 'synt404_1709186522_0.png',\n",
       " 'synt405_1709187722_0.png',\n",
       " 'synt406_1709191202_0.png',\n",
       " 'synt407_1709232001_0.png',\n",
       " 'synt408_1709253121_0.png',\n",
       " 'synt409_1709254201_0.png',\n",
       " 'synt40_1717060391_0.png',\n",
       " 'synt410_1709255042_0.png',\n",
       " 'synt411_1709260082_0.png',\n",
       " 'synt412_1709260681_0.png',\n",
       " 'synt413_1709261462_0.png',\n",
       " 'synt414_1709262422_0.png',\n",
       " 'synt415_1709265902_0.png',\n",
       " 'synt416_1709265962_0.png',\n",
       " 'synt417_1709276582_0.png',\n",
       " 'synt418_1709310482_0.png',\n",
       " 'synt419_1709339881_0.png',\n",
       " 'synt41_dirty1.png',\n",
       " 'synt420_1709344621_0.png',\n",
       " 'synt421_1709388062_0.png',\n",
       " 'synt422_1709859454_0.png',\n",
       " 'synt423_1709859466_0.png',\n",
       " 'synt425_1709888254_0.png',\n",
       " 'synt426_1710079053_0.png',\n",
       " 'synt427_1710253681_0.png',\n",
       " 'synt428_1710260853_0.png',\n",
       " 'synt429_1710262654_0.png',\n",
       " 'synt430_1710266253_0.png',\n",
       " 'synt431_1710458871_0.png',\n",
       " 'synt432_1710809853_0.png',\n",
       " 'synt433_1710813452_0.png',\n",
       " 'synt434_1710835053_0.png',\n",
       " 'synt436_1711186053_0.png',\n",
       " 'synt438_1712788070_0.png',\n",
       " 'synt439_1715112967_0.png',\n",
       " 'synt440_1717060391_0.png',\n",
       " 'synt441_dirty1.png',\n",
       " 'synt4_1709186522_0.png',\n",
       " 'synt5_1709187722_0.png',\n",
       " 'synt68_1709104681_0.png',\n",
       " 'synt69_1709109361_0.png',\n",
       " 'synt6_1709191202_0.png',\n",
       " 'synt70_1709116441_0.png',\n",
       " 'synt71_1709186522_0.png',\n",
       " 'synt72_1709187722_0.png',\n",
       " 'synt73_1709191202_0.png',\n",
       " 'synt74_1709232001_0.png',\n",
       " 'synt75_1709253121_0.png',\n",
       " 'synt76_1709254201_0.png',\n",
       " 'synt77_1709255042_0.png',\n",
       " 'synt78_1709260082_0.png',\n",
       " 'synt79_1709260681_0.png',\n",
       " 'synt7_1709232001_0.png',\n",
       " 'synt80_1709261462_0.png',\n",
       " 'synt81_1709262422_0.png',\n",
       " 'synt82_1709265902_0.png',\n",
       " 'synt83_1709265962_0.png',\n",
       " 'synt84_1709276582_0.png',\n",
       " 'synt85_1709310482_0.png',\n",
       " 'synt86_1709339881_0.png',\n",
       " 'synt87_1709344621_0.png',\n",
       " 'synt88_1709388062_0.png',\n",
       " 'synt89_1709859454_0.png',\n",
       " 'synt90_1709859466_0.png',\n",
       " 'synt91_1709886453_0.png',\n",
       " 'synt92_1709888254_0.png',\n",
       " 'synt93_1710079053_0.png',\n",
       " 'synt94_1710253681_0.png',\n",
       " 'synt95_1710260853_0.png',\n",
       " 'synt96_1710262654_0.png',\n",
       " 'synt97_1710266253_0.png',\n",
       " 'synt98_1710458871_0.png',\n",
       " 'synt99_1710809853_0.png',\n",
       " 'synt9_1709254201_0.png']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9267beca-8603-485f-8306-5e09ecb0b219",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1709046321_0.png',\n",
       " '1709046334_0.png',\n",
       " '1709080522_0.png',\n",
       " '1709101222_0.png',\n",
       " '1709103022_0.png',\n",
       " '1709104681_0.png',\n",
       " '1709109361_0.png',\n",
       " '1709116441_0.png',\n",
       " '1709117582_0.png',\n",
       " '1709143887_0.png',\n",
       " '1709186522_0.png',\n",
       " '1709187722_0.png',\n",
       " '1709191202_0.png',\n",
       " '1709232001_0.png',\n",
       " '1709253121_0.png',\n",
       " '1709254201_0.png',\n",
       " '1709255042_0.png',\n",
       " '1709255402_0.png',\n",
       " '1709260082_0.png',\n",
       " '1709260681_0.png',\n",
       " '1709261462_0.png',\n",
       " '1709262422_0.png',\n",
       " '1709263081_0.png',\n",
       " '1709265902_0.png',\n",
       " '1709265962_0.png',\n",
       " '1709269442_0.png',\n",
       " '1709271842_0.png',\n",
       " '1709276582_0.png',\n",
       " '1709278599_0.png',\n",
       " '1709278661_0.png',\n",
       " '1709283396_0.png',\n",
       " '1709310482_0.png',\n",
       " '1709339881_0.png',\n",
       " '1709344621_0.png',\n",
       " '1709388062_0.png',\n",
       " '1709520200_0.png',\n",
       " '1709550918_0.png',\n",
       " '1709587678_0.png',\n",
       " '1709807287_0.png',\n",
       " '1709827069_0.png',\n",
       " '1709855872_0.png',\n",
       " '1709857666_0.png',\n",
       " '1709859454_0.png',\n",
       " '1709859466_0.png',\n",
       " '1709863065_0.png',\n",
       " '1709864860_0.png',\n",
       " '1709864863_0.png',\n",
       " '1709866664_0.png',\n",
       " '1709868465_0.png',\n",
       " '1709870253_0.png',\n",
       " '1709870265_0.png',\n",
       " '1709872064_0.png',\n",
       " '1709875661_0.png',\n",
       " '1709875666_0.png',\n",
       " '1709877464_0.png',\n",
       " '1709879265_0.png',\n",
       " '1709881065_0.png',\n",
       " '1709882867_0.png',\n",
       " '1709884664_0.png',\n",
       " '1709886453_0.png',\n",
       " '1709888254_0.png',\n",
       " '1709893685_0.png',\n",
       " '1710079053_0.png',\n",
       " '1710147478_0.png',\n",
       " '1710253681_0.png',\n",
       " '1710260853_0.png',\n",
       " '1710262654_0.png',\n",
       " '1710262674_0.png',\n",
       " '1710264453_0.png',\n",
       " '1710264471_0.png',\n",
       " '1710266253_0.png',\n",
       " '1710268053_0.png',\n",
       " '1710269853_0.png',\n",
       " '1710271653_0.png',\n",
       " '1710273453_0.png',\n",
       " '1710273470_0.png',\n",
       " '1710275253_0.png',\n",
       " '1710277054_0.png',\n",
       " '1710322071_0.png',\n",
       " '1710370672_0.png',\n",
       " '1710381477_0.png',\n",
       " '1710390473_0.png',\n",
       " '1710399478_0.png',\n",
       " '1710430072_0.png',\n",
       " '1710458871_0.png',\n",
       " '1710584874_0.png',\n",
       " '1710604665_0.png',\n",
       " '1710809853_0.png',\n",
       " '1710811652_0.png',\n",
       " '1710813452_0.png',\n",
       " '1710815253_0.png',\n",
       " '1710835053_0.png',\n",
       " '1710847680_0.png',\n",
       " '1710917853_0.png',\n",
       " '1711009681_0.png',\n",
       " '1711090673_0.png',\n",
       " '1711119479_0.png',\n",
       " '1711177052_0.png',\n",
       " '1711178852_0.png',\n",
       " '1711178872_0.png',\n",
       " '1711180652_0.png',\n",
       " '1711182453_0.png',\n",
       " '1711184253_0.png',\n",
       " '1711186053_0.png',\n",
       " '1711187868_0.png',\n",
       " '1711256271_0.png',\n",
       " '1711268872_0.png',\n",
       " '1711268876_0.png',\n",
       " '1711330053_0.png',\n",
       " '1711393084_0.png',\n",
       " '1711412853_0.png',\n",
       " '1711421860_0.png',\n",
       " '1711430853_0.png',\n",
       " '1711448853_0.png',\n",
       " '1711450664_0.png',\n",
       " '1711456067_0.png',\n",
       " '1711459680_0.png',\n",
       " '1711510053_0.png',\n",
       " '1712555862_0.png',\n",
       " '1712564893_0.png',\n",
       " '1712577488_0.png',\n",
       " '1712741269_0.png',\n",
       " '1712788070_0.png',\n",
       " '1713302111_0.png',\n",
       " '1713350757_0.png',\n",
       " '1713767248_0.png',\n",
       " '1715041648_0.png',\n",
       " '1715074048_0.png',\n",
       " '1715112967_0.png',\n",
       " '1715158095_0.png',\n",
       " '1716017248_0.png',\n",
       " '1717022950_0.png',\n",
       " '1717058869_0.png',\n",
       " '1717059450_0.png',\n",
       " '1717060391_0.png',\n",
       " '1717061248_0.png',\n",
       " '1717063048_0.png',\n",
       " '1717068448_0.png',\n",
       " '1717390039_0.png',\n",
       " '17_1.png',\n",
       " '17_2.png',\n",
       " '29_1.png',\n",
       " '29_2.png',\n",
       " 'F1_1_1_1.ts_f_1000.png',\n",
       " 'F1_1_1_1.ts_f_500.png',\n",
       " 'F1_1_1_2.ts_f_1000.png',\n",
       " 'F1_1_1_2.ts_f_500.png',\n",
       " 'F1_1_2_1.ts_f_1000.png',\n",
       " 'F1_1_2_1.ts_f_500.png',\n",
       " 'F1_1_2_2.ts_f_1000.png',\n",
       " 'F1_1_2_2.ts_f_500.png',\n",
       " 'F1_1_3_1.ts_f_1000.png',\n",
       " 'F1_1_3_1.ts_f_500.png',\n",
       " 'F1_1_4_2.ts_f_1000.png',\n",
       " 'F1_1_4_2.ts_f_500.png',\n",
       " 'F1_1_5_1.ts_f_1000.png',\n",
       " 'F1_1_5_1.ts_f_500.png',\n",
       " 'F1_1_5_2.ts_f_1000.png',\n",
       " 'F1_1_5_2.ts_f_500.png',\n",
       " 'F1_2_2_1.ts_f_1000.png',\n",
       " 'F1_2_2_1.ts_f_500.png',\n",
       " 'F1_2_2_2.ts_f_1000.png',\n",
       " 'F1_2_2_2.ts_f_500.png',\n",
       " 'F1_2_3_1.ts_f_1000.png',\n",
       " 'F1_2_3_1.ts_f_500.png',\n",
       " 'F1_2_3_2.ts_f_1000.png',\n",
       " 'F1_2_3_2.ts_f_500.png',\n",
       " 'F1_2_4_1.ts_f_1000.png',\n",
       " 'F1_2_4_1.ts_f_500.png',\n",
       " 'F1_2_4_2.ts_f_1000.png',\n",
       " 'F1_2_4_2.ts_f_500.png',\n",
       " 'F1_2_5_1.ts_f_1000.png',\n",
       " 'F1_2_5_1.ts_f_500.png',\n",
       " 'F1_2_5_2.ts_f_1000.png',\n",
       " 'F1_2_5_2.ts_f_500.png',\n",
       " 'F2_1_1_1.ts_f_1000.png',\n",
       " 'F2_1_1_1.ts_f_500.png',\n",
       " 'F2_1_1_2.ts_f_1000.png',\n",
       " 'F2_1_1_2.ts_f_500.png',\n",
       " 'F2_1_2_2.ts_f_1000.png',\n",
       " 'F2_1_2_2.ts_f_500.png',\n",
       " 'F2_2_1_1.ts_f_1000.png',\n",
       " 'F2_2_1_1.ts_f_500.png',\n",
       " 'F2_2_1_2.ts_f_1000.png',\n",
       " 'F2_2_1_2.ts_f_500.png',\n",
       " 'F2_2_2_1.ts_f_1000.png',\n",
       " 'F2_2_2_1.ts_f_500.png',\n",
       " 'F2_2_2_2.ts_f_1000.png',\n",
       " 'F2_2_2_2.ts_f_500.png',\n",
       " 'F2_2_3_1.ts_f_1000.png',\n",
       " 'F2_2_3_1.ts_f_500.png',\n",
       " 'F2_2_3_2.ts_f_1000.png',\n",
       " 'F2_2_3_2.ts_f_500.png',\n",
       " 'F4_1_1_1.ts_f_1000.png',\n",
       " 'F4_1_1_1.ts_f_500.png',\n",
       " 'F4_1_1_2.ts_f_1000.png',\n",
       " 'F4_1_1_2.ts_f_500.png',\n",
       " 'F4_1_2_1.ts_f_1000.png',\n",
       " 'F4_1_2_1.ts_f_500.png',\n",
       " 'F4_1_2_2.ts_f_1000.png',\n",
       " 'F4_1_2_2.ts_f_500.png',\n",
       " 'F4_1_3_1.ts_f_1000.png',\n",
       " 'F4_1_3_1.ts_f_500.png',\n",
       " 'F4_1_3_2.ts_f_1000.png',\n",
       " 'F4_1_3_2.ts_f_500.png',\n",
       " 'F4_2_2_1.ts_f_1000.png',\n",
       " 'F4_2_2_1.ts_f_500.png',\n",
       " 'F4_2_2_2.ts_f_1000.png',\n",
       " 'F4_2_2_2.ts_f_500.png',\n",
       " 'F4_2_3_1.ts_f_1000.png',\n",
       " 'F4_2_3_1.ts_f_500.png',\n",
       " 'F4_2_3_2.ts_f_1000.png',\n",
       " 'F4_2_3_2.ts_f_500.png',\n",
       " 'F5_1_1_1.ts_f_1000.png',\n",
       " 'F5_1_1_1.ts_f_500.png',\n",
       " 'F5_1_1_2.ts_f_1000.png',\n",
       " 'F5_1_1_2.ts_f_500.png',\n",
       " 'F5_1_2_1.ts_f_1000.png',\n",
       " 'F5_1_2_1.ts_f_500.png',\n",
       " 'F5_1_2_2.ts_f_1000.png',\n",
       " 'F5_1_2_2.ts_f_500.png',\n",
       " 'F5_1_3_1.ts_f_1000.png',\n",
       " 'F5_1_3_1.ts_f_500.png',\n",
       " 'F5_1_3_2.ts_f_1000.png',\n",
       " 'F5_1_3_2.ts_f_500.png',\n",
       " 'F5_2_1_1.ts_f_1000.png',\n",
       " 'F5_2_1_1.ts_f_500.png',\n",
       " 'F5_2_1_2.ts_f_1000.png',\n",
       " 'F5_2_1_2.ts_f_500.png',\n",
       " 'F5_2_2_1.ts_f_1000.png',\n",
       " 'F5_2_2_1.ts_f_500.png',\n",
       " 'F5_2_2_2.ts_f_1000.png',\n",
       " 'F5_2_2_2.ts_f_500.png',\n",
       " 'F5_2_3_1.ts_f_1000.png',\n",
       " 'F5_2_3_1.ts_f_500.png',\n",
       " 'F5_2_3_2.ts_f_1000.png',\n",
       " 'F5_2_3_2.ts_f_500.png',\n",
       " 'F7_1_1_1.ts_f_1000.png',\n",
       " 'F7_1_1_1.ts_f_500.png',\n",
       " 'F7_1_1_2.ts_f_1000.png',\n",
       " 'F7_1_1_2.ts_f_500.png',\n",
       " 'F7_1_2_1.ts_f_1000.png',\n",
       " 'F7_1_2_1.ts_f_500.png',\n",
       " 'F7_1_2_2.ts_f_1000.png',\n",
       " 'F7_1_2_2.ts_f_500.png',\n",
       " 'F7_2_1_1.ts_f_1000.png',\n",
       " 'F7_2_1_1.ts_f_500.png',\n",
       " 'F7_2_1_2.ts_f_1000.png',\n",
       " 'F7_2_1_2.ts_f_500.png',\n",
       " 'gr1.png',\n",
       " '100_1710813452_0.png',\n",
       " '101_1710835053_0.png',\n",
       " '103_1711186053_0.png',\n",
       " '105_1712788070_0.png',\n",
       " '106_1715112967_0.png',\n",
       " '107_1717060391_0.png',\n",
       " '10_1709255042_0.png',\n",
       " '11_1709260082_0.png',\n",
       " '12_1709260681_0.png',\n",
       " '13_1709261462_0.png',\n",
       " '14_1709262422_0.png',\n",
       " '15_1709265902_0.png',\n",
       " '16_1709265962_0.png',\n",
       " '17_1709276582_0.png',\n",
       " '18_1709310482_0.png',\n",
       " '1_1709104681_0.png',\n",
       " '20_1709344621_0.png',\n",
       " '21_1709388062_0.png',\n",
       " '22_1709859454_0.png',\n",
       " '23_1709859466_0.png',\n",
       " '256_1709104681_0.png',\n",
       " '257_1709109361_0.png',\n",
       " '258_1709116441_0.png',\n",
       " '259_1709186522_0.png',\n",
       " '25_1709888254_0.png',\n",
       " '260_1709187722_0.png',\n",
       " '261_1709191202_0.png',\n",
       " '262_1709232001_0.png',\n",
       " '263_1709253121_0.png',\n",
       " '264_1709254201_0.png',\n",
       " '265_1709255042_0.png',\n",
       " '266_1709260082_0.png',\n",
       " '267_1709260681_0.png',\n",
       " '268_1709261462_0.png',\n",
       " '269_1709262422_0.png',\n",
       " '26_1710079053_0.png',\n",
       " '270_1709265902_0.png',\n",
       " '271_1709265962_0.png',\n",
       " '272_1709276582_0.png',\n",
       " '273_1709310482_0.png',\n",
       " '274_1709339881_0.png',\n",
       " '275_1709344621_0.png',\n",
       " '276_1709388062_0.png',\n",
       " '277_1709859454_0.png',\n",
       " '278_1709859466_0.png',\n",
       " '27_1710253681_0.png',\n",
       " '280_1709888254_0.png',\n",
       " '282_1710253681_0.png',\n",
       " '283_1710260853_0.png',\n",
       " '284_1710262654_0.png',\n",
       " '285_1710266253_0.png',\n",
       " '287_1710809853_0.png',\n",
       " '288_1710813452_0.png',\n",
       " '28_1710260853_0.png',\n",
       " '291_1711186053_0.png',\n",
       " '293_1712788070_0.png',\n",
       " '294_1715112967_0.png',\n",
       " '295_1717060391_0.png',\n",
       " '296_dirty1.png',\n",
       " '29_1710262654_0.png',\n",
       " '2_1709109361_0.png',\n",
       " '30_1710266253_0.png',\n",
       " '31_1710458871_0.png',\n",
       " '32_1710809853_0.png',\n",
       " '33_1710813452_0.png',\n",
       " '34_1710835053_0.png',\n",
       " '36_1711186053_0.png',\n",
       " '38_1712788070_0.png',\n",
       " '39_1715112967_0.png',\n",
       " '3_1709116441_0.png',\n",
       " '401_1709104681_0.png',\n",
       " '402_1709109361_0.png',\n",
       " '403_1709116441_0.png',\n",
       " '404_1709186522_0.png',\n",
       " '405_1709187722_0.png',\n",
       " '406_1709191202_0.png',\n",
       " '407_1709232001_0.png',\n",
       " '408_1709253121_0.png',\n",
       " '409_1709254201_0.png',\n",
       " '40_1717060391_0.png',\n",
       " '410_1709255042_0.png',\n",
       " '411_1709260082_0.png',\n",
       " '412_1709260681_0.png',\n",
       " '413_1709261462_0.png',\n",
       " '414_1709262422_0.png',\n",
       " '415_1709265902_0.png',\n",
       " '416_1709265962_0.png',\n",
       " '417_1709276582_0.png',\n",
       " '418_1709310482_0.png',\n",
       " '419_1709339881_0.png',\n",
       " '41_dirty1.png',\n",
       " '420_1709344621_0.png',\n",
       " '421_1709388062_0.png',\n",
       " '422_1709859454_0.png',\n",
       " '423_1709859466_0.png',\n",
       " '425_1709888254_0.png',\n",
       " '426_1710079053_0.png',\n",
       " '427_1710253681_0.png',\n",
       " '428_1710260853_0.png',\n",
       " '429_1710262654_0.png',\n",
       " '430_1710266253_0.png',\n",
       " '431_1710458871_0.png',\n",
       " '432_1710809853_0.png',\n",
       " '433_1710813452_0.png',\n",
       " '434_1710835053_0.png',\n",
       " '436_1711186053_0.png',\n",
       " '438_1712788070_0.png',\n",
       " '439_1715112967_0.png',\n",
       " '440_1717060391_0.png',\n",
       " '441_dirty1.png',\n",
       " '4_1709186522_0.png',\n",
       " '5_1709187722_0.png',\n",
       " '68_1709104681_0.png',\n",
       " '69_1709109361_0.png',\n",
       " '6_1709191202_0.png',\n",
       " '70_1709116441_0.png',\n",
       " '71_1709186522_0.png',\n",
       " '72_1709187722_0.png',\n",
       " '73_1709191202_0.png',\n",
       " '74_1709232001_0.png',\n",
       " '75_1709253121_0.png',\n",
       " '76_1709254201_0.png',\n",
       " '77_1709255042_0.png',\n",
       " '78_1709260082_0.png',\n",
       " '79_1709260681_0.png',\n",
       " '7_1709232001_0.png',\n",
       " '80_1709261462_0.png',\n",
       " '81_1709262422_0.png',\n",
       " '82_1709265902_0.png',\n",
       " '83_1709265962_0.png',\n",
       " '84_1709276582_0.png',\n",
       " '85_1709310482_0.png',\n",
       " '86_1709339881_0.png',\n",
       " '87_1709344621_0.png',\n",
       " '88_1709388062_0.png',\n",
       " '89_1709859454_0.png',\n",
       " '90_1709859466_0.png',\n",
       " '91_1709886453_0.png',\n",
       " '92_1709888254_0.png',\n",
       " '93_1710079053_0.png',\n",
       " '94_1710253681_0.png',\n",
       " '95_1710260853_0.png',\n",
       " '96_1710262654_0.png',\n",
       " '97_1710266253_0.png',\n",
       " '98_1710458871_0.png',\n",
       " '99_1710809853_0.png',\n",
       " '9_1709254201_0.png']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "bcea64cb-ff78-4d16-9649-631c26d21fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = smp.Unet(encoder_name=\"mobilenet_v2\", encoder_weights=\"imagenet\", in_channels=3, classes=1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2129dfca-89c2-4ac9-8cee-7c5d492c76d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4a14f4a8-4678-4130-bba1-f759f586657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize images to 256x256\n",
    "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9886bea3-9aad-4029-99f2-9914ab93567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SegmentationDataset(train_images, mask_files, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "672e30a9-3133-4c62-8734-054ca49cbe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SegmentationDataset(val_images, mask_files, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3cd1b5ef-52a7-46a1-ae07-563b31b7db7d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1709046321_0.png',\n",
       " '1709046334_0.png',\n",
       " '1709080522_0.png',\n",
       " '1709101222_0.png',\n",
       " '1709103022_0.png',\n",
       " '1709104681_0.png',\n",
       " '1709109361_0.png',\n",
       " '1709116441_0.png',\n",
       " '1709117582_0.png',\n",
       " '1709143887_0.png',\n",
       " '1709186522_0.png',\n",
       " '1709187722_0.png',\n",
       " '1709191202_0.png',\n",
       " '1709232001_0.png',\n",
       " '1709253121_0.png',\n",
       " '1709254201_0.png',\n",
       " '1709255042_0.png',\n",
       " '1709255402_0.png',\n",
       " '1709260082_0.png',\n",
       " '1709260681_0.png',\n",
       " '1709261462_0.png',\n",
       " '1709262422_0.png',\n",
       " '1709263081_0.png',\n",
       " '1709265902_0.png',\n",
       " '1709265962_0.png',\n",
       " '1709269442_0.png',\n",
       " '1709271842_0.png',\n",
       " '1709276582_0.png',\n",
       " '1709278599_0.png',\n",
       " '1709278661_0.png',\n",
       " '1709283396_0.png',\n",
       " '1709310482_0.png',\n",
       " '1709339881_0.png',\n",
       " '1709344621_0.png',\n",
       " '1709388062_0.png',\n",
       " '1709520200_0.png',\n",
       " '1709550918_0.png',\n",
       " '1709587678_0.png',\n",
       " '1709807287_0.png',\n",
       " '1709827069_0.png',\n",
       " '1709855872_0.png',\n",
       " '1709857666_0.png',\n",
       " '1709859454_0.png',\n",
       " '1709859466_0.png',\n",
       " '1709863065_0.png',\n",
       " '1709864860_0.png',\n",
       " '1709864863_0.png',\n",
       " '1709866664_0.png',\n",
       " '1709868465_0.png',\n",
       " '1709870253_0.png',\n",
       " '1709870265_0.png',\n",
       " '1709872064_0.png',\n",
       " '1709875661_0.png',\n",
       " '1709875666_0.png',\n",
       " '1709877464_0.png',\n",
       " '1709879265_0.png',\n",
       " '1709881065_0.png',\n",
       " '1709882867_0.png',\n",
       " '1709884664_0.png',\n",
       " '1709886453_0.png',\n",
       " '1709888254_0.png',\n",
       " '1709893685_0.png',\n",
       " '1710079053_0.png',\n",
       " '1710147478_0.png',\n",
       " '1710253681_0.png',\n",
       " '1710260853_0.png',\n",
       " '1710262654_0.png',\n",
       " '1710262674_0.png',\n",
       " '1710264453_0.png',\n",
       " '1710264471_0.png',\n",
       " '1710266253_0.png',\n",
       " '1710268053_0.png',\n",
       " '1710269853_0.png',\n",
       " '1710271653_0.png',\n",
       " '1710273453_0.png',\n",
       " '1710273470_0.png',\n",
       " '1710275253_0.png',\n",
       " '1710277054_0.png',\n",
       " '1710322071_0.png',\n",
       " '1710370672_0.png',\n",
       " '1710381477_0.png',\n",
       " '1710390473_0.png',\n",
       " '1710399478_0.png',\n",
       " '1710430072_0.png',\n",
       " '1710458871_0.png',\n",
       " '1710584874_0.png',\n",
       " '1710604665_0.png',\n",
       " '1710809853_0.png',\n",
       " '1710811652_0.png',\n",
       " '1710813452_0.png',\n",
       " '1710815253_0.png',\n",
       " '1710835053_0.png',\n",
       " '1710847680_0.png',\n",
       " '1710917853_0.png',\n",
       " '1711009681_0.png',\n",
       " '1711090673_0.png',\n",
       " '1711119479_0.png',\n",
       " '1711177052_0.png',\n",
       " '1711178852_0.png',\n",
       " '1711178872_0.png',\n",
       " '1711180652_0.png',\n",
       " '1711182453_0.png',\n",
       " '1711184253_0.png',\n",
       " '1711186053_0.png',\n",
       " '1711187868_0.png',\n",
       " '1711256271_0.png',\n",
       " '1711268872_0.png',\n",
       " '1711268876_0.png',\n",
       " '1711330053_0.png',\n",
       " '1711393084_0.png',\n",
       " '1711412853_0.png',\n",
       " '1711421860_0.png',\n",
       " '1711430853_0.png',\n",
       " '1711448853_0.png',\n",
       " '1711450664_0.png',\n",
       " '1711456067_0.png',\n",
       " '1711459680_0.png',\n",
       " '1711510053_0.png',\n",
       " '1712555862_0.png',\n",
       " '1712564893_0.png',\n",
       " '1712577488_0.png',\n",
       " '1712741269_0.png',\n",
       " '1712788070_0.png',\n",
       " '1713302111_0.png',\n",
       " '1713350757_0.png',\n",
       " '1713767248_0.png',\n",
       " '1715041648_0.png',\n",
       " '1715074048_0.png',\n",
       " '1715112967_0.png',\n",
       " '1715158095_0.png',\n",
       " '1716017248_0.png',\n",
       " '1717022950_0.png',\n",
       " '1717058869_0.png',\n",
       " '1717059450_0.png',\n",
       " '1717060391_0.png',\n",
       " '1717061248_0.png',\n",
       " '1717063048_0.png',\n",
       " '1717068448_0.png',\n",
       " '1717390039_0.png',\n",
       " '17_1.png',\n",
       " '17_2.png',\n",
       " '29_1.png',\n",
       " '29_2.png',\n",
       " 'F1_1_1_1.ts_f_1000.png',\n",
       " 'F1_1_1_1.ts_f_500.png',\n",
       " 'F1_1_1_2.ts_f_1000.png',\n",
       " 'F1_1_1_2.ts_f_500.png',\n",
       " 'F1_1_2_1.ts_f_1000.png',\n",
       " 'F1_1_2_1.ts_f_500.png',\n",
       " 'F1_1_2_2.ts_f_1000.png',\n",
       " 'F1_1_2_2.ts_f_500.png',\n",
       " 'F1_1_3_1.ts_f_1000.png',\n",
       " 'F1_1_3_1.ts_f_500.png',\n",
       " 'F1_1_4_2.ts_f_1000.png',\n",
       " 'F1_1_4_2.ts_f_500.png',\n",
       " 'F1_1_5_1.ts_f_1000.png',\n",
       " 'F1_1_5_1.ts_f_500.png',\n",
       " 'F1_1_5_2.ts_f_1000.png',\n",
       " 'F1_1_5_2.ts_f_500.png',\n",
       " 'F1_2_2_1.ts_f_1000.png',\n",
       " 'F1_2_2_1.ts_f_500.png',\n",
       " 'F1_2_2_2.ts_f_1000.png',\n",
       " 'F1_2_2_2.ts_f_500.png',\n",
       " 'F1_2_3_1.ts_f_1000.png',\n",
       " 'F1_2_3_1.ts_f_500.png',\n",
       " 'F1_2_3_2.ts_f_1000.png',\n",
       " 'F1_2_3_2.ts_f_500.png',\n",
       " 'F1_2_4_1.ts_f_1000.png',\n",
       " 'F1_2_4_1.ts_f_500.png',\n",
       " 'F1_2_4_2.ts_f_1000.png',\n",
       " 'F1_2_4_2.ts_f_500.png',\n",
       " 'F1_2_5_1.ts_f_1000.png',\n",
       " 'F1_2_5_1.ts_f_500.png',\n",
       " 'F1_2_5_2.ts_f_1000.png',\n",
       " 'F1_2_5_2.ts_f_500.png',\n",
       " 'F2_1_1_1.ts_f_1000.png',\n",
       " 'F2_1_1_1.ts_f_500.png',\n",
       " 'F2_1_1_2.ts_f_1000.png',\n",
       " 'F2_1_1_2.ts_f_500.png',\n",
       " 'F2_1_2_2.ts_f_1000.png',\n",
       " 'F2_1_2_2.ts_f_500.png',\n",
       " 'F2_2_1_1.ts_f_1000.png',\n",
       " 'F2_2_1_1.ts_f_500.png',\n",
       " 'F2_2_1_2.ts_f_1000.png',\n",
       " 'F2_2_1_2.ts_f_500.png',\n",
       " 'F2_2_2_1.ts_f_1000.png',\n",
       " 'F2_2_2_1.ts_f_500.png',\n",
       " 'F2_2_2_2.ts_f_1000.png',\n",
       " 'F2_2_2_2.ts_f_500.png',\n",
       " 'F2_2_3_1.ts_f_1000.png',\n",
       " 'F2_2_3_1.ts_f_500.png',\n",
       " 'F2_2_3_2.ts_f_1000.png',\n",
       " 'F2_2_3_2.ts_f_500.png',\n",
       " 'F4_1_1_1.ts_f_1000.png',\n",
       " 'F4_1_1_1.ts_f_500.png',\n",
       " 'F4_1_1_2.ts_f_1000.png',\n",
       " 'F4_1_1_2.ts_f_500.png',\n",
       " 'F4_1_2_1.ts_f_1000.png',\n",
       " 'F4_1_2_1.ts_f_500.png',\n",
       " 'F4_1_2_2.ts_f_1000.png',\n",
       " 'F4_1_2_2.ts_f_500.png',\n",
       " 'F4_1_3_1.ts_f_1000.png',\n",
       " 'F4_1_3_1.ts_f_500.png',\n",
       " 'F4_1_3_2.ts_f_1000.png',\n",
       " 'F4_1_3_2.ts_f_500.png',\n",
       " 'F4_2_2_1.ts_f_1000.png',\n",
       " 'F4_2_2_1.ts_f_500.png',\n",
       " 'F4_2_2_2.ts_f_1000.png',\n",
       " 'F4_2_2_2.ts_f_500.png',\n",
       " 'F4_2_3_1.ts_f_1000.png',\n",
       " 'F4_2_3_1.ts_f_500.png',\n",
       " 'F4_2_3_2.ts_f_1000.png',\n",
       " 'F4_2_3_2.ts_f_500.png',\n",
       " 'F5_1_1_1.ts_f_1000.png',\n",
       " 'F5_1_1_1.ts_f_500.png',\n",
       " 'F5_1_1_2.ts_f_1000.png',\n",
       " 'F5_1_1_2.ts_f_500.png',\n",
       " 'F5_1_2_1.ts_f_1000.png',\n",
       " 'F5_1_2_1.ts_f_500.png',\n",
       " 'F5_1_2_2.ts_f_1000.png',\n",
       " 'F5_1_2_2.ts_f_500.png',\n",
       " 'F5_1_3_1.ts_f_1000.png',\n",
       " 'F5_1_3_1.ts_f_500.png',\n",
       " 'F5_1_3_2.ts_f_1000.png',\n",
       " 'F5_1_3_2.ts_f_500.png',\n",
       " 'F5_2_1_1.ts_f_1000.png',\n",
       " 'F5_2_1_1.ts_f_500.png',\n",
       " 'F5_2_1_2.ts_f_1000.png',\n",
       " 'F5_2_1_2.ts_f_500.png',\n",
       " 'F5_2_2_1.ts_f_1000.png',\n",
       " 'F5_2_2_1.ts_f_500.png',\n",
       " 'F5_2_2_2.ts_f_1000.png',\n",
       " 'F5_2_2_2.ts_f_500.png',\n",
       " 'F5_2_3_1.ts_f_1000.png',\n",
       " 'F5_2_3_1.ts_f_500.png',\n",
       " 'F5_2_3_2.ts_f_1000.png',\n",
       " 'F5_2_3_2.ts_f_500.png',\n",
       " 'F7_1_1_1.ts_f_1000.png',\n",
       " 'F7_1_1_1.ts_f_500.png',\n",
       " 'F7_1_1_2.ts_f_1000.png',\n",
       " 'F7_1_1_2.ts_f_500.png',\n",
       " 'F7_1_2_1.ts_f_1000.png',\n",
       " 'F7_1_2_1.ts_f_500.png',\n",
       " 'F7_1_2_2.ts_f_1000.png',\n",
       " 'F7_1_2_2.ts_f_500.png',\n",
       " 'F7_2_1_1.ts_f_1000.png',\n",
       " 'F7_2_1_1.ts_f_500.png',\n",
       " 'F7_2_1_2.ts_f_1000.png',\n",
       " 'F7_2_1_2.ts_f_500.png',\n",
       " 'gr1.png',\n",
       " 'synt100_1710813452_0.png',\n",
       " 'synt101_1710835053_0.png',\n",
       " 'synt103_1711186053_0.png',\n",
       " 'synt105_1712788070_0.png',\n",
       " 'synt106_1715112967_0.png',\n",
       " 'synt107_1717060391_0.png',\n",
       " 'synt10_1709255042_0.png',\n",
       " 'synt11_1709260082_0.png',\n",
       " 'synt12_1709260681_0.png',\n",
       " 'synt13_1709261462_0.png',\n",
       " 'synt14_1709262422_0.png',\n",
       " 'synt15_1709265902_0.png',\n",
       " 'synt16_1709265962_0.png',\n",
       " 'synt17_1709276582_0.png',\n",
       " 'synt18_1709310482_0.png',\n",
       " 'synt1_1709104681_0.png',\n",
       " 'synt20_1709344621_0.png',\n",
       " 'synt21_1709388062_0.png',\n",
       " 'synt22_1709859454_0.png',\n",
       " 'synt23_1709859466_0.png',\n",
       " 'synt256_1709104681_0.png',\n",
       " 'synt257_1709109361_0.png',\n",
       " 'synt258_1709116441_0.png',\n",
       " 'synt259_1709186522_0.png',\n",
       " 'synt25_1709888254_0.png',\n",
       " 'synt260_1709187722_0.png',\n",
       " 'synt261_1709191202_0.png',\n",
       " 'synt262_1709232001_0.png',\n",
       " 'synt263_1709253121_0.png',\n",
       " 'synt264_1709254201_0.png',\n",
       " 'synt265_1709255042_0.png',\n",
       " 'synt266_1709260082_0.png',\n",
       " 'synt267_1709260681_0.png',\n",
       " 'synt268_1709261462_0.png',\n",
       " 'synt269_1709262422_0.png',\n",
       " 'synt26_1710079053_0.png',\n",
       " 'synt270_1709265902_0.png',\n",
       " 'synt271_1709265962_0.png',\n",
       " 'synt272_1709276582_0.png',\n",
       " 'synt273_1709310482_0.png',\n",
       " 'synt274_1709339881_0.png',\n",
       " 'synt275_1709344621_0.png',\n",
       " 'synt276_1709388062_0.png',\n",
       " 'synt277_1709859454_0.png',\n",
       " 'synt278_1709859466_0.png',\n",
       " 'synt27_1710253681_0.png',\n",
       " 'synt280_1709888254_0.png',\n",
       " 'synt282_1710253681_0.png',\n",
       " 'synt283_1710260853_0.png',\n",
       " 'synt284_1710262654_0.png',\n",
       " 'synt285_1710266253_0.png',\n",
       " 'synt287_1710809853_0.png',\n",
       " 'synt288_1710813452_0.png',\n",
       " 'synt28_1710260853_0.png',\n",
       " 'synt291_1711186053_0.png',\n",
       " 'synt293_1712788070_0.png',\n",
       " 'synt294_1715112967_0.png',\n",
       " 'synt295_1717060391_0.png',\n",
       " 'synt296_dirty1.png',\n",
       " 'synt29_1710262654_0.png',\n",
       " 'synt2_1709109361_0.png',\n",
       " 'synt30_1710266253_0.png',\n",
       " 'synt31_1710458871_0.png',\n",
       " 'synt32_1710809853_0.png',\n",
       " 'synt33_1710813452_0.png',\n",
       " 'synt34_1710835053_0.png',\n",
       " 'synt36_1711186053_0.png',\n",
       " 'synt38_1712788070_0.png',\n",
       " 'synt39_1715112967_0.png',\n",
       " 'synt3_1709116441_0.png',\n",
       " 'synt401_1709104681_0.png',\n",
       " 'synt402_1709109361_0.png',\n",
       " 'synt403_1709116441_0.png',\n",
       " 'synt404_1709186522_0.png',\n",
       " 'synt405_1709187722_0.png',\n",
       " 'synt406_1709191202_0.png',\n",
       " 'synt407_1709232001_0.png',\n",
       " 'synt408_1709253121_0.png',\n",
       " 'synt409_1709254201_0.png',\n",
       " 'synt40_1717060391_0.png',\n",
       " 'synt410_1709255042_0.png',\n",
       " 'synt411_1709260082_0.png',\n",
       " 'synt412_1709260681_0.png',\n",
       " 'synt413_1709261462_0.png',\n",
       " 'synt414_1709262422_0.png',\n",
       " 'synt415_1709265902_0.png',\n",
       " 'synt416_1709265962_0.png',\n",
       " 'synt417_1709276582_0.png',\n",
       " 'synt418_1709310482_0.png',\n",
       " 'synt419_1709339881_0.png',\n",
       " 'synt41_dirty1.png',\n",
       " 'synt420_1709344621_0.png',\n",
       " 'synt421_1709388062_0.png',\n",
       " 'synt422_1709859454_0.png',\n",
       " 'synt423_1709859466_0.png',\n",
       " 'synt425_1709888254_0.png',\n",
       " 'synt426_1710079053_0.png',\n",
       " 'synt427_1710253681_0.png',\n",
       " 'synt428_1710260853_0.png',\n",
       " 'synt429_1710262654_0.png',\n",
       " 'synt430_1710266253_0.png',\n",
       " 'synt431_1710458871_0.png',\n",
       " 'synt432_1710809853_0.png',\n",
       " 'synt433_1710813452_0.png',\n",
       " 'synt434_1710835053_0.png',\n",
       " 'synt436_1711186053_0.png',\n",
       " 'synt438_1712788070_0.png',\n",
       " 'synt439_1715112967_0.png',\n",
       " 'synt440_1717060391_0.png',\n",
       " 'synt441_dirty1.png',\n",
       " 'synt4_1709186522_0.png',\n",
       " 'synt5_1709187722_0.png',\n",
       " 'synt68_1709104681_0.png',\n",
       " 'synt69_1709109361_0.png',\n",
       " 'synt6_1709191202_0.png',\n",
       " 'synt70_1709116441_0.png',\n",
       " 'synt71_1709186522_0.png',\n",
       " 'synt72_1709187722_0.png',\n",
       " 'synt73_1709191202_0.png',\n",
       " 'synt74_1709232001_0.png',\n",
       " 'synt75_1709253121_0.png',\n",
       " 'synt76_1709254201_0.png',\n",
       " 'synt77_1709255042_0.png',\n",
       " 'synt78_1709260082_0.png',\n",
       " 'synt79_1709260681_0.png',\n",
       " 'synt7_1709232001_0.png',\n",
       " 'synt80_1709261462_0.png',\n",
       " 'synt81_1709262422_0.png',\n",
       " 'synt82_1709265902_0.png',\n",
       " 'synt83_1709265962_0.png',\n",
       " 'synt84_1709276582_0.png',\n",
       " 'synt85_1709310482_0.png',\n",
       " 'synt86_1709339881_0.png',\n",
       " 'synt87_1709344621_0.png',\n",
       " 'synt88_1709388062_0.png',\n",
       " 'synt89_1709859454_0.png',\n",
       " 'synt90_1709859466_0.png',\n",
       " 'synt91_1709886453_0.png',\n",
       " 'synt92_1709888254_0.png',\n",
       " 'synt93_1710079053_0.png',\n",
       " 'synt94_1710253681_0.png',\n",
       " 'synt95_1710260853_0.png',\n",
       " 'synt96_1710262654_0.png',\n",
       " 'synt97_1710266253_0.png',\n",
       " 'synt98_1710458871_0.png',\n",
       " 'synt99_1710809853_0.png',\n",
       " 'synt9_1709254201_0.png'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(set(list(map(lambda x: x.replace('jpg', 'png'), train_images))+list(map(lambda x: x.replace('jpg', 'png'), val_images))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7204ca5e-12a6-4503-8d30-2423a2a4e7e1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1709046321_0.png',\n",
       " '1709046334_0.png',\n",
       " '1709080522_0.png',\n",
       " '1709101222_0.png',\n",
       " '1709103022_0.png',\n",
       " '1709104681_0.png',\n",
       " '1709109361_0.png',\n",
       " '1709116441_0.png',\n",
       " '1709117582_0.png',\n",
       " '1709143887_0.png',\n",
       " '1709186522_0.png',\n",
       " '1709187722_0.png',\n",
       " '1709191202_0.png',\n",
       " '1709232001_0.png',\n",
       " '1709253121_0.png',\n",
       " '1709254201_0.png',\n",
       " '1709255042_0.png',\n",
       " '1709255402_0.png',\n",
       " '1709260082_0.png',\n",
       " '1709260681_0.png',\n",
       " '1709261462_0.png',\n",
       " '1709262422_0.png',\n",
       " '1709263081_0.png',\n",
       " '1709265902_0.png',\n",
       " '1709265962_0.png',\n",
       " '1709269442_0.png',\n",
       " '1709271842_0.png',\n",
       " '1709276582_0.png',\n",
       " '1709278599_0.png',\n",
       " '1709278661_0.png',\n",
       " '1709283396_0.png',\n",
       " '1709310482_0.png',\n",
       " '1709339881_0.png',\n",
       " '1709344621_0.png',\n",
       " '1709388062_0.png',\n",
       " '1709520200_0.png',\n",
       " '1709550918_0.png',\n",
       " '1709587678_0.png',\n",
       " '1709807287_0.png',\n",
       " '1709827069_0.png',\n",
       " '1709855872_0.png',\n",
       " '1709857666_0.png',\n",
       " '1709859454_0.png',\n",
       " '1709859466_0.png',\n",
       " '1709863065_0.png',\n",
       " '1709864860_0.png',\n",
       " '1709864863_0.png',\n",
       " '1709866664_0.png',\n",
       " '1709868465_0.png',\n",
       " '1709870253_0.png',\n",
       " '1709870265_0.png',\n",
       " '1709872064_0.png',\n",
       " '1709875661_0.png',\n",
       " '1709875666_0.png',\n",
       " '1709877464_0.png',\n",
       " '1709879265_0.png',\n",
       " '1709881065_0.png',\n",
       " '1709882867_0.png',\n",
       " '1709884664_0.png',\n",
       " '1709886453_0.png',\n",
       " '1709888254_0.png',\n",
       " '1709893685_0.png',\n",
       " '1710079053_0.png',\n",
       " '1710147478_0.png',\n",
       " '1710253681_0.png',\n",
       " '1710260853_0.png',\n",
       " '1710262654_0.png',\n",
       " '1710262674_0.png',\n",
       " '1710264453_0.png',\n",
       " '1710264471_0.png',\n",
       " '1710266253_0.png',\n",
       " '1710268053_0.png',\n",
       " '1710269853_0.png',\n",
       " '1710271653_0.png',\n",
       " '1710273453_0.png',\n",
       " '1710273470_0.png',\n",
       " '1710275253_0.png',\n",
       " '1710277054_0.png',\n",
       " '1710322071_0.png',\n",
       " '1710370672_0.png',\n",
       " '1710381477_0.png',\n",
       " '1710390473_0.png',\n",
       " '1710399478_0.png',\n",
       " '1710430072_0.png',\n",
       " '1710458871_0.png',\n",
       " '1710584874_0.png',\n",
       " '1710604665_0.png',\n",
       " '1710809853_0.png',\n",
       " '1710811652_0.png',\n",
       " '1710813452_0.png',\n",
       " '1710815253_0.png',\n",
       " '1710835053_0.png',\n",
       " '1710847680_0.png',\n",
       " '1710917853_0.png',\n",
       " '1711009681_0.png',\n",
       " '1711090673_0.png',\n",
       " '1711119479_0.png',\n",
       " '1711177052_0.png',\n",
       " '1711178852_0.png',\n",
       " '1711178872_0.png',\n",
       " '1711180652_0.png',\n",
       " '1711182453_0.png',\n",
       " '1711184253_0.png',\n",
       " '1711186053_0.png',\n",
       " '1711187868_0.png',\n",
       " '1711256271_0.png',\n",
       " '1711268872_0.png',\n",
       " '1711268876_0.png',\n",
       " '1711330053_0.png',\n",
       " '1711393084_0.png',\n",
       " '1711412853_0.png',\n",
       " '1711421860_0.png',\n",
       " '1711430853_0.png',\n",
       " '1711448853_0.png',\n",
       " '1711450664_0.png',\n",
       " '1711456067_0.png',\n",
       " '1711459680_0.png',\n",
       " '1711510053_0.png',\n",
       " '1712555862_0.png',\n",
       " '1712564893_0.png',\n",
       " '1712577488_0.png',\n",
       " '1712741269_0.png',\n",
       " '1712788070_0.png',\n",
       " '1713302111_0.png',\n",
       " '1713350757_0.png',\n",
       " '1713767248_0.png',\n",
       " '1715041648_0.png',\n",
       " '1715074048_0.png',\n",
       " '1715112967_0.png',\n",
       " '1715158095_0.png',\n",
       " '1716017248_0.png',\n",
       " '1717022950_0.png',\n",
       " '1717058869_0.png',\n",
       " '1717059450_0.png',\n",
       " '1717060391_0.png',\n",
       " '1717061248_0.png',\n",
       " '1717063048_0.png',\n",
       " '1717068448_0.png',\n",
       " '1717390039_0.png',\n",
       " '17_1.png',\n",
       " '17_2.png',\n",
       " '29_1.png',\n",
       " '29_2.png',\n",
       " 'F1_1_1_1.ts_f_1000.png',\n",
       " 'F1_1_1_1.ts_f_500.png',\n",
       " 'F1_1_1_2.ts_f_1000.png',\n",
       " 'F1_1_1_2.ts_f_500.png',\n",
       " 'F1_1_2_1.ts_f_1000.png',\n",
       " 'F1_1_2_1.ts_f_500.png',\n",
       " 'F1_1_2_2.ts_f_1000.png',\n",
       " 'F1_1_2_2.ts_f_500.png',\n",
       " 'F1_1_3_1.ts_f_1000.png',\n",
       " 'F1_1_3_1.ts_f_500.png',\n",
       " 'F1_1_4_2.ts_f_1000.png',\n",
       " 'F1_1_4_2.ts_f_500.png',\n",
       " 'F1_1_5_1.ts_f_1000.png',\n",
       " 'F1_1_5_1.ts_f_500.png',\n",
       " 'F1_1_5_2.ts_f_1000.png',\n",
       " 'F1_1_5_2.ts_f_500.png',\n",
       " 'F1_2_2_1.ts_f_1000.png',\n",
       " 'F1_2_2_1.ts_f_500.png',\n",
       " 'F1_2_2_2.ts_f_1000.png',\n",
       " 'F1_2_2_2.ts_f_500.png',\n",
       " 'F1_2_3_1.ts_f_1000.png',\n",
       " 'F1_2_3_1.ts_f_500.png',\n",
       " 'F1_2_3_2.ts_f_1000.png',\n",
       " 'F1_2_3_2.ts_f_500.png',\n",
       " 'F1_2_4_1.ts_f_1000.png',\n",
       " 'F1_2_4_1.ts_f_500.png',\n",
       " 'F1_2_4_2.ts_f_1000.png',\n",
       " 'F1_2_4_2.ts_f_500.png',\n",
       " 'F1_2_5_1.ts_f_1000.png',\n",
       " 'F1_2_5_1.ts_f_500.png',\n",
       " 'F1_2_5_2.ts_f_1000.png',\n",
       " 'F1_2_5_2.ts_f_500.png',\n",
       " 'F2_1_1_1.ts_f_1000.png',\n",
       " 'F2_1_1_1.ts_f_500.png',\n",
       " 'F2_1_1_2.ts_f_1000.png',\n",
       " 'F2_1_1_2.ts_f_500.png',\n",
       " 'F2_1_2_2.ts_f_1000.png',\n",
       " 'F2_1_2_2.ts_f_500.png',\n",
       " 'F2_2_1_1.ts_f_1000.png',\n",
       " 'F2_2_1_1.ts_f_500.png',\n",
       " 'F2_2_1_2.ts_f_1000.png',\n",
       " 'F2_2_1_2.ts_f_500.png',\n",
       " 'F2_2_2_1.ts_f_1000.png',\n",
       " 'F2_2_2_1.ts_f_500.png',\n",
       " 'F2_2_2_2.ts_f_1000.png',\n",
       " 'F2_2_2_2.ts_f_500.png',\n",
       " 'F2_2_3_1.ts_f_1000.png',\n",
       " 'F2_2_3_1.ts_f_500.png',\n",
       " 'F2_2_3_2.ts_f_1000.png',\n",
       " 'F2_2_3_2.ts_f_500.png',\n",
       " 'F4_1_1_1.ts_f_1000.png',\n",
       " 'F4_1_1_1.ts_f_500.png',\n",
       " 'F4_1_1_2.ts_f_1000.png',\n",
       " 'F4_1_1_2.ts_f_500.png',\n",
       " 'F4_1_2_1.ts_f_1000.png',\n",
       " 'F4_1_2_1.ts_f_500.png',\n",
       " 'F4_1_2_2.ts_f_1000.png',\n",
       " 'F4_1_2_2.ts_f_500.png',\n",
       " 'F4_1_3_1.ts_f_1000.png',\n",
       " 'F4_1_3_1.ts_f_500.png',\n",
       " 'F4_1_3_2.ts_f_1000.png',\n",
       " 'F4_1_3_2.ts_f_500.png',\n",
       " 'F4_2_2_1.ts_f_1000.png',\n",
       " 'F4_2_2_1.ts_f_500.png',\n",
       " 'F4_2_2_2.ts_f_1000.png',\n",
       " 'F4_2_2_2.ts_f_500.png',\n",
       " 'F4_2_3_1.ts_f_1000.png',\n",
       " 'F4_2_3_1.ts_f_500.png',\n",
       " 'F4_2_3_2.ts_f_1000.png',\n",
       " 'F4_2_3_2.ts_f_500.png',\n",
       " 'F5_1_1_1.ts_f_1000.png',\n",
       " 'F5_1_1_1.ts_f_500.png',\n",
       " 'F5_1_1_2.ts_f_1000.png',\n",
       " 'F5_1_1_2.ts_f_500.png',\n",
       " 'F5_1_2_1.ts_f_1000.png',\n",
       " 'F5_1_2_1.ts_f_500.png',\n",
       " 'F5_1_2_2.ts_f_1000.png',\n",
       " 'F5_1_2_2.ts_f_500.png',\n",
       " 'F5_1_3_1.ts_f_1000.png',\n",
       " 'F5_1_3_1.ts_f_500.png',\n",
       " 'F5_1_3_2.ts_f_1000.png',\n",
       " 'F5_1_3_2.ts_f_500.png',\n",
       " 'F5_2_1_1.ts_f_1000.png',\n",
       " 'F5_2_1_1.ts_f_500.png',\n",
       " 'F5_2_1_2.ts_f_1000.png',\n",
       " 'F5_2_1_2.ts_f_500.png',\n",
       " 'F5_2_2_1.ts_f_1000.png',\n",
       " 'F5_2_2_1.ts_f_500.png',\n",
       " 'F5_2_2_2.ts_f_1000.png',\n",
       " 'F5_2_2_2.ts_f_500.png',\n",
       " 'F5_2_3_1.ts_f_1000.png',\n",
       " 'F5_2_3_1.ts_f_500.png',\n",
       " 'F5_2_3_2.ts_f_1000.png',\n",
       " 'F5_2_3_2.ts_f_500.png',\n",
       " 'F7_1_1_1.ts_f_1000.png',\n",
       " 'F7_1_1_1.ts_f_500.png',\n",
       " 'F7_1_1_2.ts_f_1000.png',\n",
       " 'F7_1_1_2.ts_f_500.png',\n",
       " 'F7_1_2_1.ts_f_1000.png',\n",
       " 'F7_1_2_1.ts_f_500.png',\n",
       " 'F7_1_2_2.ts_f_1000.png',\n",
       " 'F7_1_2_2.ts_f_500.png',\n",
       " 'F7_2_1_1.ts_f_1000.png',\n",
       " 'F7_2_1_1.ts_f_500.png',\n",
       " 'F7_2_1_2.ts_f_1000.png',\n",
       " 'F7_2_1_2.ts_f_500.png',\n",
       " 'gr1.png',\n",
       " '100_1710813452_0.png',\n",
       " '101_1710835053_0.png',\n",
       " '103_1711186053_0.png',\n",
       " '105_1712788070_0.png',\n",
       " '106_1715112967_0.png',\n",
       " '107_1717060391_0.png',\n",
       " '10_1709255042_0.png',\n",
       " '11_1709260082_0.png',\n",
       " '12_1709260681_0.png',\n",
       " '13_1709261462_0.png',\n",
       " '14_1709262422_0.png',\n",
       " '15_1709265902_0.png',\n",
       " '16_1709265962_0.png',\n",
       " '17_1709276582_0.png',\n",
       " '18_1709310482_0.png',\n",
       " '1_1709104681_0.png',\n",
       " '20_1709344621_0.png',\n",
       " '21_1709388062_0.png',\n",
       " '22_1709859454_0.png',\n",
       " '23_1709859466_0.png',\n",
       " '256_1709104681_0.png',\n",
       " '257_1709109361_0.png',\n",
       " '258_1709116441_0.png',\n",
       " '259_1709186522_0.png',\n",
       " '25_1709888254_0.png',\n",
       " '260_1709187722_0.png',\n",
       " '261_1709191202_0.png',\n",
       " '262_1709232001_0.png',\n",
       " '263_1709253121_0.png',\n",
       " '264_1709254201_0.png',\n",
       " '265_1709255042_0.png',\n",
       " '266_1709260082_0.png',\n",
       " '267_1709260681_0.png',\n",
       " '268_1709261462_0.png',\n",
       " '269_1709262422_0.png',\n",
       " '26_1710079053_0.png',\n",
       " '270_1709265902_0.png',\n",
       " '271_1709265962_0.png',\n",
       " '272_1709276582_0.png',\n",
       " '273_1709310482_0.png',\n",
       " '274_1709339881_0.png',\n",
       " '275_1709344621_0.png',\n",
       " '276_1709388062_0.png',\n",
       " '277_1709859454_0.png',\n",
       " '278_1709859466_0.png',\n",
       " '27_1710253681_0.png',\n",
       " '280_1709888254_0.png',\n",
       " '282_1710253681_0.png',\n",
       " '283_1710260853_0.png',\n",
       " '284_1710262654_0.png',\n",
       " '285_1710266253_0.png',\n",
       " '287_1710809853_0.png',\n",
       " '288_1710813452_0.png',\n",
       " '28_1710260853_0.png',\n",
       " '291_1711186053_0.png',\n",
       " '293_1712788070_0.png',\n",
       " '294_1715112967_0.png',\n",
       " '295_1717060391_0.png',\n",
       " '296_dirty1.png',\n",
       " '29_1710262654_0.png',\n",
       " '2_1709109361_0.png',\n",
       " '30_1710266253_0.png',\n",
       " '31_1710458871_0.png',\n",
       " '32_1710809853_0.png',\n",
       " '33_1710813452_0.png',\n",
       " '34_1710835053_0.png',\n",
       " '36_1711186053_0.png',\n",
       " '38_1712788070_0.png',\n",
       " '39_1715112967_0.png',\n",
       " '3_1709116441_0.png',\n",
       " '401_1709104681_0.png',\n",
       " '402_1709109361_0.png',\n",
       " '403_1709116441_0.png',\n",
       " '404_1709186522_0.png',\n",
       " '405_1709187722_0.png',\n",
       " '406_1709191202_0.png',\n",
       " '407_1709232001_0.png',\n",
       " '408_1709253121_0.png',\n",
       " '409_1709254201_0.png',\n",
       " '40_1717060391_0.png',\n",
       " '410_1709255042_0.png',\n",
       " '411_1709260082_0.png',\n",
       " '412_1709260681_0.png',\n",
       " '413_1709261462_0.png',\n",
       " '414_1709262422_0.png',\n",
       " '415_1709265902_0.png',\n",
       " '416_1709265962_0.png',\n",
       " '417_1709276582_0.png',\n",
       " '418_1709310482_0.png',\n",
       " '419_1709339881_0.png',\n",
       " '41_dirty1.png',\n",
       " '420_1709344621_0.png',\n",
       " '421_1709388062_0.png',\n",
       " '422_1709859454_0.png',\n",
       " '423_1709859466_0.png',\n",
       " '425_1709888254_0.png',\n",
       " '426_1710079053_0.png',\n",
       " '427_1710253681_0.png',\n",
       " '428_1710260853_0.png',\n",
       " '429_1710262654_0.png',\n",
       " '430_1710266253_0.png',\n",
       " '431_1710458871_0.png',\n",
       " '432_1710809853_0.png',\n",
       " '433_1710813452_0.png',\n",
       " '434_1710835053_0.png',\n",
       " '436_1711186053_0.png',\n",
       " '438_1712788070_0.png',\n",
       " '439_1715112967_0.png',\n",
       " '440_1717060391_0.png',\n",
       " '441_dirty1.png',\n",
       " '4_1709186522_0.png',\n",
       " '5_1709187722_0.png',\n",
       " '68_1709104681_0.png',\n",
       " '69_1709109361_0.png',\n",
       " '6_1709191202_0.png',\n",
       " '70_1709116441_0.png',\n",
       " '71_1709186522_0.png',\n",
       " '72_1709187722_0.png',\n",
       " '73_1709191202_0.png',\n",
       " '74_1709232001_0.png',\n",
       " '75_1709253121_0.png',\n",
       " '76_1709254201_0.png',\n",
       " '77_1709255042_0.png',\n",
       " '78_1709260082_0.png',\n",
       " '79_1709260681_0.png',\n",
       " '7_1709232001_0.png',\n",
       " '80_1709261462_0.png',\n",
       " '81_1709262422_0.png',\n",
       " '82_1709265902_0.png',\n",
       " '83_1709265962_0.png',\n",
       " '84_1709276582_0.png',\n",
       " '85_1709310482_0.png',\n",
       " '86_1709339881_0.png',\n",
       " '87_1709344621_0.png',\n",
       " '88_1709388062_0.png',\n",
       " '89_1709859454_0.png',\n",
       " '90_1709859466_0.png',\n",
       " '91_1709886453_0.png',\n",
       " '92_1709888254_0.png',\n",
       " '93_1710079053_0.png',\n",
       " '94_1710253681_0.png',\n",
       " '95_1710260853_0.png',\n",
       " '96_1710262654_0.png',\n",
       " '97_1710266253_0.png',\n",
       " '98_1710458871_0.png',\n",
       " '99_1710809853_0.png',\n",
       " '9_1709254201_0.png']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5bf1f70f-03ab-4b60-89f4-9231733994c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mask_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "735f361e-8b44-42f8-85c2-e2100a018ec0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1717058869_0.jpg',\n",
       " 'F5_2_1_1.ts_f_1000.jpg',\n",
       " 'F7_1_1_1.ts_f_500.jpg',\n",
       " '1712564893_0.jpg',\n",
       " '1717063048_0.jpg',\n",
       " '1710262654_0.jpg',\n",
       " 'F5_1_2_1.ts_f_1000.jpg',\n",
       " 'F1_1_4_2.ts_f_500.jpg',\n",
       " 'F1_2_4_1.ts_f_500.jpg',\n",
       " 'F5_2_2_1.ts_f_1000.jpg',\n",
       " 'F5_2_3_1.ts_f_1000.jpg',\n",
       " 'F1_1_5_1.ts_f_500.jpg',\n",
       " '1711510053_0.jpg',\n",
       " '1710458871_0.jpg',\n",
       " '1713767248_0.jpg',\n",
       " 'F1_2_4_1.ts_f_1000.jpg',\n",
       " 'F7_2_1_1.ts_f_1000.jpg',\n",
       " '1711393084_0.jpg',\n",
       " 'F1_2_5_1.ts_f_500.jpg',\n",
       " 'F5_2_3_1.ts_f_500.jpg',\n",
       " '1710271653_0.jpg',\n",
       " 'F1_1_5_2.ts_f_500.jpg',\n",
       " '1711456067_0.jpg',\n",
       " '1710273470_0.jpg',\n",
       " 'F7_1_1_1.ts_f_1000.jpg',\n",
       " '1710399478_0.jpg',\n",
       " '1709278661_0.jpg',\n",
       " '1715041648_0.jpg',\n",
       " '1710370672_0.jpg',\n",
       " '1710604665_0.jpg',\n",
       " '1709104681_0.jpg',\n",
       " '1709881065_0.jpg',\n",
       " 'F2_2_2_1.ts_f_500.jpg',\n",
       " 'F5_2_2_2.ts_f_1000.jpg',\n",
       " 'F1_2_4_2.ts_f_500.jpg',\n",
       " '1710260853_0.jpg',\n",
       " '17_2.png',\n",
       " '1713350757_0.jpg',\n",
       " 'F5_1_1_1.ts_f_1000.jpg',\n",
       " 'F1_1_2_1.ts_f_500.jpg',\n",
       " '1709310482_0.jpg',\n",
       " '1709191202_0.jpg',\n",
       " '1709520200_0.jpg',\n",
       " '1709278599_0.jpg',\n",
       " '1709859454_0.jpg',\n",
       " 'F5_1_2_1.ts_f_500.jpg',\n",
       " '1709263081_0.jpg',\n",
       " '1710917853_0.jpg',\n",
       " 'F4_2_3_2.ts_f_500.jpg',\n",
       " '1711182453_0.jpg',\n",
       " '1709872064_0.jpg',\n",
       " '1711090673_0.jpg',\n",
       " 'F1_1_1_1.ts_f_1000.jpg',\n",
       " 'F4_1_2_2.ts_f_1000.jpg',\n",
       " '1710275253_0.jpg',\n",
       " '1709857666_0.jpg',\n",
       " '1711448853_0.jpg',\n",
       " 'F1_1_3_1.ts_f_500.jpg',\n",
       " '1710322071_0.jpg',\n",
       " 'F1_1_2_1.ts_f_1000.jpg',\n",
       " '1709271842_0.jpg',\n",
       " 'F7_1_2_1.ts_f_1000.jpg',\n",
       " 'F1_2_3_1.ts_f_500.jpg',\n",
       " 'F7_1_2_2.ts_f_1000.jpg',\n",
       " '1717068448_0.jpg',\n",
       " '1709046321_0.jpg',\n",
       " '1709080522_0.jpg',\n",
       " '1710277054_0.jpg',\n",
       " '1709864863_0.jpg',\n",
       " '1711180652_0.jpg',\n",
       " '1715074048_0.jpg',\n",
       " 'F1_1_2_2.ts_f_500.jpg',\n",
       " 'F7_2_1_2.ts_f_500.jpg',\n",
       " '1710815253_0.jpg',\n",
       " '1710584874_0.jpg',\n",
       " 'F2_1_1_1.ts_f_1000.jpg',\n",
       " 'F4_1_1_2.ts_f_1000.jpg',\n",
       " '1711178852_0.jpg',\n",
       " '1709550918_0.jpg',\n",
       " '1709893685_0.jpg',\n",
       " 'F1_2_2_2.ts_f_500.jpg',\n",
       " '17_1.png',\n",
       " '1709187722_0.jpg',\n",
       " 'F7_1_2_2.ts_f_500.jpg',\n",
       " 'F4_1_2_1.ts_f_500.jpg',\n",
       " '1709276582_0.jpg',\n",
       " 'F1_1_5_1.ts_f_1000.jpg',\n",
       " '1709103022_0.jpg',\n",
       " '1712788070_0.jpg',\n",
       " '1709339881_0.jpg',\n",
       " 'F1_1_1_1.ts_f_500.jpg',\n",
       " 'F2_1_1_2.ts_f_500.jpg',\n",
       " '1717390039_0.jpg',\n",
       " '1710079053_0.jpg',\n",
       " '1717061248_0.jpg',\n",
       " '1715112967_0.jpg',\n",
       " 'F5_2_1_1.ts_f_500.jpg',\n",
       " 'F4_2_2_1.ts_f_1000.jpg',\n",
       " '1710266253_0.jpg',\n",
       " 'F2_2_3_2.ts_f_500.jpg',\n",
       " '1710253681_0.jpg',\n",
       " '1709863065_0.jpg',\n",
       " 'F1_2_3_2.ts_f_1000.jpg',\n",
       " '1709855872_0.jpg',\n",
       " '1713302111_0.jpg',\n",
       " 'F1_1_4_2.ts_f_1000.jpg',\n",
       " '1709265902_0.jpg',\n",
       " 'F4_2_3_2.ts_f_1000.jpg',\n",
       " 'F2_2_1_2.ts_f_1000.jpg',\n",
       " '1710390473_0.jpg',\n",
       " '1709827069_0.jpg',\n",
       " 'F4_2_3_1.ts_f_1000.jpg',\n",
       " '1709866664_0.jpg',\n",
       " '1711009681_0.jpg',\n",
       " 'F2_2_3_1.ts_f_500.jpg',\n",
       " 'F1_2_2_2.ts_f_1000.jpg',\n",
       " '1709859466_0.jpg',\n",
       " 'F1_1_1_2.ts_f_1000.jpg',\n",
       " 'F2_2_1_1.ts_f_500.jpg',\n",
       " '1709101222_0.jpg',\n",
       " '1711256271_0.jpg',\n",
       " '1709875666_0.jpg',\n",
       " '1717059450_0.jpg',\n",
       " 'F5_2_1_2.ts_f_1000.jpg',\n",
       " 'F4_1_1_1.ts_f_1000.jpg',\n",
       " 'F5_2_2_2.ts_f_500.jpg',\n",
       " '1709870253_0.jpg',\n",
       " 'F1_2_3_1.ts_f_1000.jpg',\n",
       " '1710381477_0.jpg',\n",
       " '1709388062_0.jpg',\n",
       " 'F4_2_2_1.ts_f_500.jpg',\n",
       " '1709116441_0.jpg',\n",
       " 'F1_2_5_1.ts_f_1000.jpg',\n",
       " '1711412853_0.jpg',\n",
       " '1710835053_0.jpg',\n",
       " '1710430072_0.jpg',\n",
       " 'F5_1_3_2.ts_f_1000.jpg',\n",
       " 'F5_2_1_2.ts_f_500.jpg',\n",
       " '1710813452_0.jpg',\n",
       " '1709117582_0.jpg',\n",
       " '1709232001_0.jpg',\n",
       " '1709886453_0.jpg',\n",
       " 'F5_1_1_2.ts_f_1000.jpg',\n",
       " '1717022950_0.jpg',\n",
       " '1709255402_0.jpg',\n",
       " 'F1_2_3_2.ts_f_500.jpg',\n",
       " '1710269853_0.jpg',\n",
       " 'F5_1_2_2.ts_f_500.jpg',\n",
       " '1717060391_0.jpg',\n",
       " 'F4_1_3_2.ts_f_500.jpg',\n",
       " 'F5_2_2_1.ts_f_500.jpg',\n",
       " '1710147478_0.jpg',\n",
       " '1709877464_0.jpg',\n",
       " '1711268876_0.jpg',\n",
       " '1709870265_0.jpg',\n",
       " 'F7_1_1_2.ts_f_1000.jpg',\n",
       " 'F1_2_5_2.ts_f_500.jpg',\n",
       " 'F4_2_2_2.ts_f_500.jpg',\n",
       " 'F2_2_3_1.ts_f_1000.jpg',\n",
       " 'F7_2_1_1.ts_f_500.jpg',\n",
       " 'F4_2_2_2.ts_f_1000.jpg',\n",
       " 'F5_1_3_1.ts_f_1000.jpg',\n",
       " 'F1_2_4_2.ts_f_1000.jpg',\n",
       " '1709884664_0.jpg',\n",
       " '1709868465_0.jpg',\n",
       " '1710811652_0.jpg',\n",
       " '1709262422_0.jpg',\n",
       " '1709882867_0.jpg',\n",
       " 'F4_1_3_2.ts_f_1000.jpg',\n",
       " 'F1_2_2_1.ts_f_500.jpg',\n",
       " 'F7_1_2_1.ts_f_500.jpg',\n",
       " 'F2_2_2_2.ts_f_1000.jpg',\n",
       " 'F2_2_3_2.ts_f_1000.jpg',\n",
       " '1715158095_0.jpg',\n",
       " '1709587678_0.jpg',\n",
       " 'F1_1_5_2.ts_f_1000.jpg',\n",
       " 'F5_2_3_2.ts_f_1000.jpg',\n",
       " '1709046334_0.jpg',\n",
       " '1709875661_0.jpg',\n",
       " 'F1_1_2_2.ts_f_1000.jpg',\n",
       " '1716017248_0.jpg',\n",
       " 'F1_1_3_1.ts_f_1000.jpg',\n",
       " '1711186053_0.jpg',\n",
       " '1711178872_0.jpg',\n",
       " '1711459680_0.jpg',\n",
       " '1710809853_0.jpg',\n",
       " 'F4_1_3_1.ts_f_500.jpg',\n",
       " '1710273453_0.jpg',\n",
       " 'F5_1_1_1.ts_f_500.jpg',\n",
       " 'F4_2_3_1.ts_f_500.jpg',\n",
       " '1712741269_0.jpg',\n",
       " 'gr1.jpg',\n",
       " '1709261462_0.jpg',\n",
       " 'F2_2_2_2.ts_f_500.jpg',\n",
       " '1710268053_0.jpg',\n",
       " '1711268872_0.jpg',\n",
       " '1709253121_0.jpg',\n",
       " '1710847680_0.jpg',\n",
       " 'F2_1_2_2.ts_f_1000.jpg',\n",
       " '1711184253_0.jpg',\n",
       " 'synt100_1710813452_0.png',\n",
       " 'synt101_1710835053_0.png',\n",
       " 'synt103_1711186053_0.png',\n",
       " 'synt105_1712788070_0.png',\n",
       " 'synt106_1715112967_0.png',\n",
       " 'synt107_1717060391_0.png',\n",
       " 'synt10_1709255042_0.png',\n",
       " 'synt11_1709260082_0.png',\n",
       " 'synt12_1709260681_0.png',\n",
       " 'synt13_1709261462_0.png',\n",
       " 'synt14_1709262422_0.png',\n",
       " 'synt15_1709265902_0.png',\n",
       " 'synt16_1709265962_0.png',\n",
       " 'synt17_1709276582_0.png',\n",
       " 'synt18_1709310482_0.png',\n",
       " 'synt1_1709104681_0.png',\n",
       " 'synt20_1709344621_0.png',\n",
       " 'synt21_1709388062_0.png',\n",
       " 'synt22_1709859454_0.png',\n",
       " 'synt23_1709859466_0.png',\n",
       " 'synt256_1709104681_0.png',\n",
       " 'synt257_1709109361_0.png',\n",
       " 'synt258_1709116441_0.png',\n",
       " 'synt259_1709186522_0.png',\n",
       " 'synt25_1709888254_0.png',\n",
       " 'synt260_1709187722_0.png',\n",
       " 'synt261_1709191202_0.png',\n",
       " 'synt262_1709232001_0.png',\n",
       " 'synt263_1709253121_0.png',\n",
       " 'synt264_1709254201_0.png',\n",
       " 'synt265_1709255042_0.png',\n",
       " 'synt266_1709260082_0.png',\n",
       " 'synt267_1709260681_0.png',\n",
       " 'synt268_1709261462_0.png',\n",
       " 'synt269_1709262422_0.png',\n",
       " 'synt26_1710079053_0.png',\n",
       " 'synt270_1709265902_0.png',\n",
       " 'synt271_1709265962_0.png',\n",
       " 'synt272_1709276582_0.png',\n",
       " 'synt273_1709310482_0.png',\n",
       " 'synt274_1709339881_0.png',\n",
       " 'synt275_1709344621_0.png',\n",
       " 'synt276_1709388062_0.png',\n",
       " 'synt277_1709859454_0.png',\n",
       " 'synt278_1709859466_0.png',\n",
       " 'synt27_1710253681_0.png',\n",
       " 'synt280_1709888254_0.png',\n",
       " 'synt282_1710253681_0.png',\n",
       " 'synt283_1710260853_0.png',\n",
       " 'synt284_1710262654_0.png',\n",
       " 'synt285_1710266253_0.png',\n",
       " 'synt287_1710809853_0.png',\n",
       " 'synt288_1710813452_0.png',\n",
       " 'synt28_1710260853_0.png',\n",
       " 'synt291_1711186053_0.png',\n",
       " 'synt293_1712788070_0.png',\n",
       " 'synt294_1715112967_0.png',\n",
       " 'synt295_1717060391_0.png',\n",
       " 'synt296_dirty1.png',\n",
       " 'synt29_1710262654_0.png',\n",
       " 'synt2_1709109361_0.png',\n",
       " 'synt30_1710266253_0.png',\n",
       " 'synt31_1710458871_0.png',\n",
       " 'synt32_1710809853_0.png',\n",
       " 'synt33_1710813452_0.png',\n",
       " 'synt34_1710835053_0.png',\n",
       " 'synt36_1711186053_0.png',\n",
       " 'synt38_1712788070_0.png',\n",
       " 'synt39_1715112967_0.png',\n",
       " 'synt3_1709116441_0.png',\n",
       " 'synt401_1709104681_0.png',\n",
       " 'synt402_1709109361_0.png',\n",
       " 'synt403_1709116441_0.png',\n",
       " 'synt404_1709186522_0.png',\n",
       " 'synt405_1709187722_0.png',\n",
       " 'synt406_1709191202_0.png',\n",
       " 'synt407_1709232001_0.png',\n",
       " 'synt408_1709253121_0.png',\n",
       " 'synt409_1709254201_0.png',\n",
       " 'synt40_1717060391_0.png',\n",
       " 'synt410_1709255042_0.png',\n",
       " 'synt411_1709260082_0.png',\n",
       " 'synt412_1709260681_0.png',\n",
       " 'synt413_1709261462_0.png',\n",
       " 'synt414_1709262422_0.png',\n",
       " 'synt415_1709265902_0.png',\n",
       " 'synt416_1709265962_0.png',\n",
       " 'synt417_1709276582_0.png',\n",
       " 'synt418_1709310482_0.png',\n",
       " 'synt419_1709339881_0.png',\n",
       " 'synt41_dirty1.png',\n",
       " 'synt420_1709344621_0.png',\n",
       " 'synt421_1709388062_0.png',\n",
       " 'synt422_1709859454_0.png',\n",
       " 'synt423_1709859466_0.png',\n",
       " 'synt425_1709888254_0.png',\n",
       " 'synt426_1710079053_0.png',\n",
       " 'synt427_1710253681_0.png',\n",
       " 'synt428_1710260853_0.png',\n",
       " 'synt429_1710262654_0.png',\n",
       " 'synt430_1710266253_0.png',\n",
       " 'synt431_1710458871_0.png',\n",
       " 'synt432_1710809853_0.png',\n",
       " 'synt433_1710813452_0.png',\n",
       " 'synt434_1710835053_0.png',\n",
       " 'synt436_1711186053_0.png',\n",
       " 'synt438_1712788070_0.png',\n",
       " 'synt439_1715112967_0.png',\n",
       " 'synt440_1717060391_0.png',\n",
       " 'synt441_dirty1.png',\n",
       " 'synt4_1709186522_0.png',\n",
       " 'synt5_1709187722_0.png',\n",
       " 'synt68_1709104681_0.png',\n",
       " 'synt69_1709109361_0.png',\n",
       " 'synt6_1709191202_0.png',\n",
       " 'synt70_1709116441_0.png',\n",
       " 'synt71_1709186522_0.png',\n",
       " 'synt72_1709187722_0.png',\n",
       " 'synt73_1709191202_0.png',\n",
       " 'synt74_1709232001_0.png',\n",
       " 'synt75_1709253121_0.png',\n",
       " 'synt76_1709254201_0.png',\n",
       " 'synt77_1709255042_0.png',\n",
       " 'synt78_1709260082_0.png',\n",
       " 'synt79_1709260681_0.png',\n",
       " 'synt7_1709232001_0.png',\n",
       " 'synt80_1709261462_0.png',\n",
       " 'synt81_1709262422_0.png',\n",
       " 'synt82_1709265902_0.png',\n",
       " 'synt83_1709265962_0.png',\n",
       " 'synt84_1709276582_0.png',\n",
       " 'synt85_1709310482_0.png',\n",
       " 'synt86_1709339881_0.png',\n",
       " 'synt87_1709344621_0.png',\n",
       " 'synt88_1709388062_0.png',\n",
       " 'synt89_1709859454_0.png',\n",
       " 'synt90_1709859466_0.png',\n",
       " 'synt91_1709886453_0.png',\n",
       " 'synt92_1709888254_0.png',\n",
       " 'synt93_1710079053_0.png',\n",
       " 'synt94_1710253681_0.png',\n",
       " 'synt95_1710260853_0.png',\n",
       " 'synt96_1710262654_0.png',\n",
       " 'synt97_1710266253_0.png',\n",
       " 'synt98_1710458871_0.png',\n",
       " 'synt99_1710809853_0.png',\n",
       " 'synt9_1709254201_0.png']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6415c6e9-9fd2-4117-9ba4-17ad51c153e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1709046334_0.png', 'F5_2_1_1.ts_f_1000.jpg')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_files[1], train_images[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2dc6c831-01ff-4d81-9683-21ec1942fb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "742"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(mask_files+train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4046193c-f3d7-4b9d-b16b-4138cc5e2bbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256, 256])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "81a56a18-e7ef-4ce7-a44e-c4b2fae237bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9704124b-c760-42d8-bc0b-7c511739fbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = smp.Unet(encoder_name=\"resnet34\", encoder_weights=\"imagenet\", in_channels=3, classes=1).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Use appropriate loss for your task\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "eb6d301d-569f-43ee-b904-6831a2bf8730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "91e9e6e7-d9cb-4a75-a9a4-21c30f94b042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:08<00:00,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/11], Loss: 0.4378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:08<00:00,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/11], Loss: 0.2798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:07<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/11], Loss: 0.2269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:07<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/11], Loss: 0.2247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:08<00:00,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/11], Loss: 0.2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:08<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/11], Loss: 0.1835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:08<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/11], Loss: 0.1452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:08<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/11], Loss: 0.1771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:08<00:00,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/11], Loss: 0.1726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:08<00:00,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/11], Loss: 0.1390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:08<00:00,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/11], Loss: 0.1411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0963\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 11\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, masks in tqdm(train_loader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f'Validation Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2b267baf-5906-4c79-8f5f-2b0988cee885",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/home/jovyan/nazar/123/123/misis_chill/krutoy_dockerfile/baseline.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9a358695-7c3f-4b7a-8d5f-4074619111c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Unet' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[182], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jovyan/nazar/123/123/misis_chill/krutoy_dockerfile/baseline.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Unet' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "model.save('/home/jovyan/nazar/123/123/misis_chill/krutoy_dockerfile/baseline.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8ea7cc06-fa1a-46d0-af2c-80be79a66be3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "the number of columns changed from 457 to 271 at row 2; use `usecols` to select a subset and avoid this error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/labels/val/1_1709104681_0.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/numpy/lib/npyio.py:1373\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1371\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1373\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32m~/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/numpy/lib/npyio.py:1016\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     data \u001b[38;5;241m=\u001b[39m _preprocess_comments(data, comments, encoding)\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_dtype_via_object_chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1016\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_filelike\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimaginary_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimaginary_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiplines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilelike\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilelike\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbyte_converters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbyte_converters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;66;03m# This branch reads the file into chunks of object arrays and then\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;66;03m# casts them to the desired actual dtype.  This ensures correct\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;66;03m# string-length and datetime-unit discovery (like `arr.astype()`).\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m     \u001b[38;5;66;03m# Due to chunking, certain error reports are less clear, currently.\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filelike:\n",
      "\u001b[0;31mValueError\u001b[0m: the number of columns changed from 457 to 271 at row 2; use `usecols` to select a subset and avoid this error"
     ]
    }
   ],
   "source": [
    "np.loadtxt('/home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/labels/val/1_1709104681_0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f6047109-a49d-48f3-a20e-9f4690fe683b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mimage_list\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_list' is not defined"
     ]
    }
   ],
   "source": [
    "image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a73b014c45fcc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo11n-seg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f384455-df42-4b13-a5be-8d2f5b8cadd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nc': 80,\n",
       " 'scales': {'n': [0.5, 0.25, 1024],\n",
       "  's': [0.5, 0.5, 1024],\n",
       "  'm': [0.5, 1.0, 512],\n",
       "  'l': [1.0, 1.0, 512],\n",
       "  'x': [1.0, 1.5, 512]},\n",
       " 'backbone': [[-1, 1, 'Conv', [64, 3, 2]],\n",
       "  [-1, 1, 'Conv', [128, 3, 2]],\n",
       "  [-1, 2, 'C3k2', [256, False, 0.25]],\n",
       "  [-1, 1, 'Conv', [256, 3, 2]],\n",
       "  [-1, 2, 'C3k2', [512, False, 0.25]],\n",
       "  [-1, 1, 'Conv', [512, 3, 2]],\n",
       "  [-1, 2, 'C3k2', [512, True]],\n",
       "  [-1, 1, 'Conv', [1024, 3, 2]],\n",
       "  [-1, 2, 'C3k2', [1024, True]],\n",
       "  [-1, 1, 'SPPF', [1024, 5]],\n",
       "  [-1, 2, 'C2PSA', [1024]]],\n",
       " 'head': [[-1, 1, 'nn.Upsample', ['None', 2, 'nearest']],\n",
       "  [[-1, 6], 1, 'Concat', [1]],\n",
       "  [-1, 2, 'C3k2', [512, False]],\n",
       "  [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']],\n",
       "  [[-1, 4], 1, 'Concat', [1]],\n",
       "  [-1, 2, 'C3k2', [256, False]],\n",
       "  [-1, 1, 'Conv', [256, 3, 2]],\n",
       "  [[-1, 13], 1, 'Concat', [1]],\n",
       "  [-1, 2, 'C3k2', [512, False]],\n",
       "  [-1, 1, 'Conv', [512, 3, 2]],\n",
       "  [[-1, 10], 1, 'Concat', [1]],\n",
       "  [-1, 2, 'C3k2', [1024, True]],\n",
       "  [[16, 19, 22], 1, 'Segment', ['nc', 32, 256]]],\n",
       " 'scale': 'n',\n",
       " 'yaml_file': 'yolo11n-seg.yaml',\n",
       " 'ch': 3}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec0acc4-e6c5-4fc6-9686-6f2334cc53b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4d7d986b-fc45-4bf7-8b52-d84560535820",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/home/jovyan/nazar/123/123/misis_chill/custom.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e09be0ae-0ae7-4847-9a62-4187bf99d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.model.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "113b1560-dc6c-4e7a-bec6-a9ff032a1331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ade5ec11-9c95-4af8-9e2d-f19775fd8e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jovyan/nazar/123/123/misis_chill/custom_config.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "f3c5e120-87cd-452b-a685-81a11164ccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ no model scale passed. Assuming scale='l'.\n",
      "Transferred 134/1077 items from pretrained weights\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('/home/jovyan/nazar/123/123/misis_chill/custom_config.yaml').load('/home/jovyan/nazar/123/123/misis_chill/custom.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a23721b-1a73-44d5-8815-4bce1bb91e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv(\n",
       "  (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "  (act): SiLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d9841482-ab69-45b3-a57f-2cd4db100937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c09a256a-d522-43a8-ae46-c642bea92df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model[0].conv = nn.Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "375731ea-4c42-496a-bcc1-75e45ac58bb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SegmentationModel(\n",
       "  (model): Sequential(\n",
       "    (0): Conv(\n",
       "      (conv): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Conv(\n",
       "      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (2): C3k2(\n",
       "      (cv1): Conv(\n",
       "        (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (cv2): Conv(\n",
       "        (conv): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (m): ModuleList(\n",
       "        (0): Bottleneck(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(8, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (4): C3k2(\n",
       "      (cv1): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (cv2): Conv(\n",
       "        (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (m): ModuleList(\n",
       "        (0): Bottleneck(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): Conv(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (6): C3k2(\n",
       "      (cv1): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (cv2): Conv(\n",
       "        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (m): ModuleList(\n",
       "        (0): C3k(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): Conv(\n",
       "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (8): C3k2(\n",
       "      (cv1): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (cv2): Conv(\n",
       "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (m): ModuleList(\n",
       "        (0): C3k(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): SPPF(\n",
       "      (cv1): Conv(\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (cv2): Conv(\n",
       "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (10): C2PSA(\n",
       "      (cv1): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (cv2): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (m): Sequential(\n",
       "        (0): PSABlock(\n",
       "          (attn): Attention(\n",
       "            (qkv): Conv(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (proj): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (pe): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): Identity()\n",
       "            )\n",
       "          )\n",
       "          (ffn): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (12): Concat()\n",
       "    (13): C3k2(\n",
       "      (cv1): Conv(\n",
       "        (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (cv2): Conv(\n",
       "        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (m): ModuleList(\n",
       "        (0): Bottleneck(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (14): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (15): Concat()\n",
       "    (16): C3k2(\n",
       "      (cv1): Conv(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (cv2): Conv(\n",
       "        (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (m): ModuleList(\n",
       "        (0): Bottleneck(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (17): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (18): Concat()\n",
       "    (19): C3k2(\n",
       "      (cv1): Conv(\n",
       "        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (cv2): Conv(\n",
       "        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (m): ModuleList(\n",
       "        (0): Bottleneck(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (20): Conv(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (21): Concat()\n",
       "    (22): C3k2(\n",
       "      (cv1): Conv(\n",
       "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (cv2): Conv(\n",
       "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (m): ModuleList(\n",
       "        (0): C3k(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (23): Segment(\n",
       "      (cv2): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv(\n",
       "            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv(\n",
       "            (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (cv3): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): DWConv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): DWConv(\n",
       "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): DWConv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): DWConv(\n",
       "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): DWConv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): DWConv(\n",
       "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (dfl): DFL(\n",
       "        (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (proto): Proto(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (upsample): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv3): Conv(\n",
       "          (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cv4): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv(\n",
       "            (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv(\n",
       "            (conv): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4d85abfbc10ae302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.47 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.44 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (A100 80GB PCIe, 81252MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=segment, mode=train, model=yolo11n-seg.pt, data=./data.yaml, epochs=20, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=train11, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/segment/train11\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    683635  ultralytics.nn.modules.head.Segment          [1, 32, 64, [64, 128, 256]]   \n",
      "YOLO11n-seg summary: 355 layers, 2,842,803 parameters, 2,842,787 gradients, 10.4 GFLOPs\n",
      "\n",
      "Transferred 510/561 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/segment/train11', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/labels/train... 660 images, 517 backgrounds, 0 corrupt: 100%|██████████| 816/816 [00:01<00:00, 566.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/labels/train.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/labels/val... 230 images, 128 backgrounds, 0 corrupt: 100%|██████████| 230/230 [00:00<00:00, 332.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/labels/val.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/segment/train11/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 90 weight(decay=0.0), 101 weight(decay=0.0005), 100 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/segment/train11\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20      3.39G      1.559      3.105      2.579      1.386         54        640: 100%|██████████| 51/51 [00:06<00:00,  7.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:01<00:00,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975      0.757     0.0511      0.131      0.076      0.742     0.0501      0.127     0.0663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20      3.36G      1.346      2.288      1.639        1.2        118        640: 100%|██████████| 51/51 [00:05<00:00,  8.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:01<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975      0.535      0.492       0.44      0.239      0.534      0.491      0.428      0.221\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20      3.25G      1.318      2.204      1.459      1.201        171        640: 100%|██████████| 51/51 [00:05<00:00,  9.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:01<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975      0.772      0.558      0.631      0.366      0.782      0.569      0.637      0.334\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20      3.46G      1.306      2.096      1.381      1.205         93        640: 100%|██████████| 51/51 [00:05<00:00,  9.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:00<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975       0.73      0.602      0.658      0.401      0.737      0.599       0.65      0.324\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20      3.31G      1.242      2.035      1.237      1.173         88        640: 100%|██████████| 51/51 [00:05<00:00,  8.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:00<00:00,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975      0.839      0.689      0.762      0.512      0.839      0.689      0.752       0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20      3.38G      1.258      2.016      1.241      1.166        104        640: 100%|██████████| 51/51 [00:05<00:00,  9.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:00<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975      0.809      0.654      0.729      0.483      0.809      0.654      0.726      0.421\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20      3.36G       1.18      1.959      1.164      1.141        177        640: 100%|██████████| 51/51 [00:05<00:00,  9.49it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:00<00:00,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975      0.834      0.723      0.779      0.507      0.831       0.72      0.767      0.435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20       3.3G      1.168      1.923      1.109      1.148         70        640: 100%|██████████| 51/51 [00:05<00:00,  9.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:00<00:00,  8.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975      0.854      0.712      0.767      0.518      0.846      0.713      0.764      0.476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20      3.42G      1.108      1.815       1.02      1.095         88        640: 100%|██████████| 51/51 [00:05<00:00,  9.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:00<00:00,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975      0.876      0.746       0.81      0.559      0.884      0.731      0.801      0.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20       3.3G       1.09      1.797      1.009      1.096        106        640: 100%|██████████| 51/51 [00:05<00:00,  9.57it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:00<00:00,  8.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975      0.843      0.767      0.814      0.573      0.848      0.755      0.796      0.466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20      3.28G      1.083      1.711      2.009      1.097          9        640: 100%|██████████| 51/51 [00:05<00:00,  8.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:00<00:00,  8.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975      0.858      0.731      0.798      0.554      0.826      0.752      0.787      0.476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20      3.24G      1.121      1.778      1.107      1.118         92        640: 100%|██████████| 51/51 [00:04<00:00, 10.43it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:00<00:00,  8.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975      0.797      0.731      0.781      0.549      0.798      0.728      0.774      0.467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20      3.23G      1.048      1.715      1.094      1.093         63        640: 100%|██████████| 51/51 [00:04<00:00, 10.43it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:01<00:00,  7.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975       0.85      0.761      0.815      0.582      0.853       0.76       0.81      0.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20       3.3G      1.049      1.624     0.9599      1.092         40        640: 100%|██████████| 51/51 [00:04<00:00, 10.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:00<00:00,  8.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975      0.861      0.746      0.816      0.582      0.855       0.74      0.803      0.499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20      3.23G      1.053      1.658      2.089      1.062         16        640: 100%|██████████| 51/51 [00:04<00:00, 10.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:01<00:00,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975      0.884      0.753      0.831      0.601      0.878      0.747      0.818        0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20      3.27G     0.9996      1.583     0.9799      1.074         76        640: 100%|██████████| 51/51 [00:05<00:00,  9.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:00<00:00,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975      0.881      0.782      0.838      0.617      0.878      0.775      0.822       0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20      3.29G     0.9701       1.58     0.8502      1.053         84        640: 100%|██████████| 51/51 [00:05<00:00,  9.75it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:00<00:00,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975      0.882      0.803      0.854      0.639      0.908      0.778      0.843      0.549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20      3.29G      0.965      1.536     0.8409      1.037         20        640: 100%|██████████| 51/51 [00:05<00:00, 10.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:00<00:00,  8.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975        0.9      0.794      0.857      0.633      0.897      0.794       0.85      0.541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20      3.21G     0.8877      1.384     0.7692       1.01         83        640: 100%|██████████| 51/51 [00:05<00:00,  9.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:00<00:00,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975      0.884      0.823      0.864      0.652      0.903      0.809      0.856       0.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20       3.3G     0.8866      1.411      0.753      1.015         18        640: 100%|██████████| 51/51 [00:05<00:00, 10.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:00<00:00,  8.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975      0.894      0.831      0.871      0.661      0.891      0.828      0.864      0.557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 epochs completed in 0.041 hours.\n",
      "Optimizer stripped from runs/segment/train11/weights/last.pt, 6.0MB\n",
      "Optimizer stripped from runs/segment/train11/weights/best.pt, 6.0MB\n",
      "\n",
      "Validating runs/segment/train11/weights/best.pt...\n",
      "Ultralytics 8.3.44 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (A100 80GB PCIe, 81252MiB)\n",
      "YOLO11n-seg summary (fused): 265 layers, 2,834,763 parameters, 0 gradients, 10.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:02<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975      0.896      0.831      0.872      0.661      0.891       0.83      0.865      0.558\n",
      "Speed: 0.3ms preprocess, 0.8ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/train11\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_results = model.train(\n",
    "    data=\"./data.yaml\",  # path to dataset YAML\n",
    "    epochs=20,  # number of training epochs\n",
    "    imgsz=640,  # training image size\n",
    "    device=\"0\",  # device to run on, i.e. device=0 or device=0,1,2,3 or device=cpu\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2660d21c659b40fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.44 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (A100 80GB PCIe, 81252MiB)\n",
      "YOLO11n-seg summary (fused): 265 layers, 2,834,763 parameters, 0 gradients, 10.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/labels/val.cache... 230 images, 128 backgrounds, 0 corrupt: 100%|██████████| 230/230 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 15/15 [00:03<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        975      0.896      0.831      0.871      0.663      0.891      0.826      0.859      0.548\n",
      "Speed: 0.3ms preprocess, 1.6ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/train112\u001b[0m\n",
      "ultralytics.utils.metrics.SegmentMetrics object with attributes:\n",
      "\n",
      "ap_class_index: array([0])\n",
      "box: ultralytics.utils.metrics.Metric object\n",
      "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7fe0385f18d0>\n",
      "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)', 'Precision-Recall(M)', 'F1-Confidence(M)', 'Precision-Confidence(M)', 'Recall-Confidence(M)']\n",
      "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,\n",
      "             0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,\n",
      "             0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,      0.9951,\n",
      "             0.9951,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,\n",
      "            0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,\n",
      "            0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,     0.99267,\n",
      "            0.99267,     0.99267,     0.98955,     0.98955,     0.98955,     0.98955,     0.98955,     0.98955,     0.98955,     0.98955,     0.98955,     0.98955,     0.98955,     0.98955,     0.98955,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,\n",
      "            0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98738,     0.98715,\n",
      "            0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,\n",
      "            0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,\n",
      "            0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,     0.98715,\n",
      "            0.98715,     0.98715,     0.98715,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,\n",
      "            0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98575,     0.98434,     0.98434,     0.98434,     0.98434,     0.98434,     0.98434,     0.98434,     0.98434,     0.98434,     0.98434,     0.98434,\n",
      "            0.98434,     0.98434,     0.98434,     0.98434,     0.98434,     0.98434,     0.98434,     0.98434,     0.98434,     0.98434,     0.98434,     0.98434,     0.98434,     0.98434,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,\n",
      "             0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,      0.9833,\n",
      "            0.98204,     0.98204,     0.98204,     0.98204,     0.98204,     0.98204,     0.98204,     0.98204,     0.98204,     0.98204,     0.98204,     0.98204,     0.98204,     0.98204,     0.98204,     0.98204,     0.98204,     0.98204,     0.98204,     0.98204,     0.98204,     0.98204,     0.98047,\n",
      "            0.98047,     0.98047,     0.98047,     0.98047,     0.98047,     0.98047,     0.98047,     0.98047,     0.98047,     0.97901,     0.97901,     0.97901,     0.97901,     0.97901,     0.97901,     0.97901,     0.97901,     0.97901,     0.97901,     0.97901,     0.97753,     0.97753,     0.97753,\n",
      "            0.97753,     0.97753,     0.97753,     0.97753,     0.97753,     0.97753,     0.97623,     0.97623,     0.97623,     0.97623,     0.97623,     0.97623,     0.97623,     0.97623,     0.97623,     0.97623,     0.97623,     0.97623,     0.97623,     0.97477,     0.97477,     0.97477,     0.97477,\n",
      "            0.97477,     0.97477,     0.97477,     0.97396,     0.97396,     0.97396,     0.97396,     0.97396,     0.97396,     0.97396,     0.97396,     0.97396,     0.97396,     0.97396,     0.97396,     0.97396,     0.97396,     0.97396,     0.97396,     0.97396,     0.97396,     0.97396,     0.97396,\n",
      "            0.97284,     0.97284,     0.97284,     0.97284,     0.97284,     0.97284,     0.97284,     0.97284,     0.97284,     0.97284,     0.97284,     0.97284,     0.97284,     0.97157,     0.97157,     0.97157,     0.97157,     0.97157,     0.97157,     0.97157,     0.97157,     0.97097,     0.97097,\n",
      "            0.97097,     0.97097,     0.97097,     0.97097,     0.97097,     0.97097,     0.97097,     0.97097,     0.97097,     0.97097,     0.97097,     0.97097,     0.97097,     0.97097,     0.97097,     0.97097,     0.97097,     0.97097,     0.97097,     0.96984,     0.96984,     0.96984,     0.96984,\n",
      "            0.96984,     0.96984,     0.96984,     0.96984,     0.96984,     0.96984,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,\n",
      "             0.9697,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,      0.9697,     0.96828,     0.96702,     0.96702,     0.96702,     0.96702,     0.96567,     0.96567,     0.96486,     0.96486,     0.96486,     0.96486,\n",
      "            0.96486,     0.96486,     0.96486,     0.96486,     0.96486,     0.96486,     0.96486,     0.96486,     0.96486,     0.96221,     0.96221,     0.96221,     0.96137,     0.96137,     0.96137,     0.96137,     0.96137,     0.96137,     0.96137,     0.96137,     0.96137,     0.96137,     0.96011,\n",
      "            0.96011,      0.9591,      0.9591,      0.9591,      0.9591,      0.9591,      0.9591,     0.95798,     0.95798,     0.95798,     0.95798,     0.95759,     0.95759,     0.95759,     0.95759,     0.95759,     0.95759,     0.95759,     0.95759,     0.95759,     0.95759,     0.95759,     0.95759,\n",
      "            0.95759,     0.95759,     0.95759,     0.95759,     0.95759,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95376,     0.95376,\n",
      "            0.95376,     0.95376,     0.95376,     0.95376,     0.95269,     0.95269,     0.95269,     0.95189,     0.95189,     0.95189,     0.95189,     0.95189,     0.95189,     0.95189,     0.95189,     0.95109,     0.95109,     0.95109,     0.95109,     0.95109,     0.95109,     0.95109,     0.95013,\n",
      "            0.95013,     0.95013,     0.95013,     0.94904,     0.94904,     0.94677,     0.94677,     0.94571,     0.94571,     0.94486,     0.94486,     0.94486,     0.94486,     0.94486,     0.94375,     0.94271,     0.94271,     0.94161,     0.94052,     0.93951,     0.93951,     0.93842,     0.93505,\n",
      "            0.93407,     0.93407,     0.93309,     0.93309,     0.93229,     0.93229,     0.93229,     0.93229,     0.93012,     0.93012,     0.92926,     0.92926,     0.92926,     0.92601,       0.925,     0.92417,     0.92417,     0.92417,     0.92108,     0.92108,      0.9193,      0.9193,      0.9193,\n",
      "             0.9193,     0.91841,     0.91841,     0.91647,     0.91647,     0.91551,     0.91475,     0.91475,     0.91475,     0.91284,     0.91284,     0.91096,     0.91096,     0.90899,     0.90827,     0.90827,     0.90827,     0.90632,     0.90541,      0.9046,      0.9046,     0.90168,     0.89778,\n",
      "            0.89701,     0.89701,     0.89625,     0.89625,     0.89352,     0.89352,     0.89352,     0.89266,     0.89192,     0.89192,     0.89119,     0.89119,     0.89034,      0.8847,      0.8754,     0.85744,     0.85744,     0.85581,     0.85098,     0.85098,     0.85098,     0.84939,     0.84694,\n",
      "            0.84623,     0.84296,     0.82721,     0.82656,     0.81463,     0.80481,     0.80481,     0.79582,     0.79376,     0.78598,     0.78598,     0.78399,     0.78221,     0.78221,     0.74103,     0.74103,     0.74103,     0.74084,     0.74084,     0.73913,     0.72487,     0.72449,     0.72349,\n",
      "            0.71886,     0.71886,     0.69456,     0.68371,     0.68341,      0.6788,     0.67425,     0.65152,     0.64302,     0.63382,     0.62401,     0.59669,     0.59166,     0.57115,     0.57115,     0.56109,     0.54994,     0.54994,     0.51814,     0.51385,     0.43304,     0.42745,     0.42745,\n",
      "            0.41556,     0.40137,     0.40018,     0.38963,     0.38749,     0.36955,     0.35781,     0.34546,     0.29654,     0.28104,     0.25416,     0.24288,      0.1885,     0.17323,     0.16639,     0.12453,     0.10885,      0.1036,    0.095538,    0.095538,    0.095538,    0.089528,    0.069016,\n",
      "           0.067402,    0.063954,    0.063134,    0.062314,    0.061494,    0.060674,    0.059854,    0.059034,    0.058214,    0.057395,    0.056575,    0.055755,    0.054935,    0.054115,    0.053295,    0.052475,    0.051655,    0.050835,    0.050015,    0.049195,    0.048375,    0.047555,    0.046736,\n",
      "           0.045916,    0.045096,    0.044276,    0.043456,    0.042636,    0.041816,    0.040996,    0.040176,    0.039356,    0.038536,    0.037716,    0.036897,    0.036077,    0.035257,    0.034437,    0.033617,    0.032797,    0.031977,    0.031157,    0.030337,    0.029517,    0.028697,    0.027877,\n",
      "           0.027057,    0.026238,    0.025418,    0.024598,    0.023778,    0.022958,    0.022138,    0.021318,    0.020498,    0.019678,    0.018858,    0.018038,    0.017218,    0.016398,    0.015579,    0.014759,    0.013939,    0.013119,    0.012299,    0.011479,    0.010659,   0.0098391,   0.0090191,\n",
      "          0.0081992,   0.0073793,   0.0065594,   0.0057395,   0.0049195,   0.0040996,   0.0032797,   0.0024598,   0.0016398,  0.00081992,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.12089,     0.12099,     0.23153,     0.30777,     0.36228,     0.40524,     0.43984,     0.47137,     0.49726,     0.51412,     0.53144,     0.54607,     0.55859,     0.57226,     0.58164,     0.59322,     0.60232,     0.61004,     0.61794,     0.62783,     0.63527,     0.64029,     0.64927,\n",
      "            0.65545,     0.65906,     0.66404,     0.67054,     0.67619,     0.67895,     0.68244,     0.68722,     0.69162,     0.69536,     0.69805,     0.70038,     0.70471,     0.70871,     0.70981,      0.7126,     0.71448,     0.71706,     0.71989,     0.72189,     0.72337,     0.72794,     0.72957,\n",
      "            0.73072,     0.73314,     0.73644,     0.73819,     0.73936,     0.74069,     0.74208,     0.74363,     0.74597,      0.7477,     0.74862,     0.74925,     0.74955,     0.75088,       0.752,     0.75305,     0.75424,     0.75595,       0.757,      0.7592,     0.76158,     0.76326,     0.76404,\n",
      "            0.76553,     0.76619,     0.76735,     0.76843,     0.76896,     0.77054,     0.77237,     0.77395,     0.77484,     0.77593,     0.77707,     0.77834,     0.77874,      0.7798,      0.7815,     0.78238,     0.78406,     0.78445,     0.78484,     0.78622,     0.78686,     0.78808,     0.78877,\n",
      "            0.78921,     0.78951,     0.78935,     0.79133,     0.79161,     0.79193,     0.79147,     0.79162,     0.79177,     0.79175,     0.79187,     0.79185,     0.79239,     0.79301,     0.79362,     0.79377,      0.7939,     0.79417,     0.79524,     0.79561,     0.79596,     0.79643,     0.79689,\n",
      "            0.79748,     0.79817,     0.79964,     0.79984,     0.79971,     0.80023,     0.80053,     0.79933,     0.79874,     0.79902,     0.79922,     0.79974,     0.80155,     0.80164,     0.80173,     0.80183,     0.80202,     0.80396,     0.80446,     0.80517,     0.80566,     0.80694,     0.80741,\n",
      "            0.80797,      0.8085,     0.80871,     0.80901,      0.8095,     0.80983,     0.81057,     0.81089,     0.81111,     0.81274,     0.81297,     0.81325,     0.81438,     0.81626,     0.81667,      0.8169,     0.81729,      0.8175,     0.81758,     0.81767,     0.81775,     0.81791,      0.8192,\n",
      "            0.81949,     0.81961,     0.81973,     0.82054,     0.82139,     0.82083,     0.82157,     0.82169,     0.82131,     0.82178,     0.82193,     0.82208,     0.82239,     0.82138,     0.82174,     0.82355,     0.82418,     0.82445,     0.82513,     0.82571,     0.82549,     0.82527,     0.82547,\n",
      "            0.82615,     0.82606,     0.82607,     0.82627,     0.82636,     0.82644,     0.82652,     0.82661,      0.8269,     0.82729,     0.82761,     0.82797,     0.82818,     0.82852,     0.82893,     0.82934,     0.83002,     0.83019,     0.83035,     0.83071,     0.83043,     0.83089,     0.83101,\n",
      "            0.83112,     0.83124,     0.83225,     0.83278,     0.83299,     0.83308,     0.83317,     0.83327,     0.83351,     0.83418,     0.83446,     0.83488,     0.83454,     0.83471,     0.83507,     0.83533,     0.83549,     0.83566,     0.83613,     0.83634,     0.83655,     0.83675,     0.83702,\n",
      "            0.83745,     0.83758,     0.83771,      0.8381,     0.83942,     0.83959,     0.83974,      0.8399,     0.84016,     0.84055,     0.84015,     0.84052,     0.84026,     0.84072,     0.84094,     0.84108,     0.84122,     0.84142,     0.84163,     0.84191,     0.84241,     0.84271,     0.84291,\n",
      "             0.8444,      0.8455,     0.84583,     0.84698,     0.84731,     0.84737,     0.84744,      0.8475,     0.84757,     0.84763,     0.84773,     0.84792,     0.84811,     0.84782,     0.84754,     0.84786,     0.84801,     0.84808,     0.84815,     0.84823,      0.8483,     0.84837,     0.84889,\n",
      "            0.84905,      0.8492,     0.84915,     0.84897,     0.84879,     0.84882,     0.84896,     0.84857,     0.84904,      0.8495,     0.84977,     0.84956,     0.84924,     0.85009,     0.84976,     0.84939,     0.84896,     0.84841,     0.84891,     0.84917,     0.84964,     0.84978,     0.84992,\n",
      "            0.85006,     0.85053,     0.85071,     0.85089,     0.85062,     0.85048,     0.85073,     0.85106,     0.85076,     0.85008,     0.85014,     0.85021,     0.85028,     0.85035,     0.85041,     0.85048,     0.85099,     0.85117,     0.85135,     0.85147,     0.85158,     0.85169,      0.8518,\n",
      "            0.85324,      0.8536,     0.85388,     0.85421,      0.8547,     0.85499,      0.8551,     0.85521,     0.85531,     0.85607,     0.85653,     0.85693,     0.85769,     0.85809,      0.8584,     0.85853,     0.85859,     0.85865,     0.85871,     0.85877,     0.85883,     0.85889,     0.85884,\n",
      "             0.8585,     0.85868,     0.85903,     0.85942,     0.85979,     0.85997,     0.86014,     0.86024,     0.86034,     0.86044,     0.86054,     0.86108,     0.86118,     0.86128,     0.86138,     0.86147,     0.86163,     0.86179,     0.86197,     0.86229,     0.86263,     0.86275,      0.8629,\n",
      "            0.86306,     0.86373,     0.86407,     0.86416,     0.86425,     0.86434,     0.86442,     0.86451,     0.86478,     0.86479,     0.86441,     0.86453,     0.86464,     0.86476,     0.86373,     0.86345,     0.86332,     0.86319,     0.86306,     0.86293,     0.86292,     0.86297,     0.86302,\n",
      "            0.86307,     0.86312,     0.86317,     0.86321,     0.86326,     0.86331,     0.86308,     0.86291,     0.86315,     0.86244,     0.86217,     0.86203,     0.86215,     0.86226,     0.86238,     0.86281,      0.8631,     0.86331,     0.86267,      0.8625,     0.86233,     0.86217,     0.86248,\n",
      "            0.86206,     0.86173,     0.86141,     0.86154,     0.86168,     0.86182,     0.86177,     0.86164,      0.8615,     0.86136,     0.86143,     0.86228,     0.86246,     0.86263,     0.86273,     0.86283,     0.86293,     0.86303,     0.86297,     0.86301,     0.86357,     0.86293,     0.86284,\n",
      "            0.86305,     0.86306,     0.86298,     0.86289,      0.8628,     0.86271,     0.86262,     0.86253,      0.8626,     0.86272,     0.86284,     0.86295,     0.86244,     0.86273,     0.86289,     0.86299,     0.86309,     0.86319,     0.86329,     0.86161,     0.86131,     0.86155,     0.86178,\n",
      "             0.8621,      0.8615,     0.86108,     0.86121,     0.86146,     0.86192,     0.86173,     0.86154,     0.86135,     0.86119,     0.86103,     0.86087,     0.86071,     0.86143,     0.86091,     0.86068,     0.86044,     0.86036,     0.86031,     0.86025,      0.8602,     0.86015,      0.8601,\n",
      "            0.86004,     0.85999,     0.85994,     0.85989,     0.85983,      0.8598,     0.86018,      0.8597,     0.85971,     0.85979,     0.85988,     0.85997,     0.86005,     0.85973,     0.85926,     0.85892,     0.85899,     0.85916,     0.85932,     0.85974,     0.85966,     0.85959,     0.85952,\n",
      "            0.85945,     0.85937,      0.8593,     0.85923,     0.85913,      0.8589,     0.85867,     0.85888,     0.85885,     0.85865,     0.85844,     0.85773,     0.85762,     0.85751,      0.8574,     0.85729,     0.85718,     0.85699,     0.85677,     0.85655,      0.8569,     0.85724,     0.85732,\n",
      "            0.85685,     0.85648,     0.85627,     0.85656,     0.85693,     0.85721,     0.85733,     0.85745,     0.85757,      0.8574,     0.85707,     0.85611,      0.8558,     0.85603,     0.85619,     0.85615,     0.85611,     0.85607,     0.85603,     0.85599,     0.85595,     0.85591,     0.85587,\n",
      "            0.85583,     0.85579,     0.85575,     0.85571,     0.85568,     0.85564,      0.8556,     0.85602,     0.85585,     0.85568,     0.85551,     0.85556,     0.85593,     0.85634,     0.85551,     0.85497,     0.85532,     0.85491,     0.85512,     0.85559,       0.855,     0.85474,     0.85448,\n",
      "            0.85363,     0.85318,     0.85351,      0.8533,     0.85309,     0.85263,     0.85208,     0.85177,     0.85161,     0.85177,     0.85194,     0.85143,     0.85156,      0.8517,     0.85183,     0.85201,      0.8522,     0.85236,     0.85241,     0.85246,     0.85252,     0.85257,     0.85262,\n",
      "            0.85267,     0.85273,     0.85278,      0.8528,     0.85248,     0.85221,     0.85263,     0.85242,     0.85217,     0.85186,     0.85148,     0.85152,     0.85172,     0.85138,     0.85122,     0.85116,     0.85131,     0.85146,     0.85116,     0.85046,     0.85037,     0.85057,     0.85053,\n",
      "            0.85008,     0.85026,     0.85045,     0.85031,     0.84989,      0.8484,     0.84805,     0.84743,     0.84757,     0.84647,     0.84635,     0.84573,     0.84541,     0.84558,     0.84387,     0.84355,      0.8433,     0.84336,     0.84311,     0.84274,     0.84246,     0.84093,     0.83992,\n",
      "            0.83919,     0.83899,     0.83853,     0.83816,     0.83672,     0.83605,     0.83584,     0.83563,     0.83536,     0.83492,      0.8343,     0.83385,     0.83353,     0.83335,     0.83335,     0.83283,     0.83225,     0.83024,      0.8301,     0.82932,     0.82915,     0.82899,     0.82883,\n",
      "            0.82811,     0.82778,     0.82745,     0.82591,     0.82555,     0.82505,     0.82427,      0.8238,     0.82353,     0.82318,     0.82265,     0.82097,     0.81966,     0.81931,      0.8197,     0.81987,     0.82003,     0.82028,     0.82031,     0.81968,     0.81928,     0.81767,     0.81634,\n",
      "             0.8161,     0.81585,     0.81484,     0.81423,     0.81146,     0.81115,     0.81075,     0.80948,     0.80962,     0.80977,     0.80991,      0.8089,     0.80756,     0.80759,     0.80689,     0.80609,      0.8058,     0.80551,     0.80518,     0.80485,     0.80454,     0.80425,     0.80394,\n",
      "            0.80363,     0.80364,     0.80296,     0.80261,     0.80254,     0.80255,     0.80117,     0.80075,     0.79944,     0.79865,     0.79694,     0.79661,     0.79617,     0.79567,     0.79509,     0.79472,     0.79456,     0.79481,     0.79334,     0.79246,     0.79211,     0.79024,     0.78863,\n",
      "            0.78625,     0.78481,      0.7833,     0.78279,     0.78279,     0.78245,     0.78209,     0.78147,     0.78068,      0.7797,     0.77894,     0.77863,     0.77749,     0.77539,     0.77484,     0.77359,     0.77308,     0.77136,     0.76926,     0.76696,     0.76582,      0.7624,     0.76209,\n",
      "            0.76177,     0.76123,     0.76003,     0.75546,     0.75458,     0.75271,     0.75046,     0.74962,     0.74874,     0.74631,      0.7451,     0.74425,     0.74206,     0.74012,     0.73842,     0.73759,     0.73615,     0.73405,     0.73171,     0.72809,     0.72494,     0.72259,     0.72115,\n",
      "            0.71987,     0.71951,     0.71885,     0.71678,     0.71511,     0.71133,     0.70932,     0.70697,     0.70552,     0.70351,     0.70298,     0.70135,      0.7017,     0.69773,     0.69434,     0.69289,     0.69156,     0.69084,     0.68905,     0.68536,     0.68426,     0.68141,     0.67968,\n",
      "            0.67802,     0.67478,     0.67466,     0.67198,     0.66966,     0.66756,     0.66559,     0.66074,     0.65671,     0.65536,      0.6531,     0.65128,     0.64761,     0.64584,     0.64226,      0.6402,     0.63782,     0.63496,     0.63093,     0.62696,     0.62047,     0.61797,     0.61322,\n",
      "            0.60594,     0.60288,     0.60102,     0.59878,     0.59775,     0.59491,      0.5901,     0.58616,     0.57861,     0.57567,     0.57001,      0.5628,     0.56003,     0.55436,     0.54553,     0.54164,     0.53795,     0.53155,     0.52493,     0.52107,     0.51481,     0.51131,     0.50767,\n",
      "            0.50081,     0.49642,     0.49195,     0.49017,      0.4857,     0.48305,     0.47676,     0.46835,     0.46223,     0.45766,     0.45201,     0.44966,     0.44276,     0.43774,     0.43234,     0.42591,      0.4223,     0.41282,     0.40685,     0.39665,     0.38982,     0.38293,     0.37562,\n",
      "            0.37151,     0.37024,     0.36471,     0.36117,     0.35599,      0.3504,     0.34416,     0.33237,     0.32123,      0.3105,     0.30234,     0.29688,     0.28766,     0.28336,     0.27477,     0.26979,     0.26349,     0.26028,     0.25295,     0.24693,     0.24089,     0.23525,      0.2264,\n",
      "             0.2189,     0.21178,     0.20618,     0.18849,     0.18515,     0.17609,     0.17039,     0.16495,      0.1584,     0.15587,     0.15015,     0.14869,     0.14682,     0.13876,     0.13274,     0.12749,     0.12354,     0.11876,     0.11332,     0.11102,     0.10761,     0.10042,    0.095032,\n",
      "            0.09276,    0.089155,    0.085438,    0.081974,    0.078427,     0.07705,    0.071428,    0.064182,        0.06,    0.057164,    0.051095,    0.047888,    0.045315,    0.043876,    0.042465,    0.037236,    0.035723,    0.034864,      0.0331,    0.031594,    0.030705,     0.02939,    0.027659,\n",
      "           0.025924,    0.024494,    0.023648,    0.022885,    0.019921,    0.014976,    0.010378,   0.0088538,   0.0076515,   0.0066509,   0.0037184,   0.0026562,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.064688,    0.064748,     0.13258,     0.18517,     0.22617,     0.26085,     0.29034,     0.31859,     0.34272,     0.35927,     0.37659,     0.39184,     0.40528,     0.42006,     0.43072,     0.44379,     0.45405,     0.46289,     0.47205,     0.48368,     0.49257,     0.49863,     0.50961,\n",
      "            0.51762,     0.52248,     0.52877,     0.53706,     0.54435,     0.54794,     0.55328,      0.5596,     0.56587,     0.57089,     0.57539,     0.57856,     0.58449,     0.59001,     0.59201,      0.5959,     0.59899,     0.60264,     0.60664,     0.60949,     0.61159,     0.61816,     0.62051,\n",
      "            0.62218,     0.62621,     0.63104,     0.63376,     0.63588,     0.63785,     0.63991,     0.64222,     0.64627,     0.64886,     0.65025,     0.65177,     0.65223,     0.65424,     0.65594,     0.65754,     0.65936,     0.66197,     0.66358,     0.66697,     0.67066,     0.67327,     0.67508,\n",
      "            0.67741,     0.67906,     0.68088,     0.68321,     0.68467,     0.68718,     0.69009,     0.69261,     0.69468,     0.69645,     0.69828,     0.70034,     0.70098,     0.70271,     0.70547,     0.70691,     0.70965,     0.71029,     0.71094,      0.7132,     0.71425,     0.71627,     0.71741,\n",
      "            0.71814,     0.71864,     0.71905,     0.72235,     0.72283,     0.72336,     0.72328,     0.72354,     0.72379,     0.72431,     0.72482,     0.72533,     0.72624,     0.72728,     0.72831,     0.72857,     0.72878,     0.72924,     0.73104,     0.73168,     0.73226,     0.73306,     0.73383,\n",
      "            0.73485,     0.73601,     0.73851,     0.73886,     0.73901,     0.74026,     0.74078,     0.74084,     0.74143,     0.74192,     0.74226,     0.74316,     0.74629,     0.74645,     0.74661,     0.74677,     0.74711,     0.75049,     0.75135,     0.75259,     0.75345,     0.75569,     0.75651,\n",
      "             0.7575,     0.75843,      0.7588,     0.75933,     0.76019,     0.76077,     0.76209,     0.76265,     0.76304,     0.76593,     0.76634,     0.76684,     0.76884,     0.77221,     0.77294,     0.77336,     0.77406,     0.77443,     0.77458,     0.77473,     0.77488,     0.77516,     0.77748,\n",
      "            0.77802,     0.77823,     0.77845,     0.77991,     0.78144,      0.7821,     0.78345,     0.78392,     0.78382,     0.78468,     0.78495,     0.78522,     0.78578,     0.78565,     0.78631,     0.78962,     0.79079,     0.79128,     0.79254,     0.79373,     0.79365,     0.79358,     0.79404,\n",
      "            0.79531,      0.7957,     0.79603,     0.79641,     0.79657,     0.79672,     0.79688,     0.79703,     0.79758,      0.7983,     0.79889,     0.79957,     0.79997,     0.80059,     0.80137,     0.80213,     0.80341,     0.80371,     0.80402,      0.8047,     0.80598,     0.80685,     0.80707,\n",
      "            0.80729,     0.80751,     0.80942,     0.81042,     0.81081,     0.81099,     0.81116,     0.81134,      0.8118,     0.81308,     0.81361,      0.8146,     0.81449,     0.81501,      0.8157,     0.81619,      0.8165,     0.81682,     0.81773,     0.81813,     0.81853,     0.81891,     0.81943,\n",
      "            0.82025,      0.8205,     0.82076,      0.8215,     0.82404,     0.82437,     0.82466,     0.82496,     0.82546,     0.82651,      0.8264,     0.82713,     0.82758,     0.82847,      0.8289,     0.82917,     0.82945,     0.82983,     0.83024,     0.83078,     0.83176,     0.83235,     0.83274,\n",
      "            0.83565,      0.8378,     0.83847,     0.84072,     0.84137,      0.8415,     0.84163,     0.84175,     0.84188,     0.84201,     0.84221,     0.84258,     0.84295,     0.84288,     0.84283,     0.84346,     0.84375,      0.8439,     0.84404,     0.84419,     0.84433,     0.84447,     0.84551,\n",
      "            0.84581,     0.84612,      0.8462,     0.84615,     0.84611,     0.84637,      0.8469,      0.8468,     0.84783,     0.84874,     0.84929,     0.84932,     0.84923,     0.85096,     0.85089,     0.85079,     0.85068,     0.85054,     0.85167,     0.85221,     0.85316,     0.85344,     0.85372,\n",
      "              0.854,     0.85495,     0.85532,     0.85568,     0.85573,      0.8559,     0.85641,     0.85739,     0.85732,     0.85719,     0.85733,     0.85747,      0.8576,     0.85774,     0.85788,     0.85801,     0.85905,     0.85941,     0.85978,     0.86003,     0.86026,     0.86048,     0.86071,\n",
      "            0.86365,      0.8644,     0.86497,     0.86564,     0.86664,     0.86725,     0.86747,     0.86769,     0.86791,     0.86946,     0.87042,     0.87125,     0.87282,     0.87365,      0.8743,     0.87456,     0.87469,     0.87481,     0.87494,     0.87506,     0.87519,     0.87531,     0.87538,\n",
      "             0.8753,     0.87599,     0.87671,     0.87752,     0.87829,     0.87866,     0.87902,     0.87923,     0.87944,     0.87965,     0.87986,     0.88099,      0.8812,      0.8814,     0.88161,     0.88182,     0.88214,     0.88248,     0.88286,     0.88352,     0.88424,     0.88563,     0.88595,\n",
      "            0.88628,      0.8877,     0.88842,      0.8886,     0.88879,     0.88898,     0.88916,     0.88935,     0.88992,      0.8903,     0.89028,     0.89053,     0.89077,     0.89102,     0.89097,     0.89179,     0.89177,     0.89174,     0.89172,     0.89169,     0.89175,     0.89186,     0.89196,\n",
      "            0.89207,     0.89217,     0.89227,     0.89238,     0.89248,     0.89259,     0.89261,     0.89289,      0.8934,     0.89338,     0.89332,     0.89337,     0.89362,     0.89387,     0.89411,     0.89503,     0.89566,     0.89611,     0.89611,     0.89608,     0.89605,     0.89602,     0.89698,\n",
      "             0.8969,     0.89684,     0.89678,     0.89708,     0.89738,     0.89768,     0.89776,     0.89773,     0.89771,     0.89768,     0.89804,     0.89988,     0.90027,     0.90066,     0.90088,     0.90109,      0.9013,     0.90151,     0.90165,     0.90269,     0.90392,     0.90443,     0.90479,\n",
      "            0.90525,      0.9054,     0.90538,     0.90536,     0.90535,     0.90533,     0.90532,      0.9053,      0.9055,     0.90575,     0.90601,     0.90626,     0.90638,     0.90702,     0.90738,      0.9076,     0.90782,     0.90804,     0.90825,     0.90798,     0.90889,     0.90942,     0.90995,\n",
      "            0.91065,     0.91084,     0.91077,     0.91121,     0.91177,     0.91284,     0.91281,     0.91278,     0.91275,     0.91272,      0.9127,     0.91267,     0.91265,     0.91471,     0.91463,     0.91459,     0.91456,     0.91454,     0.91453,     0.91453,     0.91452,     0.91451,      0.9145,\n",
      "            0.91449,     0.91448,     0.91448,     0.91447,     0.91446,     0.91449,     0.91535,     0.91542,     0.91559,     0.91578,     0.91598,     0.91618,     0.91637,     0.91642,     0.91634,     0.91629,     0.91658,     0.91696,     0.91734,     0.91841,      0.9184,     0.91838,     0.91837,\n",
      "            0.91836,     0.91835,     0.91834,     0.91833,     0.91831,     0.91828,     0.91824,     0.91898,     0.91927,     0.91924,     0.91921,      0.9191,     0.91909,     0.91907,     0.91905,     0.91904,     0.91902,     0.91899,     0.91896,     0.91892,     0.91979,     0.92058,     0.92106,\n",
      "            0.92099,     0.92094,     0.92105,     0.92173,     0.92259,     0.92322,     0.92351,     0.92379,     0.92408,     0.92414,     0.92409,     0.92396,     0.92407,      0.9246,       0.925,     0.92499,     0.92499,     0.92498,     0.92498,     0.92497,     0.92497,     0.92496,     0.92495,\n",
      "            0.92495,     0.92494,     0.92494,     0.92493,     0.92493,     0.92492,     0.92491,     0.92601,     0.92599,     0.92596,     0.92594,     0.92628,     0.92714,     0.92812,     0.92908,     0.92901,     0.93011,     0.93038,     0.93087,     0.93227,     0.93219,     0.93216,     0.93213,\n",
      "            0.93202,     0.93196,     0.93307,     0.93305,     0.93302,     0.93356,     0.93397,     0.93393,     0.93402,     0.93441,      0.9348,     0.93504,     0.93537,     0.93569,     0.93602,     0.93644,     0.93691,      0.9373,     0.93742,     0.93755,     0.93768,      0.9378,     0.93793,\n",
      "            0.93806,     0.93818,     0.93831,     0.93842,     0.93838,     0.93841,     0.93943,     0.93948,     0.93945,     0.93941,     0.93937,     0.93971,      0.9402,     0.94087,     0.94156,     0.94185,     0.94221,     0.94257,     0.94268,      0.9426,     0.94293,     0.94342,     0.94373,\n",
      "            0.94373,     0.94419,     0.94465,     0.94484,     0.94479,     0.94463,      0.9446,     0.94453,     0.94522,     0.94559,     0.94844,     0.94896,     0.94893,     0.95011,     0.94994,     0.94991,     0.94989,     0.95046,     0.95105,     0.95101,     0.95098,     0.95084,     0.95074,\n",
      "            0.95121,     0.95184,      0.9518,     0.95176,     0.95163,     0.95156,     0.95154,     0.95152,      0.9515,     0.95182,     0.95261,     0.95256,     0.95253,     0.95279,     0.95373,     0.95368,     0.95363,     0.95345,     0.95456,     0.95459,     0.95458,     0.95456,     0.95455,\n",
      "            0.95449,     0.95446,     0.95443,     0.95429,     0.95425,     0.95421,     0.95414,      0.9541,     0.95407,     0.95404,     0.95399,     0.95384,     0.95372,     0.95369,     0.95506,     0.95551,     0.95595,     0.95664,     0.95756,     0.95751,     0.95748,     0.95734,     0.95723,\n",
      "            0.95721,     0.95719,      0.9571,     0.95705,     0.95681,     0.95679,     0.95675,     0.95667,     0.95708,     0.95748,     0.95789,      0.9579,     0.95778,     0.95906,     0.95904,     0.95897,     0.95895,     0.95893,      0.9589,     0.95887,     0.95885,     0.95882,      0.9588,\n",
      "            0.95877,     0.95962,     0.96005,     0.96002,     0.96044,     0.96135,     0.96124,     0.96121,     0.96111,     0.96105,     0.96091,     0.96089,     0.96085,     0.96217,     0.96213,      0.9621,     0.96375,     0.96485,     0.96475,     0.96469,     0.96466,     0.96453,     0.96441,\n",
      "            0.96565,     0.96696,     0.96686,     0.96683,     0.96969,     0.96967,     0.96965,     0.96961,     0.96956,      0.9695,     0.96945,     0.96943,     0.96936,     0.96923,      0.9692,     0.96912,     0.96909,     0.96898,     0.96884,      0.9687,     0.96862,      0.9684,     0.96838,\n",
      "            0.96836,      0.9694,     0.96976,     0.96947,     0.97095,     0.97084,      0.9707,     0.97065,      0.9706,     0.97045,     0.97038,     0.97032,     0.97019,     0.97007,     0.96996,      0.9715,     0.97142,     0.97129,     0.97278,     0.97257,     0.97239,     0.97391,     0.97383,\n",
      "            0.97376,     0.97374,     0.97371,     0.97359,      0.9735,     0.97328,     0.97317,     0.97476,     0.97468,     0.97458,     0.97455,     0.97483,     0.97621,     0.97603,     0.97586,     0.97578,     0.97637,     0.97748,     0.97739,     0.97721,     0.97821,     0.97887,     0.97879,\n",
      "            0.97871,     0.97875,     0.98045,     0.98033,     0.98023,     0.98014,     0.98199,      0.9818,     0.98163,     0.98158,     0.98149,     0.98141,     0.98212,     0.98322,     0.98309,     0.98301,     0.98292,     0.98281,     0.98265,     0.98249,     0.98223,     0.98431,     0.98414,\n",
      "            0.98386,     0.98375,     0.98368,     0.98359,     0.98355,     0.98344,      0.9856,     0.98546,      0.9852,     0.98509,     0.98489,     0.98562,     0.98705,     0.98687,     0.98658,     0.98645,     0.98632,      0.9861,     0.98587,     0.98573,     0.98549,     0.98536,     0.98522,\n",
      "            0.98496,     0.98478,      0.9846,     0.98453,     0.98434,     0.98733,     0.98711,     0.98681,     0.98659,     0.98642,      0.9862,     0.98953,     0.98933,     0.98917,     0.99263,     0.99249,     0.99241,     0.99219,     0.99205,     0.99179,     0.99162,     0.99143,     0.99123,\n",
      "            0.99111,     0.99107,      0.9909,      0.9908,     0.99063,     0.99045,     0.99174,     0.99489,     0.99467,     0.99446,     0.99428,     0.99416,     0.99394,     0.99383,     0.99361,     0.99347,     0.99329,      0.9932,     0.99297,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.92103,     0.92103,     0.91282,     0.91077,     0.90974,     0.90769,     0.90667,     0.90564,     0.90564,     0.90359,     0.90256,     0.90051,     0.89846,     0.89744,     0.89538,     0.89436,     0.89436,     0.89436,     0.89436,     0.89436,     0.89436,     0.89436,     0.89436,\n",
      "            0.89333,     0.89231,     0.89231,     0.89231,     0.89231,     0.89231,     0.89026,     0.89026,     0.88923,     0.88923,     0.88718,     0.88718,     0.88718,     0.88718,     0.88615,     0.88615,     0.88513,     0.88513,     0.88513,     0.88513,     0.88513,     0.88513,     0.88513,\n",
      "            0.88513,      0.8841,      0.8841,     0.88384,     0.88308,     0.88308,     0.88308,     0.88308,     0.88205,     0.88205,     0.88205,     0.88103,     0.88103,     0.88103,     0.88103,     0.88103,     0.88103,     0.88103,     0.88103,     0.88103,     0.88103,     0.88103,        0.88,\n",
      "               0.88,     0.87897,     0.87897,     0.87795,     0.87692,     0.87692,     0.87692,     0.87692,      0.8759,      0.8759,      0.8759,      0.8759,      0.8759,      0.8759,      0.8759,      0.8759,      0.8759,      0.8759,      0.8759,      0.8759,      0.8759,      0.8759,      0.8759,\n",
      "             0.8759,      0.8759,     0.87487,     0.87487,     0.87487,     0.87487,     0.87385,     0.87385,     0.87385,     0.87305,     0.87259,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,\n",
      "            0.87179,     0.87179,     0.87179,     0.87179,     0.87127,     0.87077,     0.87077,     0.86784,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,\n",
      "            0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86564,\n",
      "            0.86564,     0.86564,     0.86564,     0.86564,     0.86564,     0.86359,     0.86359,     0.86328,     0.86256,     0.86256,     0.86256,     0.86256,     0.86256,     0.86051,     0.86051,     0.86051,     0.86051,     0.86051,     0.86051,     0.86037,     0.85998,     0.85959,     0.85949,\n",
      "            0.85949,     0.85882,     0.85846,     0.85846,     0.85846,     0.85846,     0.85846,     0.85846,     0.85846,     0.85846,     0.85846,     0.85846,     0.85846,     0.85846,     0.85846,     0.85846,     0.85846,     0.85846,     0.85846,     0.85846,     0.85641,     0.85641,     0.85641,\n",
      "            0.85641,     0.85641,     0.85641,     0.85641,     0.85641,     0.85641,     0.85641,     0.85641,     0.85641,     0.85641,     0.85641,      0.8562,     0.85561,     0.85538,     0.85538,     0.85538,     0.85538,     0.85538,     0.85538,     0.85538,     0.85538,     0.85538,     0.85538,\n",
      "            0.85538,     0.85538,     0.85538,     0.85538,     0.85538,     0.85538,     0.85538,     0.85538,     0.85538,     0.85508,     0.85436,     0.85436,     0.85333,     0.85333,     0.85333,     0.85333,     0.85333,     0.85333,     0.85333,     0.85333,     0.85333,     0.85333,     0.85333,\n",
      "            0.85333,     0.85333,     0.85333,     0.85333,     0.85333,     0.85333,     0.85333,     0.85333,     0.85333,     0.85333,     0.85333,     0.85333,     0.85333,     0.85282,     0.85231,     0.85231,     0.85231,     0.85231,     0.85231,     0.85231,     0.85231,     0.85231,     0.85231,\n",
      "            0.85231,     0.85231,     0.85211,      0.8518,     0.85149,     0.85128,     0.85102,     0.85035,     0.85026,     0.85026,     0.85026,      0.8498,     0.84925,     0.84923,     0.84864,     0.84799,     0.84724,      0.8463,     0.84615,     0.84615,     0.84615,     0.84615,     0.84615,\n",
      "            0.84615,     0.84615,     0.84615,     0.84615,     0.84558,     0.84513,     0.84513,     0.84481,      0.8443,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,\n",
      "            0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84308,     0.84291,\n",
      "            0.84233,     0.84205,     0.84205,     0.84205,     0.84205,     0.84205,     0.84205,     0.84205,     0.84205,     0.84205,     0.84205,     0.84205,     0.84205,     0.84205,     0.84205,     0.84205,     0.84205,     0.84205,     0.84205,     0.84205,     0.84205,     0.84103,     0.84103,\n",
      "            0.84103,     0.84103,     0.84103,     0.84103,     0.84103,     0.84103,     0.84103,     0.84103,     0.84103,     0.84071,        0.84,        0.84,        0.84,        0.84,     0.83811,     0.83685,     0.83663,     0.83641,     0.83619,     0.83597,      0.8359,      0.8359,      0.8359,\n",
      "             0.8359,      0.8359,      0.8359,      0.8359,      0.8359,      0.8359,     0.83544,     0.83487,     0.83487,     0.83358,     0.83312,     0.83282,     0.83282,     0.83282,     0.83282,     0.83282,     0.83282,     0.83282,     0.83163,     0.83135,     0.83106,     0.83078,     0.83053,\n",
      "            0.82982,     0.82926,     0.82872,     0.82872,     0.82872,     0.82872,     0.82856,     0.82833,      0.8281,     0.82786,     0.82769,     0.82769,     0.82769,     0.82769,     0.82769,     0.82769,     0.82769,     0.82769,     0.82746,     0.82667,     0.82667,     0.82506,     0.82462,\n",
      "            0.82462,     0.82452,     0.82437,     0.82422,     0.82407,     0.82392,     0.82377,     0.82362,     0.82359,     0.82359,     0.82359,     0.82359,     0.82256,     0.82256,     0.82256,     0.82256,     0.82256,     0.82256,     0.82256,     0.81975,     0.81846,     0.81846,     0.81846,\n",
      "            0.81846,     0.81723,     0.81653,     0.81641,     0.81641,     0.81638,     0.81606,     0.81574,     0.81543,     0.81516,      0.8149,     0.81464,     0.81437,     0.81402,     0.81316,     0.81276,     0.81237,     0.81223,     0.81215,     0.81206,     0.81197,     0.81189,      0.8118,\n",
      "            0.81171,     0.81163,     0.81154,     0.81145,     0.81136,     0.81128,     0.81128,     0.81038,     0.81026,     0.81026,     0.81026,     0.81026,     0.81026,     0.80965,     0.80887,     0.80831,     0.80821,     0.80821,     0.80821,     0.80811,     0.80799,     0.80787,     0.80775,\n",
      "            0.80763,     0.80751,      0.8074,     0.80728,     0.80711,     0.80673,     0.80636,     0.80615,     0.80588,     0.80555,     0.80521,     0.80404,     0.80386,     0.80368,      0.8035,     0.80332,     0.80314,     0.80283,     0.80247,     0.80211,     0.80205,     0.80205,     0.80182,\n",
      "            0.80106,     0.80046,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,     0.79964,     0.79911,     0.79755,     0.79692,     0.79692,     0.79691,     0.79684,     0.79678,     0.79671,     0.79665,     0.79659,     0.79652,     0.79646,     0.79639,\n",
      "            0.79633,     0.79626,      0.7962,     0.79614,     0.79607,     0.79601,     0.79594,     0.79586,     0.79558,     0.79531,     0.79503,     0.79487,     0.79487,     0.79487,     0.79274,     0.79186,     0.79166,     0.79077,     0.79077,     0.79057,     0.78962,      0.7892,     0.78878,\n",
      "            0.78741,     0.78668,     0.78645,     0.78611,     0.78578,     0.78462,     0.78339,      0.7829,     0.78256,     0.78256,     0.78256,     0.78154,     0.78154,     0.78154,     0.78154,     0.78154,     0.78154,     0.78154,     0.78154,     0.78154,     0.78154,     0.78154,     0.78154,\n",
      "            0.78154,     0.78154,     0.78154,      0.7815,     0.78099,     0.78051,     0.78051,     0.78013,     0.77973,     0.77923,     0.77862,     0.77846,     0.77846,     0.77744,      0.7767,     0.77641,     0.77641,     0.77641,     0.77584,     0.77472,     0.77436,     0.77436,     0.77408,\n",
      "            0.77333,     0.77333,     0.77333,     0.77298,     0.77232,     0.76997,     0.76941,     0.76843,     0.76821,     0.76615,      0.7641,     0.76276,     0.76226,     0.76177,      0.7591,     0.75861,     0.75823,     0.75795,     0.75717,      0.7566,     0.75618,     0.75379,     0.75224,\n",
      "            0.75077,     0.75007,     0.74935,     0.74878,     0.74657,     0.74555,     0.74523,      0.7449,     0.74449,     0.74359,     0.74213,     0.74144,     0.74095,     0.74051,     0.73996,     0.73917,     0.73828,     0.73523,     0.73436,     0.73311,     0.73286,     0.73262,     0.73237,\n",
      "            0.73129,     0.73079,      0.7303,     0.72798,     0.72743,     0.72668,     0.72552,     0.72482,     0.72441,     0.72389,      0.7231,      0.7206,     0.71864,     0.71812,     0.71795,     0.71795,     0.71795,     0.71795,     0.71747,     0.71653,     0.71594,     0.71356,     0.71161,\n",
      "            0.71124,     0.71088,     0.70939,      0.7085,     0.70445,       0.704,     0.70341,     0.70154,     0.70154,     0.70154,     0.70154,     0.70001,     0.69807,     0.69744,     0.69641,     0.69526,     0.69483,     0.69441,     0.69394,     0.69346,     0.69302,     0.69259,     0.69216,\n",
      "            0.69171,     0.69128,     0.69005,     0.68954,     0.68923,     0.68877,      0.6868,      0.6862,     0.68433,      0.6832,     0.68077,     0.68031,     0.67967,      0.6783,     0.67747,     0.67695,      0.6759,     0.67573,     0.67366,     0.67241,     0.67192,      0.6693,     0.66704,\n",
      "            0.66307,     0.66041,     0.65832,     0.65762,     0.65629,     0.65582,     0.65532,     0.65448,      0.6534,     0.65204,       0.651,     0.65059,     0.64902,     0.64617,     0.64541,     0.64371,     0.64303,     0.64069,     0.63786,     0.63476,     0.63324,     0.62867,     0.62825,\n",
      "            0.62783,     0.62667,     0.62489,     0.61884,     0.61707,     0.61462,     0.61167,     0.61059,     0.60944,     0.60628,     0.60471,     0.60362,     0.60079,      0.5983,     0.59611,     0.59446,     0.59262,     0.58995,     0.58639,     0.58183,     0.57789,     0.57437,     0.57257,\n",
      "              0.571,     0.57055,     0.56972,     0.56717,     0.56512,     0.56048,     0.55803,     0.55461,     0.55285,     0.55042,     0.54978,     0.54769,     0.54769,     0.54292,     0.53889,     0.53716,     0.53538,      0.5342,     0.53208,     0.52774,     0.52615,      0.5226,     0.52059,\n",
      "            0.51867,     0.51487,     0.51426,     0.51119,     0.50854,     0.50615,     0.50339,     0.49792,     0.49339,     0.49188,     0.48937,     0.48735,     0.48308,     0.48084,     0.47691,     0.47467,     0.47208,     0.46898,     0.46463,     0.46037,     0.45346,     0.45036,     0.44537,\n",
      "            0.43778,     0.43462,     0.43269,     0.43039,     0.42934,     0.42644,     0.42111,     0.41714,     0.40958,     0.40665,     0.40106,     0.39385,     0.39091,     0.38544,       0.377,     0.37331,     0.36983,     0.36384,     0.35769,     0.35414,      0.3484,     0.34522,     0.34193,\n",
      "            0.33577,     0.33185,     0.32789,     0.32632,     0.32239,     0.31974,     0.31428,     0.30704,     0.30182,     0.29795,     0.29319,     0.29093,      0.2852,     0.28106,     0.27635,     0.27113,     0.26822,     0.26063,      0.2559,      0.2479,     0.24259,     0.23729,     0.23171,\n",
      "             0.2286,     0.22764,     0.22348,     0.22083,     0.21698,     0.21285,     0.20821,     0.19951,     0.19155,     0.18397,     0.17827,     0.17449,     0.16816,     0.16523,     0.15943,     0.15609,     0.15189,     0.14976,     0.14494,     0.14085,     0.13694,      0.1333,     0.12765,\n",
      "             0.1229,     0.11843,     0.11494,     0.10405,     0.10202,    0.096543,    0.093131,    0.089888,    0.086013,    0.084525,    0.081168,    0.080318,    0.079227,    0.074552,    0.071088,    0.068086,    0.065836,    0.063128,    0.060062,    0.058773,    0.056865,    0.052866,    0.049887,\n",
      "           0.048636,    0.046657,    0.044625,    0.042739,    0.040814,    0.040068,    0.037037,    0.033155,    0.030928,    0.029423,    0.026217,    0.024532,    0.023183,     0.02243,    0.021693,    0.018971,    0.018187,    0.017741,    0.016829,    0.016051,    0.015592,    0.014914,    0.014023,\n",
      "           0.013132,    0.012399,    0.011965,    0.011575,    0.010061,   0.0075447,   0.0052161,   0.0044466,   0.0038405,   0.0033365,   0.0018626,   0.0013299,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,\n",
      "            0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,\n",
      "            0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,\n",
      "            0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,\n",
      "            0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,\n",
      "            0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,     0.99286,\n",
      "            0.99286,     0.99286,     0.99286,     0.99286,     0.99286,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,\n",
      "             0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,\n",
      "             0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,      0.9902,\n",
      "            0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,\n",
      "            0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,\n",
      "            0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,     0.98901,\n",
      "            0.98901,     0.98606,     0.98606,     0.98606,     0.98606,     0.98606,     0.98606,     0.98606,     0.98606,     0.98606,     0.98606,     0.98606,     0.98606,     0.98606,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,\n",
      "            0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,\n",
      "            0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,\n",
      "            0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,\n",
      "            0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,     0.98458,\n",
      "            0.98458,     0.98458,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,\n",
      "            0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,      0.9821,      0.9821,      0.9821,      0.9821,      0.9821,      0.9821,      0.9821,      0.9821,      0.9821,      0.9821,      0.9821,      0.9821,\n",
      "             0.9821,      0.9821,      0.9821,      0.9821,      0.9821,      0.9821,      0.9821,      0.9821,      0.9821,      0.9821,      0.9821,      0.9821,      0.9821,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,\n",
      "            0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98121,     0.98004,\n",
      "            0.98004,     0.98004,     0.98004,     0.98004,     0.98004,     0.98004,     0.98004,     0.98004,     0.98004,     0.98004,     0.98004,     0.98004,     0.98004,     0.98004,     0.98004,     0.98004,     0.98004,     0.98004,     0.98004,     0.98004,     0.98004,     0.97852,     0.97852,\n",
      "            0.97852,     0.97852,     0.97852,     0.97852,     0.97852,     0.97852,     0.97852,     0.97852,      0.9771,      0.9771,      0.9771,      0.9771,      0.9771,      0.9771,      0.9771,      0.9771,      0.9771,      0.9771,      0.9771,     0.97566,     0.97566,     0.97566,     0.97566,\n",
      "            0.97566,     0.97566,     0.97566,     0.97566,     0.97566,     0.97422,     0.97422,     0.97422,     0.97422,     0.97422,     0.97422,     0.97422,     0.97422,     0.97422,     0.97258,     0.97258,     0.97258,     0.97117,     0.97117,     0.97117,     0.97117,     0.97117,     0.97117,\n",
      "            0.97117,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.96944,     0.96944,\n",
      "            0.96944,     0.96944,     0.96944,     0.96944,     0.96944,     0.96944,     0.96944,     0.96944,     0.96944,     0.96944,     0.96944,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,\n",
      "            0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,\n",
      "            0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,\n",
      "            0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96828,     0.96702,     0.96702,     0.96702,     0.96702,     0.96567,     0.96567,     0.96486,     0.96486,     0.96486,     0.96486,\n",
      "            0.96486,     0.96486,     0.96486,     0.96486,     0.96486,     0.96486,     0.96486,     0.96486,     0.96486,     0.96221,     0.96221,     0.96221,     0.96137,     0.96137,     0.96137,     0.96137,     0.96137,     0.96137,     0.96137,     0.96137,     0.96137,     0.96137,     0.96011,\n",
      "            0.96011,      0.9591,      0.9591,      0.9591,      0.9591,      0.9591,      0.9591,     0.95798,     0.95798,     0.95798,     0.95798,     0.95759,     0.95759,     0.95759,     0.95759,     0.95759,     0.95759,     0.95759,     0.95759,     0.95759,     0.95759,     0.95759,     0.95759,\n",
      "            0.95759,     0.95759,     0.95759,     0.95759,     0.95759,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95467,     0.95376,     0.95376,\n",
      "            0.95376,     0.95376,     0.95376,     0.95376,     0.95269,     0.95269,     0.95269,     0.95189,     0.95189,     0.95189,     0.95189,     0.95189,     0.95189,     0.95189,     0.95189,     0.95109,     0.95109,     0.95109,     0.95109,     0.95109,     0.95109,     0.95109,     0.95013,\n",
      "            0.95013,     0.95013,     0.95013,     0.94904,     0.94904,     0.94677,     0.94677,     0.94571,     0.94571,     0.94486,     0.94486,     0.94486,     0.94486,     0.94486,     0.94375,     0.94271,     0.94271,     0.94161,     0.94052,     0.93951,     0.93951,     0.93842,     0.93505,\n",
      "            0.93407,     0.93407,     0.93229,     0.93229,     0.93229,     0.93229,     0.93229,     0.93229,     0.93012,     0.93012,     0.92926,     0.92926,     0.92926,     0.92601,      0.9218,      0.9218,     0.91873,     0.91873,     0.91667,     0.91579,     0.91579,     0.91482,     0.91183,\n",
      "            0.91183,     0.91088,     0.90993,     0.90899,     0.90596,     0.90411,     0.90411,     0.90216,     0.90147,     0.90147,     0.90147,     0.89955,     0.89865,     0.89787,     0.89787,     0.89497,     0.89222,     0.89222,     0.89147,     0.89147,     0.89073,     0.89073,     0.88828,\n",
      "            0.88828,     0.88828,     0.88828,     0.88466,     0.88466,     0.88466,     0.88382,     0.87823,     0.86915,     0.86915,     0.85224,     0.85224,     0.85062,     0.84584,     0.84584,     0.84584,     0.84426,     0.84012,     0.83688,     0.81962,      0.8078,     0.79808,     0.79808,\n",
      "            0.78917,     0.78713,     0.77944,     0.77944,     0.77747,     0.77572,     0.77572,     0.76951,     0.73555,     0.73555,     0.73473,     0.73473,     0.73304,     0.71891,     0.71854,     0.71212,     0.68806,     0.68689,     0.68689,     0.67891,     0.66797,     0.64545,     0.63704,\n",
      "             0.6175,      0.6175,     0.59047,      0.5639,     0.54172,     0.51285,     0.51101,     0.51008,     0.48344,     0.46746,     0.42808,     0.42257,     0.42257,     0.41082,     0.39517,     0.38221,      0.3662,     0.36493,     0.35335,     0.34466,     0.34155,     0.29319,       0.251,\n",
      "            0.23987,     0.18954,     0.16277,     0.15338,     0.12299,     0.12044,    0.094257,    0.094257,    0.086939,    0.082081,    0.080891,    0.071011,    0.070949,      0.0684,    0.066802,    0.064044,    0.063308,    0.062572,    0.061835,    0.061099,    0.060363,    0.059627,    0.058891,\n",
      "           0.058155,    0.057419,    0.056682,    0.055946,     0.05521,    0.054474,    0.053738,    0.053002,    0.052266,     0.05153,    0.050793,    0.050057,    0.049321,    0.048585,    0.047849,    0.047113,    0.046377,     0.04564,    0.044904,    0.044168,    0.043432,    0.042696,     0.04196,\n",
      "           0.041224,    0.040487,    0.039751,    0.039015,    0.038279,    0.037543,    0.036807,    0.036071,    0.035335,    0.034598,    0.033862,    0.033126,     0.03239,    0.031654,    0.030918,    0.030182,    0.029445,    0.028709,    0.027973,    0.027237,    0.026501,    0.025765,    0.025029,\n",
      "           0.024292,    0.023556,     0.02282,    0.022084,    0.021348,    0.020612,    0.019876,     0.01914,    0.018403,    0.017667,    0.016931,    0.016195,    0.015459,    0.014723,    0.013987,     0.01325,    0.012514,    0.011778,    0.011042,    0.010306,   0.0095698,   0.0088336,   0.0080975,\n",
      "          0.0073614,   0.0066252,   0.0058891,    0.005153,   0.0044168,   0.0036807,   0.0029445,   0.0022084,   0.0014723,  0.00073614,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.11981,     0.11991,     0.22867,     0.30396,     0.35778,     0.40021,     0.43486,     0.46603,     0.49163,      0.5077,     0.52419,     0.53923,     0.55221,     0.56572,     0.57498,     0.58642,     0.59541,     0.60305,     0.61085,     0.61991,     0.62652,     0.63148,     0.64033,\n",
      "            0.64642,     0.64921,     0.65411,     0.66052,     0.66608,     0.66881,       0.673,     0.67772,     0.68285,     0.68653,     0.68998,     0.69228,     0.69656,     0.70051,     0.70242,     0.70518,     0.70703,     0.70959,     0.71238,     0.71436,     0.71582,     0.72034,     0.72196,\n",
      "             0.7231,     0.72549,     0.72875,      0.7307,     0.73249,     0.73381,     0.73519,     0.73672,     0.73903,     0.74074,     0.74166,     0.74227,     0.74257,     0.74389,       0.745,     0.74604,     0.74722,     0.74891,     0.74995,     0.75213,     0.75449,     0.75615,     0.75691,\n",
      "            0.75839,     0.75993,     0.76108,     0.76305,     0.76356,     0.76513,     0.76695,     0.76712,     0.76758,     0.76866,     0.76979,     0.77105,     0.77144,      0.7725,     0.77418,     0.77505,     0.77671,      0.7771,     0.77749,     0.77885,     0.77949,      0.7807,     0.78138,\n",
      "            0.78182,     0.78212,     0.78194,     0.78391,     0.78419,     0.78451,     0.78496,     0.78511,     0.78527,     0.78524,     0.78535,     0.78533,     0.78586,     0.78648,     0.78709,     0.78723,     0.78736,     0.78763,     0.78869,     0.78906,      0.7894,     0.78987,     0.79032,\n",
      "            0.79092,      0.7916,     0.79305,     0.79325,     0.79312,     0.79363,     0.79393,     0.79352,     0.79306,     0.79334,     0.79354,     0.79405,     0.79585,     0.79594,     0.79603,     0.79613,     0.79632,     0.79825,     0.79874,     0.79944,     0.79993,      0.8012,     0.80167,\n",
      "            0.80223,     0.80275,     0.80296,     0.80325,     0.80374,     0.80407,     0.80481,     0.80512,     0.80534,     0.80696,     0.80719,     0.80747,     0.80859,     0.81046,     0.81086,     0.81109,     0.81148,     0.81169,     0.81177,     0.81185,     0.81194,      0.8119,      0.8124,\n",
      "             0.8127,     0.81282,     0.81294,     0.81374,     0.81458,       0.814,     0.81474,     0.81485,     0.81447,     0.81494,     0.81509,     0.81523,     0.81554,     0.81453,     0.81488,     0.81667,      0.8173,     0.81757,     0.81824,     0.81882,      0.8186,     0.81837,     0.81857,\n",
      "            0.81925,     0.81915,     0.81916,     0.81936,     0.81945,     0.81953,     0.81961,      0.8197,     0.81999,     0.82037,     0.82068,     0.82104,     0.82126,     0.82159,       0.822,     0.82241,     0.82308,     0.82324,      0.8234,     0.82376,     0.82347,     0.82393,     0.82404,\n",
      "            0.82416,     0.82427,     0.82527,      0.8258,       0.826,      0.8261,     0.82619,     0.82628,     0.82652,     0.82719,     0.82747,     0.82788,     0.82754,      0.8277,     0.82806,     0.82832,     0.82848,     0.82864,     0.82912,     0.82933,     0.82953,     0.82973,        0.83,\n",
      "            0.83042,     0.83055,     0.83068,     0.83107,     0.83237,     0.83255,     0.83269,     0.83285,      0.8331,     0.83349,     0.83309,     0.83346,      0.8342,     0.83466,     0.83487,     0.83501,     0.83515,     0.83535,     0.83556,     0.83584,     0.83634,     0.83663,     0.83683,\n",
      "            0.83831,      0.8394,     0.83973,     0.84087,      0.8412,     0.84126,     0.84133,     0.84139,     0.84146,     0.84152,     0.84162,     0.84181,     0.84199,      0.8417,     0.84142,     0.84174,     0.84189,     0.84196,     0.84203,      0.8421,     0.84218,     0.84225,     0.84276,\n",
      "            0.84292,     0.84307,     0.84301,     0.84283,     0.84266,     0.84268,     0.84308,     0.84336,     0.84392,     0.84437,     0.84465,     0.84443,     0.84412,     0.84496,     0.84463,     0.84425,     0.84382,     0.84327,     0.84376,     0.84402,      0.8445,     0.84463,     0.84477,\n",
      "            0.84491,     0.84538,     0.84555,     0.84573,     0.84546,     0.84532,     0.84557,     0.84589,     0.84559,     0.84491,     0.84497,     0.84504,     0.84511,     0.84517,     0.84524,     0.84531,     0.84581,     0.84599,     0.84617,     0.84629,      0.8464,     0.84651,     0.84662,\n",
      "            0.84805,     0.84841,     0.84869,     0.84901,      0.8495,     0.84979,      0.8499,        0.85,     0.85011,     0.85086,     0.85132,     0.85172,     0.85247,     0.85287,     0.85318,      0.8532,     0.85312,     0.85304,     0.85296,     0.85288,      0.8528,     0.85272,     0.85257,\n",
      "            0.85223,     0.85241,     0.85275,     0.85314,     0.85351,     0.85368,     0.85385,     0.85395,     0.85405,     0.85415,     0.85425,     0.85479,     0.85489,     0.85498,     0.85508,     0.85518,     0.85533,      0.8555,     0.85567,     0.85599,     0.85632,     0.85644,     0.85659,\n",
      "            0.85674,     0.85741,     0.85775,     0.85784,     0.85792,     0.85801,      0.8581,     0.85819,     0.85846,     0.85846,     0.85807,     0.85819,     0.85831,     0.85842,     0.85739,     0.85823,     0.85833,     0.85843,     0.85853,     0.85862,     0.85869,     0.85874,     0.85878,\n",
      "            0.85883,     0.85888,     0.85893,     0.85898,     0.85903,     0.85907,     0.85884,     0.85829,     0.85797,     0.85713,     0.85687,     0.85673,     0.85684,     0.85695,     0.85707,     0.85749,     0.85778,     0.85799,     0.85735,     0.85718,     0.85701,     0.85684,     0.85715,\n",
      "            0.85673,      0.8564,     0.85608,     0.85621,     0.85635,     0.85649,     0.85644,      0.8563,     0.85616,     0.85602,     0.85569,     0.85587,     0.85604,     0.85622,     0.85632,     0.85642,     0.85651,     0.85661,     0.85655,     0.85658,     0.85714,     0.85649,      0.8564,\n",
      "            0.85661,     0.85662,     0.85653,     0.85645,     0.85636,     0.85627,     0.85618,     0.85609,     0.85616,     0.85627,     0.85639,      0.8565,     0.85599,     0.85628,     0.85644,     0.85654,     0.85663,     0.85673,     0.85683,     0.85514,     0.85483,     0.85507,      0.8553,\n",
      "            0.85562,     0.85501,     0.85459,     0.85472,     0.85497,     0.85542,     0.85523,     0.85504,     0.85485,     0.85492,     0.85504,     0.85516,     0.85528,     0.85601,     0.85567,     0.85585,     0.85603,     0.85601,     0.85596,     0.85591,     0.85585,      0.8558,     0.85575,\n",
      "             0.8557,     0.85564,     0.85559,     0.85554,     0.85548,     0.85545,     0.85583,     0.85535,     0.85535,     0.85544,     0.85553,     0.85561,      0.8557,     0.85538,      0.8549,     0.85456,     0.85463,     0.85479,     0.85496,     0.85547,     0.85553,     0.85558,     0.85563,\n",
      "            0.85569,     0.85574,      0.8558,     0.85585,     0.85585,     0.85562,     0.85539,      0.8556,     0.85557,     0.85537,     0.85516,     0.85451,      0.8546,     0.85468,     0.85476,     0.85484,     0.85493,      0.8548,     0.85458,     0.85436,      0.8547,     0.85504,     0.85512,\n",
      "            0.85466,     0.85429,     0.85407,     0.85436,     0.85474,     0.85501,     0.85513,     0.85525,     0.85538,      0.8552,     0.85487,     0.85434,      0.8547,     0.85493,      0.8551,     0.85513,     0.85516,     0.85519,     0.85522,     0.85525,     0.85528,     0.85531,     0.85534,\n",
      "            0.85537,      0.8554,     0.85543,     0.85546,     0.85549,     0.85552,     0.85555,     0.85602,     0.85585,     0.85568,     0.85551,     0.85556,     0.85593,     0.85634,     0.85551,     0.85497,     0.85532,     0.85491,     0.85512,     0.85559,       0.855,     0.85474,     0.85448,\n",
      "            0.85363,     0.85318,     0.85239,     0.85219,     0.85198,     0.85263,     0.85208,     0.85177,     0.85161,     0.85177,     0.85194,     0.85143,     0.85156,      0.8517,     0.85183,     0.85201,      0.8522,     0.85236,     0.85241,     0.85246,     0.85252,     0.85257,     0.85262,\n",
      "            0.85267,     0.85273,     0.85278,      0.8528,     0.85248,     0.85221,     0.85263,     0.85242,     0.85217,     0.85186,     0.85148,     0.85152,     0.85172,     0.85138,     0.85122,     0.85116,     0.85131,     0.85146,     0.85116,     0.85046,     0.85037,     0.85057,     0.85053,\n",
      "            0.85008,     0.85026,     0.85045,     0.85031,     0.84989,      0.8484,     0.84805,     0.84743,     0.84757,     0.84647,     0.84635,     0.84573,     0.84541,     0.84558,     0.84387,     0.84355,      0.8433,     0.84336,     0.84311,     0.84274,     0.84246,     0.84093,     0.83992,\n",
      "            0.83919,     0.83899,     0.83853,     0.83816,     0.83672,     0.83605,     0.83584,     0.83563,     0.83536,     0.83492,      0.8343,     0.83385,     0.83353,     0.83335,     0.83335,     0.83283,     0.83225,     0.83024,      0.8301,     0.82932,     0.82915,     0.82899,     0.82883,\n",
      "            0.82811,     0.82778,     0.82745,     0.82591,     0.82555,     0.82505,     0.82427,      0.8238,     0.82353,     0.82318,     0.82265,     0.82097,     0.81966,     0.81931,      0.8197,     0.81987,     0.82003,     0.82028,     0.82031,     0.81968,     0.81928,     0.81767,     0.81634,\n",
      "             0.8161,     0.81585,     0.81484,     0.81423,     0.81146,     0.81115,     0.81075,     0.80948,     0.80962,     0.80977,     0.80991,      0.8089,     0.80756,     0.80759,     0.80689,     0.80609,      0.8058,     0.80551,     0.80518,     0.80485,     0.80454,     0.80425,     0.80394,\n",
      "            0.80363,     0.80364,     0.80296,     0.80261,     0.80254,     0.80255,     0.80117,     0.80075,     0.79944,     0.79865,     0.79694,     0.79661,     0.79617,     0.79567,     0.79509,     0.79472,     0.79456,     0.79481,     0.79334,     0.79246,     0.79211,     0.79024,     0.78863,\n",
      "            0.78625,     0.78481,      0.7833,     0.78279,     0.78156,     0.78122,     0.78086,     0.78024,     0.77946,     0.77847,     0.77771,     0.77741,     0.77626,     0.77416,     0.77361,     0.77235,     0.77185,     0.77012,     0.76802,     0.76572,     0.76458,     0.76116,     0.76084,\n",
      "            0.76053,     0.75999,     0.75879,      0.7542,     0.75207,      0.7502,     0.74794,      0.7471,     0.74622,     0.74379,     0.74257,     0.74172,     0.73952,     0.73758,     0.73587,     0.73504,      0.7336,      0.7315,     0.72915,     0.72552,     0.72237,     0.72001,     0.71856,\n",
      "            0.71729,     0.71693,     0.71626,     0.71419,     0.71252,     0.70873,     0.70672,     0.70436,      0.7029,     0.70089,     0.70036,     0.69872,     0.69908,     0.69641,     0.69302,     0.69157,     0.69023,     0.68952,     0.68772,     0.68403,     0.68293,     0.68007,     0.67834,\n",
      "            0.67668,     0.67343,     0.67331,     0.67063,     0.66831,     0.66621,     0.66423,     0.65938,     0.65534,     0.65399,     0.65173,     0.64991,     0.64624,     0.64446,     0.64088,     0.63882,     0.63644,     0.63357,     0.62954,     0.62556,     0.61907,     0.61656,     0.61181,\n",
      "            0.60452,     0.60146,     0.59959,     0.59735,     0.59632,     0.59348,     0.58866,     0.58472,     0.57716,     0.57421,     0.56855,     0.56134,     0.55856,     0.55289,     0.54405,     0.54015,     0.53646,     0.53005,     0.52342,     0.51956,     0.51329,     0.50979,     0.50614,\n",
      "            0.49928,     0.49489,     0.49041,     0.48863,     0.48416,      0.4815,     0.47521,     0.46679,     0.46066,     0.45608,     0.45043,     0.44807,     0.44117,     0.43614,     0.43074,      0.4243,     0.42069,     0.41119,     0.40522,     0.39501,     0.38817,     0.38127,     0.37396,\n",
      "            0.36985,     0.36857,     0.36303,     0.35949,      0.3543,     0.34871,     0.34246,     0.33066,     0.31951,     0.30877,      0.3006,     0.29513,     0.28591,      0.2816,       0.273,     0.26802,     0.26171,      0.2585,     0.25116,     0.24513,     0.23909,     0.23344,     0.22458,\n",
      "            0.21707,     0.20994,     0.20434,     0.18663,     0.18329,     0.17422,     0.16852,     0.16307,     0.15651,     0.15398,     0.14825,     0.14679,     0.14492,     0.13685,     0.13082,     0.12557,     0.12161,     0.11683,     0.11138,     0.10908,     0.10567,    0.098474,    0.093078,\n",
      "           0.090804,    0.087195,    0.083474,    0.080006,    0.076456,    0.075077,     0.06945,    0.062196,     0.05801,    0.055171,    0.049096,    0.045886,     0.04331,     0.04187,    0.040457,    0.035223,    0.033709,    0.032848,    0.031083,    0.029575,    0.028685,    0.027369,    0.025636,\n",
      "           0.023899,    0.022467,    0.021621,    0.020857,     0.01789,    0.014252,    0.010378,   0.0088538,   0.0076515,   0.0066509,   0.0037184,   0.0026562,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.064112,    0.064171,     0.13094,     0.18288,     0.22337,     0.25761,     0.28706,     0.31499,     0.33884,     0.35478,     0.37145,     0.38693,     0.40065,     0.41526,     0.42578,      0.4387,     0.44884,     0.45758,     0.46663,     0.47758,     0.48579,     0.49177,      0.5026,\n",
      "            0.51049,     0.51468,     0.52087,     0.52904,     0.53621,     0.53975,     0.54563,     0.55186,     0.55869,     0.56364,     0.56874,     0.57187,     0.57773,     0.58319,     0.58584,     0.58969,     0.59275,     0.59635,     0.60031,     0.60313,     0.60522,     0.61171,     0.61404,\n",
      "            0.61569,     0.61967,     0.62445,     0.62732,     0.62997,     0.63192,     0.63396,     0.63625,     0.64025,     0.64282,      0.6442,      0.6457,     0.64615,     0.64814,     0.64984,     0.65142,     0.65322,     0.65581,      0.6574,     0.66076,     0.66441,       0.667,     0.66879,\n",
      "            0.67109,     0.67351,     0.67531,     0.67842,     0.67986,     0.68236,     0.68525,      0.6865,     0.68818,     0.68992,     0.69174,     0.69378,     0.69442,     0.69613,     0.69886,     0.70029,       0.703,     0.70364,     0.70428,     0.70652,     0.70756,     0.70956,     0.71069,\n",
      "            0.71141,     0.71191,     0.71231,     0.71557,     0.71605,     0.71657,     0.71734,     0.71759,     0.71784,     0.71835,     0.71886,     0.71936,     0.72026,     0.72129,     0.72232,     0.72257,     0.72278,     0.72324,     0.72502,     0.72565,     0.72623,     0.72702,     0.72779,\n",
      "            0.72879,     0.72995,     0.73243,     0.73277,     0.73292,     0.73415,     0.73467,     0.73546,     0.73616,     0.73665,     0.73699,     0.73788,     0.74099,     0.74115,     0.74131,     0.74146,      0.7418,     0.74515,     0.74601,     0.74724,     0.74809,     0.75032,     0.75114,\n",
      "            0.75212,     0.75304,     0.75341,     0.75393,     0.75479,     0.75536,     0.75667,     0.75723,     0.75762,     0.76048,     0.76089,     0.76139,     0.76338,     0.76672,     0.76744,     0.76786,     0.76855,     0.76892,     0.76907,     0.76922,     0.76937,     0.76947,     0.77103,\n",
      "            0.77156,     0.77178,       0.772,     0.77344,     0.77496,      0.7756,     0.77693,      0.7774,     0.77729,     0.77815,     0.77841,     0.77868,     0.77924,      0.7791,     0.77975,     0.78304,     0.78419,     0.78468,     0.78592,     0.78711,     0.78703,     0.78695,     0.78741,\n",
      "            0.78866,     0.78904,     0.78937,     0.78975,     0.78991,     0.79006,     0.79021,     0.79037,     0.79091,     0.79162,     0.79221,     0.79288,     0.79328,      0.7939,     0.79466,     0.79542,     0.79669,     0.79699,     0.79729,     0.79797,     0.79923,     0.80009,      0.8003,\n",
      "            0.80052,     0.80074,     0.80263,     0.80363,     0.80401,     0.80419,     0.80436,     0.80454,     0.80499,     0.80626,     0.80679,     0.80777,     0.80766,     0.80817,     0.80886,     0.80934,     0.80965,     0.80996,     0.81087,     0.81127,     0.81166,     0.81203,     0.81256,\n",
      "            0.81337,     0.81362,     0.81387,     0.81461,     0.81712,     0.81746,     0.81774,     0.81804,     0.81853,     0.81957,     0.81946,     0.82017,     0.82161,      0.8225,     0.82292,     0.82319,     0.82347,     0.82384,     0.82425,     0.82479,     0.82577,     0.82634,     0.82674,\n",
      "            0.82962,     0.83176,     0.83242,     0.83465,      0.8353,     0.83543,     0.83556,     0.83568,     0.83581,     0.83594,     0.83613,      0.8365,     0.83687,      0.8368,     0.83674,     0.83737,     0.83766,      0.8378,     0.83795,     0.83809,     0.83823,     0.83838,      0.8394,\n",
      "            0.83971,     0.84001,     0.84009,     0.84004,     0.83999,     0.84025,     0.84103,      0.8416,     0.84272,     0.84362,     0.84417,     0.84419,     0.84411,     0.84582,     0.84575,     0.84565,     0.84553,     0.84538,     0.84651,     0.84704,     0.84799,     0.84827,     0.84855,\n",
      "            0.84882,     0.84977,     0.85013,      0.8505,     0.85054,     0.85071,     0.85121,     0.85219,     0.85211,     0.85198,     0.85212,     0.85225,     0.85239,     0.85252,     0.85266,      0.8528,     0.85382,     0.85419,     0.85455,      0.8548,     0.85502,     0.85525,     0.85548,\n",
      "            0.85839,     0.85914,     0.85971,     0.86037,     0.86137,     0.86197,     0.86219,     0.86241,     0.86263,     0.86417,     0.86513,     0.86595,     0.86751,     0.86833,     0.86898,     0.86914,     0.86912,      0.8691,     0.86908,     0.86906,     0.86904,     0.86902,     0.86899,\n",
      "            0.86891,     0.86958,      0.8703,     0.87111,     0.87188,     0.87224,      0.8726,     0.87281,     0.87301,     0.87322,     0.87343,     0.87455,     0.87476,     0.87496,     0.87517,     0.87537,     0.87569,     0.87603,     0.87641,     0.87706,     0.87777,     0.87915,     0.87947,\n",
      "            0.87979,     0.88121,     0.88192,      0.8821,     0.88229,     0.88247,     0.88266,     0.88284,     0.88341,     0.88378,     0.88375,       0.884,     0.88425,      0.8845,     0.88443,     0.88641,     0.88662,     0.88682,     0.88703,     0.88724,     0.88738,     0.88748,     0.88758,\n",
      "            0.88769,     0.88779,     0.88789,       0.888,      0.8881,      0.8882,     0.88823,     0.88811,     0.88805,     0.88788,     0.88783,     0.88787,     0.88812,     0.88836,     0.88861,     0.88952,     0.89015,      0.8906,     0.89059,     0.89055,     0.89052,     0.89049,     0.89144,\n",
      "            0.89136,      0.8913,     0.89123,     0.89153,     0.89183,     0.89212,      0.8922,     0.89218,     0.89215,     0.89212,     0.89206,     0.89319,     0.89358,     0.89396,     0.89418,     0.89439,      0.8946,     0.89481,     0.89495,     0.89597,     0.89719,     0.89769,     0.89803,\n",
      "             0.8985,     0.89864,     0.89862,      0.8986,     0.89859,     0.89857,     0.89855,     0.89854,     0.89873,     0.89899,     0.89924,     0.89949,      0.8996,     0.90024,     0.90059,     0.90081,     0.90103,     0.90124,     0.90146,     0.90116,     0.90206,     0.90258,     0.90311,\n",
      "             0.9038,     0.90398,      0.9039,     0.90435,      0.9049,     0.90596,     0.90593,     0.90589,     0.90586,     0.90609,     0.90635,     0.90662,     0.90688,     0.90895,     0.90906,     0.90946,     0.90987,     0.90992,     0.90991,     0.90991,      0.9099,     0.90989,     0.90988,\n",
      "            0.90987,     0.90986,     0.90985,     0.90984,     0.90983,     0.90987,     0.91072,     0.91079,     0.91095,     0.91115,     0.91134,     0.91154,     0.91173,     0.91177,     0.91169,     0.91164,     0.91193,     0.91231,     0.91268,     0.91385,     0.91397,      0.9141,     0.91422,\n",
      "            0.91435,     0.91447,     0.91459,     0.91472,     0.91481,     0.91478,     0.91474,     0.91547,     0.91576,     0.91573,      0.9157,     0.91565,     0.91584,     0.91603,     0.91622,     0.91642,     0.91661,     0.91664,     0.91661,     0.91657,     0.91744,     0.91822,     0.91871,\n",
      "            0.91864,     0.91858,     0.91869,     0.91937,     0.92023,     0.92085,     0.92114,     0.92142,     0.92171,     0.92177,     0.92172,     0.92204,     0.92288,     0.92341,     0.92383,      0.9239,     0.92396,     0.92403,      0.9241,     0.92417,     0.92424,     0.92431,     0.92438,\n",
      "            0.92445,     0.92452,     0.92459,     0.92465,     0.92472,     0.92479,     0.92486,     0.92601,     0.92599,     0.92596,     0.92594,     0.92628,     0.92714,     0.92812,     0.92908,     0.92901,     0.93011,     0.93038,     0.93087,     0.93227,     0.93219,     0.93216,     0.93213,\n",
      "            0.93202,     0.93196,     0.93186,     0.93183,      0.9318,     0.93356,     0.93397,     0.93393,     0.93402,     0.93441,      0.9348,     0.93504,     0.93537,     0.93569,     0.93602,     0.93644,     0.93691,      0.9373,     0.93742,     0.93755,     0.93768,      0.9378,     0.93793,\n",
      "            0.93806,     0.93818,     0.93831,     0.93842,     0.93838,     0.93841,     0.93943,     0.93948,     0.93945,     0.93941,     0.93937,     0.93971,      0.9402,     0.94087,     0.94156,     0.94185,     0.94221,     0.94257,     0.94268,      0.9426,     0.94293,     0.94342,     0.94373,\n",
      "            0.94373,     0.94419,     0.94465,     0.94484,     0.94479,     0.94463,      0.9446,     0.94453,     0.94522,     0.94559,     0.94844,     0.94896,     0.94893,     0.95011,     0.94994,     0.94991,     0.94989,     0.95046,     0.95105,     0.95101,     0.95098,     0.95084,     0.95074,\n",
      "            0.95121,     0.95184,      0.9518,     0.95176,     0.95163,     0.95156,     0.95154,     0.95152,      0.9515,     0.95182,     0.95261,     0.95256,     0.95253,     0.95279,     0.95373,     0.95368,     0.95363,     0.95345,     0.95456,     0.95459,     0.95458,     0.95456,     0.95455,\n",
      "            0.95449,     0.95446,     0.95443,     0.95429,     0.95425,     0.95421,     0.95414,      0.9541,     0.95407,     0.95404,     0.95399,     0.95384,     0.95372,     0.95369,     0.95506,     0.95551,     0.95595,     0.95664,     0.95756,     0.95751,     0.95748,     0.95734,     0.95723,\n",
      "            0.95721,     0.95719,      0.9571,     0.95705,     0.95681,     0.95679,     0.95675,     0.95667,     0.95708,     0.95748,     0.95789,      0.9579,     0.95778,     0.95906,     0.95904,     0.95897,     0.95895,     0.95893,      0.9589,     0.95887,     0.95885,     0.95882,      0.9588,\n",
      "            0.95877,     0.95962,     0.96005,     0.96002,     0.96044,     0.96135,     0.96124,     0.96121,     0.96111,     0.96105,     0.96091,     0.96089,     0.96085,     0.96217,     0.96213,      0.9621,     0.96375,     0.96485,     0.96475,     0.96469,     0.96466,     0.96453,     0.96441,\n",
      "            0.96565,     0.96696,     0.96686,     0.96683,     0.96818,     0.96815,     0.96813,     0.96809,     0.96804,     0.96798,     0.96793,     0.96791,     0.96783,     0.96769,     0.96766,     0.96757,     0.96754,     0.96743,     0.96729,     0.96713,     0.96705,     0.96682,      0.9668,\n",
      "            0.96678,     0.96781,     0.96817,     0.96786,     0.96772,      0.9676,     0.96745,     0.96739,     0.96733,     0.96717,     0.96708,     0.96703,     0.96687,     0.96674,     0.96662,     0.96815,     0.96806,     0.96792,     0.96937,     0.96914,     0.96894,     0.97044,     0.97035,\n",
      "            0.97027,     0.97024,      0.9702,     0.97007,     0.96996,     0.96972,     0.96959,     0.97116,     0.97107,     0.97094,     0.97091,     0.97118,     0.97256,     0.97419,       0.974,     0.97392,      0.9745,      0.9756,     0.97551,     0.97531,      0.9763,     0.97695,     0.97686,\n",
      "            0.97678,      0.9768,     0.97849,     0.97836,     0.97825,     0.97815,     0.97999,     0.97978,     0.97959,     0.97953,     0.97943,     0.97935,     0.98003,     0.98113,     0.98097,     0.98088,     0.98078,     0.98066,     0.98048,      0.9803,     0.98001,     0.98207,     0.98187,\n",
      "            0.98156,     0.98143,     0.98135,     0.98125,      0.9812,     0.98108,      0.9832,     0.98304,     0.98273,     0.98261,     0.98237,     0.98305,     0.98446,     0.98424,      0.9839,     0.98374,     0.98359,     0.98332,     0.98304,     0.98287,     0.98259,     0.98244,     0.98227,\n",
      "            0.98195,     0.98174,     0.98152,     0.98143,     0.98121,     0.98416,     0.98389,     0.98352,     0.98324,     0.98302,     0.98275,     0.98605,     0.98577,     0.98556,     0.98895,     0.98874,     0.98862,     0.98829,     0.98807,     0.98769,     0.98742,     0.98714,     0.98684,\n",
      "            0.98666,      0.9866,     0.98636,     0.98619,     0.98595,     0.98568,     0.98685,     0.98977,     0.98935,     0.98891,     0.98856,     0.98831,     0.98788,     0.98766,     0.98722,     0.98694,     0.98659,      0.9864,     0.98595,     0.99272,     0.99251,     0.99231,     0.99197,\n",
      "            0.99165,     0.99134,     0.99108,     0.99014,     0.98995,     0.98938,     0.98899,     0.98859,     0.98808,     0.98787,     0.98736,     0.98723,     0.98705,     0.98624,     0.98557,     0.98494,     0.98442,     0.98375,     0.98292,     0.98255,     0.98196,      0.9806,     0.97944,\n",
      "            0.97891,     0.97801,     0.97701,       0.976,     0.97487,      0.9744,     0.97231,     0.96906,     0.96683,     0.96513,     0.96086,     0.95819,     0.95574,     0.95426,     0.95271,      0.9459,     0.94357,     0.94215,       0.939,     0.93604,     0.93417,     0.93115,     0.92677,\n",
      "            0.92182,     0.91724,     0.91414,     0.91125,     0.89788,     0.95548,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.91282,     0.91282,     0.90154,     0.89949,     0.89846,     0.89641,     0.89641,     0.89538,     0.89538,     0.89231,     0.89026,     0.88923,     0.88821,     0.88718,     0.88513,      0.8841,      0.8841,      0.8841,      0.8841,     0.88308,     0.88205,     0.88205,     0.88205,\n",
      "            0.88103,     0.87897,     0.87897,     0.87897,     0.87897,     0.87897,     0.87795,     0.87795,     0.87795,     0.87795,     0.87692,     0.87692,     0.87692,     0.87692,     0.87692,     0.87692,      0.8759,      0.8759,      0.8759,      0.8759,      0.8759,      0.8759,      0.8759,\n",
      "             0.8759,     0.87487,     0.87487,     0.87487,     0.87487,     0.87487,     0.87487,     0.87487,     0.87385,     0.87385,     0.87385,     0.87282,     0.87282,     0.87282,     0.87282,     0.87282,     0.87282,     0.87282,     0.87282,     0.87282,     0.87282,     0.87282,     0.87179,\n",
      "            0.87179,     0.87179,     0.87179,     0.87179,     0.87077,     0.87077,     0.87077,     0.86918,     0.86769,     0.86769,     0.86769,     0.86769,     0.86769,     0.86769,     0.86769,     0.86769,     0.86769,     0.86769,     0.86769,     0.86769,     0.86769,     0.86769,     0.86769,\n",
      "            0.86769,     0.86769,     0.86667,     0.86667,     0.86667,     0.86667,     0.86667,     0.86667,     0.86667,     0.86587,     0.86541,     0.86462,     0.86462,     0.86462,     0.86462,     0.86462,     0.86462,     0.86462,     0.86462,     0.86462,     0.86462,     0.86462,     0.86462,\n",
      "            0.86462,     0.86462,     0.86462,     0.86462,     0.86409,     0.86359,     0.86359,     0.86154,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,\n",
      "            0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85949,     0.85928,     0.85846,\n",
      "            0.85846,     0.85846,     0.85846,     0.85846,     0.85846,     0.85641,     0.85641,      0.8561,     0.85538,     0.85538,     0.85538,     0.85538,     0.85538,     0.85333,     0.85333,     0.85333,     0.85333,     0.85333,     0.85333,     0.85319,      0.8528,     0.85241,     0.85231,\n",
      "            0.85231,     0.85164,     0.85128,     0.85128,     0.85128,     0.85128,     0.85128,     0.85128,     0.85128,     0.85128,     0.85128,     0.85128,     0.85128,     0.85128,     0.85128,     0.85128,     0.85128,     0.85128,     0.85128,     0.85128,     0.84923,     0.84923,     0.84923,\n",
      "            0.84923,     0.84923,     0.84923,     0.84923,     0.84923,     0.84923,     0.84923,     0.84923,     0.84923,     0.84923,     0.84923,     0.84902,     0.84843,     0.84821,     0.84821,     0.84821,     0.84821,     0.84821,     0.84821,     0.84821,     0.84821,     0.84821,     0.84821,\n",
      "            0.84821,     0.84821,     0.84821,     0.84821,     0.84821,     0.84821,     0.84821,     0.84821,     0.84821,      0.8479,     0.84718,     0.84718,     0.84718,     0.84718,     0.84718,     0.84718,     0.84718,     0.84718,     0.84718,     0.84718,     0.84718,     0.84718,     0.84718,\n",
      "            0.84718,     0.84718,     0.84718,     0.84718,     0.84718,     0.84718,     0.84718,     0.84718,     0.84718,     0.84718,     0.84718,     0.84718,     0.84718,     0.84666,     0.84615,     0.84615,     0.84615,     0.84615,     0.84615,     0.84615,     0.84615,     0.84615,     0.84615,\n",
      "            0.84615,     0.84615,     0.84596,     0.84565,     0.84534,     0.84513,     0.84513,     0.84513,     0.84513,     0.84513,     0.84513,     0.84467,     0.84412,      0.8441,     0.84351,     0.84287,     0.84212,     0.84117,     0.84103,     0.84103,     0.84103,     0.84103,     0.84103,\n",
      "            0.84103,     0.84103,     0.84103,     0.84103,     0.84045,        0.84,        0.84,     0.83968,     0.83917,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,\n",
      "            0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83795,     0.83785,     0.83771,     0.83757,     0.83743,      0.8373,     0.83716,     0.83702,     0.83676,\n",
      "            0.83617,      0.8359,      0.8359,      0.8359,      0.8359,      0.8359,      0.8359,      0.8359,      0.8359,      0.8359,      0.8359,      0.8359,      0.8359,      0.8359,      0.8359,      0.8359,      0.8359,      0.8359,      0.8359,      0.8359,      0.8359,     0.83487,     0.83487,\n",
      "            0.83487,     0.83487,     0.83487,     0.83487,     0.83487,     0.83487,     0.83487,     0.83487,     0.83487,     0.83456,     0.83385,     0.83385,     0.83385,     0.83385,     0.83195,     0.83179,     0.83179,     0.83179,     0.83179,     0.83179,     0.83179,     0.83179,     0.83179,\n",
      "            0.83179,     0.83179,     0.83179,     0.83179,     0.83179,     0.83179,     0.83134,     0.83041,     0.82987,     0.82845,       0.828,     0.82769,     0.82769,     0.82769,     0.82769,     0.82769,     0.82769,     0.82769,      0.8265,     0.82622,     0.82593,     0.82565,      0.8254,\n",
      "            0.82469,     0.82414,      0.8236,     0.82359,     0.82359,     0.82359,     0.82343,      0.8232,     0.82297,     0.82273,     0.82217,     0.82154,     0.82154,     0.82154,     0.82154,     0.82154,     0.82154,     0.82154,     0.82131,     0.82051,     0.82051,     0.81891,     0.81846,\n",
      "            0.81846,     0.81836,     0.81821,     0.81806,     0.81792,     0.81777,     0.81762,     0.81747,     0.81744,     0.81744,     0.81744,     0.81744,     0.81641,     0.81641,     0.81641,     0.81641,     0.81641,     0.81641,     0.81641,     0.81359,     0.81231,     0.81231,     0.81231,\n",
      "            0.81231,     0.81108,     0.81038,     0.81026,     0.81026,     0.81022,     0.80991,     0.80959,     0.80927,     0.80923,     0.80923,     0.80923,     0.80923,     0.80889,     0.80821,     0.80821,     0.80821,     0.80813,     0.80805,     0.80796,     0.80787,     0.80778,      0.8077,\n",
      "            0.80761,     0.80752,     0.80744,     0.80735,     0.80726,     0.80718,     0.80718,     0.80628,     0.80615,     0.80615,     0.80615,     0.80615,     0.80615,     0.80555,     0.80476,     0.80421,      0.8041,      0.8041,      0.8041,      0.8041,      0.8041,      0.8041,      0.8041,\n",
      "             0.8041,      0.8041,      0.8041,      0.8041,     0.80403,     0.80366,     0.80328,     0.80308,      0.8028,     0.80247,     0.80213,     0.80103,     0.80103,     0.80103,     0.80103,     0.80103,     0.80103,     0.80078,     0.80042,     0.80005,         0.8,         0.8,     0.79977,\n",
      "            0.79901,     0.79841,     0.79795,     0.79795,     0.79795,     0.79795,     0.79795,     0.79795,     0.79795,     0.79759,     0.79706,      0.7959,      0.7959,      0.7959,      0.7959,      0.7959,      0.7959,      0.7959,      0.7959,      0.7959,      0.7959,      0.7959,      0.7959,\n",
      "             0.7959,      0.7959,      0.7959,      0.7959,      0.7959,      0.7959,      0.7959,     0.79586,     0.79558,     0.79531,     0.79503,     0.79487,     0.79487,     0.79487,     0.79274,     0.79186,     0.79166,     0.79077,     0.79077,     0.79057,     0.78962,      0.7892,     0.78878,\n",
      "            0.78741,     0.78668,     0.78542,     0.78509,     0.78475,     0.78462,     0.78339,      0.7829,     0.78256,     0.78256,     0.78256,     0.78154,     0.78154,     0.78154,     0.78154,     0.78154,     0.78154,     0.78154,     0.78154,     0.78154,     0.78154,     0.78154,     0.78154,\n",
      "            0.78154,     0.78154,     0.78154,      0.7815,     0.78099,     0.78051,     0.78051,     0.78013,     0.77973,     0.77923,     0.77862,     0.77846,     0.77846,     0.77744,      0.7767,     0.77641,     0.77641,     0.77641,     0.77584,     0.77472,     0.77436,     0.77436,     0.77408,\n",
      "            0.77333,     0.77333,     0.77333,     0.77298,     0.77232,     0.76997,     0.76941,     0.76843,     0.76821,     0.76615,      0.7641,     0.76276,     0.76226,     0.76177,      0.7591,     0.75861,     0.75823,     0.75795,     0.75717,      0.7566,     0.75618,     0.75379,     0.75224,\n",
      "            0.75077,     0.75007,     0.74935,     0.74878,     0.74657,     0.74555,     0.74523,      0.7449,     0.74449,     0.74359,     0.74213,     0.74144,     0.74095,     0.74051,     0.73996,     0.73917,     0.73828,     0.73523,     0.73436,     0.73311,     0.73286,     0.73262,     0.73237,\n",
      "            0.73129,     0.73079,      0.7303,     0.72798,     0.72743,     0.72668,     0.72552,     0.72482,     0.72441,     0.72389,      0.7231,      0.7206,     0.71864,     0.71812,     0.71795,     0.71795,     0.71795,     0.71795,     0.71747,     0.71653,     0.71594,     0.71356,     0.71161,\n",
      "            0.71124,     0.71088,     0.70939,      0.7085,     0.70445,       0.704,     0.70341,     0.70154,     0.70154,     0.70154,     0.70154,     0.70001,     0.69807,     0.69744,     0.69641,     0.69526,     0.69483,     0.69441,     0.69394,     0.69346,     0.69302,     0.69259,     0.69216,\n",
      "            0.69171,     0.69128,     0.69005,     0.68954,     0.68923,     0.68877,      0.6868,      0.6862,     0.68433,      0.6832,     0.68077,     0.68031,     0.67967,      0.6783,     0.67747,     0.67695,      0.6759,     0.67573,     0.67366,     0.67241,     0.67192,      0.6693,     0.66704,\n",
      "            0.66307,     0.66041,     0.65832,     0.65762,     0.65526,      0.6548,      0.6543,     0.65345,     0.65237,     0.65102,     0.64998,     0.64956,       0.648,     0.64514,     0.64439,     0.64268,       0.642,     0.63966,     0.63683,     0.63374,     0.63221,     0.62764,     0.62722,\n",
      "             0.6268,     0.62564,     0.62387,     0.61782,     0.61502,     0.61257,     0.60962,     0.60853,     0.60739,     0.60423,     0.60266,     0.60157,     0.59873,     0.59625,     0.59406,      0.5924,     0.59057,      0.5879,     0.58434,     0.57978,     0.57584,     0.57232,     0.57052,\n",
      "            0.56894,      0.5685,     0.56767,     0.56512,     0.56307,     0.55843,     0.55598,     0.55256,      0.5508,     0.54837,     0.54773,     0.54564,     0.54564,      0.5419,     0.53786,     0.53613,     0.53436,     0.53317,     0.53106,     0.52672,     0.52513,     0.52157,     0.51957,\n",
      "            0.51765,     0.51385,     0.51324,     0.51016,     0.50752,     0.50512,     0.50237,     0.49689,     0.49237,     0.49086,     0.48834,     0.48632,     0.48205,     0.47982,     0.47589,     0.47364,     0.47105,     0.46795,      0.4636,     0.45934,     0.45244,     0.44933,     0.44434,\n",
      "            0.43675,     0.43359,     0.43167,     0.42936,     0.42831,     0.42541,     0.42009,     0.41612,     0.40855,     0.40563,     0.40004,     0.39282,     0.38989,     0.38442,     0.37597,     0.37229,      0.3688,     0.36281,     0.35667,     0.35311,     0.34738,      0.3442,      0.3409,\n",
      "            0.33474,     0.33083,     0.32686,     0.32529,     0.32136,     0.31872,     0.31325,     0.30601,      0.3008,     0.29692,     0.29217,      0.2899,     0.28417,     0.28003,     0.27533,     0.27011,      0.2672,      0.2596,     0.25487,     0.24687,     0.24157,     0.23626,     0.23069,\n",
      "            0.22758,     0.22661,     0.22246,     0.21981,     0.21595,     0.21183,     0.20718,     0.19849,     0.19052,     0.18295,     0.17725,     0.17347,     0.16714,     0.16421,      0.1584,     0.15507,     0.15086,     0.14874,     0.14391,     0.13983,     0.13591,     0.13228,     0.12662,\n",
      "            0.12188,      0.1174,     0.11391,     0.10303,     0.10099,    0.095517,    0.092105,    0.088862,    0.084987,    0.083499,    0.080142,    0.079292,    0.078201,    0.073527,    0.070062,     0.06706,     0.06481,    0.062103,    0.059036,    0.057747,    0.055839,     0.05184,    0.048861,\n",
      "            0.04761,    0.045632,      0.0436,    0.041713,    0.039788,    0.039043,    0.036011,    0.032129,    0.029902,    0.028397,    0.025192,    0.023506,    0.022157,    0.021405,    0.020667,    0.017946,    0.017161,    0.016716,    0.015803,    0.015025,    0.014566,    0.013888,    0.012998,\n",
      "           0.012106,    0.011373,     0.01094,    0.010549,   0.0090351,   0.0071795,   0.0052161,   0.0044466,   0.0038405,   0.0033365,   0.0018626,   0.0013299,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
      "fitness: 1.2623725938272088\n",
      "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)', 'metrics/precision(M)', 'metrics/recall(M)', 'metrics/mAP50(M)', 'metrics/mAP50-95(M)']\n",
      "maps: array([     1.2105])\n",
      "names: {0: 'contaminated'}\n",
      "plot: True\n",
      "results_dict: {'metrics/precision(B)': 0.8960503872205174, 'metrics/recall(B)': 0.8310610233807938, 'metrics/mAP50(B)': 0.870846319078694, 'metrics/mAP50-95(B)': 0.6627007157190562, 'metrics/precision(M)': 0.8905546704800519, 'metrics/recall(M)': 0.8262169076730385, 'metrics/mAP50(M)': 0.8586960365923556, 'metrics/mAP50-95(M)': 0.5477641267921701, 'fitness': 1.2623725938272088}\n",
      "save_dir: PosixPath('runs/segment/train112')\n",
      "seg: ultralytics.utils.metrics.Metric object\n",
      "speed: {'preprocess': 0.29408828071925947, 'inference': 1.6349139420882517, 'loss': 0.000649949778681216, 'postprocess': 1.099842527638311}\n",
      "task: 'segment'\n"
     ]
    }
   ],
   "source": [
    "results = model.val(data=\"./data.yaml\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "59855c0397ebdf83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T20:45:56.619711200Z",
     "start_time": "2024-12-05T20:45:56.612160500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def infer_image(image_path):\n",
    "    # Загрузка изображения\n",
    "    image = cv2.imread(image_path)\n",
    "    image = torch.permute(torch.Tensor(image), (2, 0, 1))\n",
    "    transform = transforms.Resize((256, 256))\n",
    "    image = transform(image)\n",
    "\n",
    "    # Инференс\n",
    "    return model(image.unsqueeze(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "bafd993999b6432d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T20:51:52.367492200Z",
     "start_time": "2024-12-05T20:51:52.357938500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Функция для создания маски с черным фоном\n",
    "def create_mask(image_path, results):\n",
    "    # Загружаем изображение и переводим в градации серого\n",
    "    image = cv2.imread(image_path)\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Создаем пустую маску с черным фоном\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # Проходим по результатам и создаем маску\n",
    "    masks = nn.functional.sigmoid(results).detach().numpy().squeeze().squeeze()\n",
    "    mask_i_resized = cv2.resize(masks, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "    mask[mask_i_resized > 0.5] = 255\n",
    "    print(mask)\n",
    "    # for result in results:\n",
    "    #     masks = \n",
    "    #     mask_i = # Получаем маски из результатов\n",
    "    #     if masks is not None:\n",
    "    #         for mask_array in masks.data:  # Получаем маски как массивы\n",
    "    #             mask_i = mask_array.numpy()  # Преобразуем маску в numpy массив\n",
    "                \n",
    "    #             # Изменяем размер маски под размер оригинального изображения\n",
    "    #             mask_i_resized = cv2.resize(mask_i, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "                \n",
    "    #             # Накладываем маску на пустую маску (255 для белого)\n",
    "    #             mask[mask_i_resized > 0] = 255\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b3bd147c-a41b-4c35-bf59-b2322a4e080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.functional.sigmoid(results).detach().numpy().squeeze().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "f1784d8c-cdba-4f22-9358-a81fd1dfed5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 900)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.resize(a, (900, 1000), interpolation=cv2.INTER_LINEAR).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "37bfa1ff-c2be-40f7-8019-2162469b7be8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (encoder): MobileNetV2Encoder(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (9): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (10): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (11): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (12): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (13): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (14): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (15): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (16): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (17): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (18): Conv2dNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UnetDecoder(\n",
       "    (center): Identity()\n",
       "    (blocks): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(1376, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(288, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(152, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(80, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ad9007d4-b4fb-4d28-ac44-ff7741915e11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (encoder): MobileNetV2Encoder(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (9): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (10): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (11): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (12): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (13): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (14): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (15): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (16): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (17): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (18): Conv2dNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UnetDecoder(\n",
       "    (center): Identity()\n",
       "    (blocks): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(1376, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(288, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(152, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(80, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "19d85a72-fd2f-4de2-8b29-5aac46121659",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 67,  61,  56],\n",
       "        [ 70,  64,  59],\n",
       "        [ 74,  68,  63],\n",
       "        ...,\n",
       "        [151, 139, 121],\n",
       "        [152, 140, 122],\n",
       "        [151, 139, 121]],\n",
       "\n",
       "       [[ 73,  67,  62],\n",
       "        [ 73,  67,  62],\n",
       "        [ 74,  68,  63],\n",
       "        ...,\n",
       "        [153, 141, 123],\n",
       "        [154, 142, 124],\n",
       "        [153, 141, 123]],\n",
       "\n",
       "       [[ 79,  73,  68],\n",
       "        [ 77,  71,  66],\n",
       "        [ 76,  70,  65],\n",
       "        ...,\n",
       "        [151, 139, 121],\n",
       "        [152, 140, 122],\n",
       "        [152, 140, 122]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 95,  86,  72],\n",
       "        [ 98,  89,  75],\n",
       "        [109, 100,  86],\n",
       "        ...,\n",
       "        [121, 115,  96],\n",
       "        [121, 115,  96],\n",
       "        [121, 115,  96]],\n",
       "\n",
       "       [[ 75,  67,  54],\n",
       "        [ 89,  81,  68],\n",
       "        [115, 107,  94],\n",
       "        ...,\n",
       "        [117, 112,  97],\n",
       "        [117, 112,  97],\n",
       "        [117, 112,  97]],\n",
       "\n",
       "       [[ 58,  50,  37],\n",
       "        [ 81,  73,  60],\n",
       "        [119, 111,  98],\n",
       "        ...,\n",
       "        [115, 109,  96],\n",
       "        [115, 109,  96],\n",
       "        [115, 109,  96]]], dtype=uint8)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imread('/home/jovyan/nazar/123/123/misis_chill/new_photos/cv_open_dataset/open_img/1711009681_0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "facee0a7-99c1-4a2d-9368-7d507fd3d2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9694, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.sigmoid(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "bb1b125643cab2da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T20:52:49.360392200Z",
     "start_time": "2024-12-05T20:52:49.205486700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = infer_image(\"/home/jovyan/nazar/123/123/misis_chill/new_photos/cv_open_dataset/open_img/1711009681_0.jpg\")\n",
    "mask_image = create_mask(\"/home/jovyan/nazar/123/123/misis_chill/new_photos/cv_open_dataset/open_msk/1711009681_0.png\", results)\n",
    "\n",
    "# Сохраняем маску в формате PNG\n",
    "mask_output_path = './mask_image.png'  # Укажите путь для сохранения маски\n",
    "cv2.imwrite(mask_output_path, mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2109623d-47d0-44ee-ba9a-476300076184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35bf1867b33cd589",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.save('/home/jovyan/nazar/123/123/misis_chill/nornikel_dockerfile/baseline5s.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bba70711-a52b-448b-bc72-489ed9167a54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): SegmentationModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): C2PSA(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): Sequential(\n",
       "          (0): PSABlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): Conv(\n",
       "                (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (proj): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (pe): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (ffn): Sequential(\n",
       "              (0): Conv(\n",
       "                (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (12): Concat()\n",
       "      (13): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (15): Concat()\n",
       "      (16): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (18): Concat()\n",
       "      (19): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (21): Concat()\n",
       "      (22): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): Segment(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (proto): Proto(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (upsample): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (cv4): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(512, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_n = YOLO('/home/jovyan/nazar/123/123/misis_chill/nornikel_dockerfile/baseline4n.pt')\n",
    "model_s = YOLO('/home/jovyan/nazar/123/123/misis_chill/nornikel_dockerfile/baseline2.pt')\n",
    "model_n.to('cpu')\n",
    "model_s.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4eebee65-549d-4110-853e-4b80975855df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model16 = model_n.to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "14352518-a1a7-4262-a0f4-ae0b2fa78448",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 6.4258e+00, -2.4766e+00,  2.6973e+00],\n",
      "          [ 2.9336e+00, -1.9578e+01, -6.4404e-01],\n",
      "          [ 5.9531e+00, -1.8320e+00,  5.4219e+00]],\n",
      "\n",
      "         [[ 7.5898e+00, -3.4043e+00,  5.2656e+00],\n",
      "          [ 8.5645e-01, -2.4359e+01,  1.3779e+00],\n",
      "          [ 5.9336e+00, -1.3379e+00,  8.2656e+00]],\n",
      "\n",
      "         [[ 1.8193e+00, -3.2617e-01,  2.2734e+00],\n",
      "          [-3.2324e-01, -8.5391e+00,  1.2676e+00],\n",
      "          [ 7.6562e-01, -2.8418e-01,  2.8613e+00]]],\n",
      "\n",
      "\n",
      "        [[[-4.4141e+00, -5.3594e+00, -1.1846e+00],\n",
      "          [-1.2844e+01, -1.3219e+01, -4.8984e+00],\n",
      "          [-1.6219e+01, -1.1250e+01, -4.8008e+00]],\n",
      "\n",
      "         [[ 1.7080e+00,  6.2500e-01, -1.7188e+00],\n",
      "          [ 4.8398e+00,  2.9902e+00, -2.4585e-01],\n",
      "          [ 6.2305e+00,  2.3594e+00, -1.8860e-02]],\n",
      "\n",
      "         [[ 1.4053e+00,  3.7676e+00,  1.7725e+00],\n",
      "          [ 8.1875e+00,  1.0406e+01,  5.7461e+00],\n",
      "          [ 8.5000e+00,  7.1055e+00,  3.8672e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.9844e+01,  3.6113e+00,  2.1125e+01],\n",
      "          [-3.9781e+01,  3.5332e+00,  3.5156e+01],\n",
      "          [-2.4219e+01,  6.0498e-01,  2.0719e+01]],\n",
      "\n",
      "         [[ 8.9219e+00, -4.2891e+00, -4.3359e+00],\n",
      "          [ 1.3023e+01, -2.0234e+00, -1.0859e+01],\n",
      "          [ 6.3398e+00, -7.2705e-01, -6.2656e+00]],\n",
      "\n",
      "         [[ 1.3633e+01, -3.8613e+00, -1.5125e+01],\n",
      "          [ 2.9312e+01, -1.1162e+00, -2.7750e+01],\n",
      "          [ 1.6906e+01,  2.2852e+00, -1.5547e+01]]],\n",
      "\n",
      "\n",
      "        [[[ 7.3203e+00, -1.0439e+00, -5.7188e+00],\n",
      "          [ 1.3367e+01, -4.8047e-01, -1.4320e+01],\n",
      "          [ 8.3672e+00,  1.7249e-01, -7.6953e+00]],\n",
      "\n",
      "         [[ 6.6133e+00, -4.0479e-01, -5.4258e+00],\n",
      "          [ 1.3688e+01,  1.6998e-02, -1.5961e+01],\n",
      "          [ 6.8281e+00,  9.6875e-01, -7.0469e+00]],\n",
      "\n",
      "         [[ 9.4482e-01, -1.9348e-01, -3.3252e-01],\n",
      "          [ 3.5781e+00,  4.2017e-01, -5.5078e+00],\n",
      "          [ 8.4570e-01,  7.6270e-01, -1.3154e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8975e+00, -2.4258e+00, -3.9336e+00],\n",
      "          [ 1.6670e+00, -2.0918e+00, -2.4023e+00],\n",
      "          [-1.7761e-01,  1.4150e+00,  3.4355e+00]],\n",
      "\n",
      "         [[ 2.2598e+00,  1.0034e-01, -2.5117e+00],\n",
      "          [ 6.5381e-01, -4.3896e-01, -4.5581e-01],\n",
      "          [-2.4043e+00,  1.0762e+00,  6.0645e-01]],\n",
      "\n",
      "         [[-1.8867e+00,  8.0643e-03, -1.9873e-01],\n",
      "          [-6.0645e-01, -5.1562e-01, -1.0488e+00],\n",
      "          [-6.6846e-01, -1.7896e-01, -3.0176e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 5.1875e+00,  3.8281e+00,  3.1602e+00],\n",
      "          [ 6.5781e+00,  6.9688e+00,  6.1875e+00],\n",
      "          [ 4.3008e+00,  3.1621e+00,  4.5078e+00]],\n",
      "\n",
      "         [[-1.1922e+01, -1.3070e+01, -1.0906e+01],\n",
      "          [-1.6844e+01, -1.9781e+01, -1.7625e+01],\n",
      "          [-1.3117e+01, -1.3828e+01, -1.3328e+01]],\n",
      "\n",
      "         [[ 6.1680e+00,  8.7500e+00,  7.2852e+00],\n",
      "          [ 9.8047e+00,  1.3156e+01,  1.0227e+01],\n",
      "          [ 9.5156e+00,  1.0016e+01,  9.3203e+00]]],\n",
      "\n",
      "\n",
      "        [[[-3.5410e+00, -1.3418e+00,  5.2461e+00],\n",
      "          [-9.9453e+00, -1.7793e+00,  1.1727e+01],\n",
      "          [-4.7344e+00, -1.7500e+00,  6.1641e+00]],\n",
      "\n",
      "         [[-5.6211e+00, -1.6279e+00,  7.2070e+00],\n",
      "          [-1.4219e+01, -1.8633e+00,  1.5391e+01],\n",
      "          [-6.1016e+00, -1.4355e+00,  7.5625e+00]],\n",
      "\n",
      "         [[-2.0117e+00, -8.9502e-01,  2.9180e+00],\n",
      "          [-7.0898e+00, -6.3916e-01,  6.9570e+00],\n",
      "          [-3.3750e+00, -3.1274e-01,  3.9492e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 6.5820e+00,  1.1969e+01,  6.2734e+00],\n",
      "          [ 3.4375e-01,  3.2373e-01, -4.1870e-01],\n",
      "          [-6.8281e+00, -1.2297e+01, -5.9531e+00]],\n",
      "\n",
      "         [[ 6.3906e+00,  1.3391e+01,  6.9336e+00],\n",
      "          [ 3.1403e-02, -7.9248e-01, -1.0078e+00],\n",
      "          [-6.6602e+00, -1.3664e+01, -5.3516e+00]],\n",
      "\n",
      "         [[ 1.0088e+00,  4.4336e+00,  1.6943e+00],\n",
      "          [ 2.4573e-01,  3.4302e-01, -6.1572e-01],\n",
      "          [-1.7461e+00, -4.8477e+00, -1.0488e+00]]],\n",
      "\n",
      "\n",
      "        [[[-5.6094e+00, -1.1164e+01, -4.3906e+00],\n",
      "          [-1.3242e+00, -7.9932e-01, -2.8247e-01],\n",
      "          [ 6.8047e+00,  1.1445e+01,  5.4062e+00]],\n",
      "\n",
      "         [[-5.3086e+00, -1.2523e+01, -4.4219e+00],\n",
      "          [-7.0654e-01, -1.6113e-01,  8.4863e-01],\n",
      "          [ 6.1055e+00,  1.0758e+01,  4.5625e+00]],\n",
      "\n",
      "         [[-2.2148e+00, -6.2617e+00, -1.9199e+00],\n",
      "          [-9.6533e-01, -3.0981e-01,  5.1123e-01],\n",
      "          [ 2.6934e+00,  5.3438e+00,  2.4121e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 6.0742e+00, -9.0078e+00, -2.6738e+00],\n",
      "          [ 1.3109e+01, -4.0469e+00,  4.9062e+00],\n",
      "          [ 3.5957e+00, -8.5078e+00,  4.6094e-01]],\n",
      "\n",
      "         [[ 3.2617e+00, -1.1484e+01, -1.3513e-01],\n",
      "          [ 1.0133e+01, -5.0273e+00,  1.1039e+01],\n",
      "          [-9.9854e-01, -1.0359e+01,  4.0938e+00]],\n",
      "\n",
      "         [[-3.1006e-01, -3.4980e+00,  1.3887e+00],\n",
      "          [ 1.4463e+00, -1.9092e+00,  5.9727e+00],\n",
      "          [-2.3945e+00, -3.8340e+00,  2.1816e+00]]],\n",
      "\n",
      "\n",
      "        [[[-5.5000e+00,  1.3609e+01, -7.7461e+00],\n",
      "          [-8.3750e+00,  2.1125e+01, -1.3641e+01],\n",
      "          [-4.2031e+00,  1.1555e+01, -7.1719e+00]],\n",
      "\n",
      "         [[-9.4219e+00,  1.6438e+01, -7.0000e+00],\n",
      "          [-1.5180e+01,  2.5891e+01, -1.0648e+01],\n",
      "          [-7.3281e+00,  1.2383e+01, -5.3203e+00]],\n",
      "\n",
      "         [[-2.3516e+00,  4.1211e+00, -1.5283e+00],\n",
      "          [-6.7773e+00,  8.6797e+00, -2.5781e+00],\n",
      "          [-2.2852e+00,  3.4238e+00, -1.1104e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 2.6348e+00, -1.0008e+01, -1.7547e+01],\n",
      "          [ 5.5742e+00, -5.4453e+00, -9.6172e+00],\n",
      "          [ 6.5469e+00,  4.9170e-01,  8.2471e-01]],\n",
      "\n",
      "         [[ 8.0371e-01,  8.3984e+00,  1.1836e+01],\n",
      "          [-2.2871e+00,  3.8574e+00,  5.7969e+00],\n",
      "          [-2.1406e+00,  4.9390e-01,  1.5127e+00]],\n",
      "\n",
      "         [[-3.0703e+00,  2.0078e+00,  7.2109e+00],\n",
      "          [-3.6855e+00,  1.3516e+00,  2.8809e+00],\n",
      "          [-3.1465e+00,  6.3965e-01,  4.4604e-01]]],\n",
      "\n",
      "\n",
      "        [[[-7.7500e+00, -7.7969e+00, -3.8828e+00],\n",
      "          [-6.9727e+00, -6.1602e+00, -1.9277e+00],\n",
      "          [-6.6797e+00, -5.0156e+00, -2.6367e+00]],\n",
      "\n",
      "         [[ 1.5000e+01,  1.4219e+01,  1.5562e+01],\n",
      "          [ 1.2523e+01,  1.3562e+01,  1.2422e+01],\n",
      "          [ 9.4375e+00,  8.0312e+00,  8.8203e+00]],\n",
      "\n",
      "         [[-7.4219e+00, -7.6719e+00, -1.1055e+01],\n",
      "          [-5.1602e+00, -8.1484e+00, -9.8594e+00],\n",
      "          [-3.4941e+00, -4.7930e+00, -6.7227e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 3.8984e+00, -1.4131e+00,  5.1328e+00],\n",
      "          [-4.2422e+00, -2.5172e+01, -4.5547e+00],\n",
      "          [ 3.2363e+00, -5.4766e+00,  2.2832e+00]],\n",
      "\n",
      "         [[ 3.3418e+00,  2.0215e+00,  5.6875e+00],\n",
      "          [-3.0469e-01, -2.0891e+01, -4.1772e-01],\n",
      "          [ 4.1289e+00, -1.5889e+00,  4.6328e+00]],\n",
      "\n",
      "         [[-4.5264e-01, -2.7129e+00, -8.3191e-02],\n",
      "          [-1.8457e+00, -1.1242e+01, -2.7539e-01],\n",
      "          [ 7.9834e-01, -3.6426e+00,  2.2969e+00]]],\n",
      "\n",
      "\n",
      "        [[[-8.7812e+00,  3.0273e+00,  7.1211e+00],\n",
      "          [ 2.2773e+00, -6.5273e+00,  5.5713e-01],\n",
      "          [ 1.0344e+01,  1.3594e+00, -9.4453e+00]],\n",
      "\n",
      "         [[-1.3375e+01,  2.2148e+00,  1.3477e+01],\n",
      "          [ 6.0254e-01, -8.7812e+00,  2.1953e+00],\n",
      "          [ 1.5352e+01,  7.6953e-01, -1.2203e+01]],\n",
      "\n",
      "         [[-6.2070e+00,  3.2695e+00,  5.1641e+00],\n",
      "          [-1.6777e+00, -4.9961e+00,  5.2227e+00],\n",
      "          [ 4.7305e+00,  1.0242e-01, -5.4414e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 3.8340e+00,  4.4922e+00,  5.1992e+00],\n",
      "          [ 6.3867e+00,  9.5938e+00,  7.6094e+00],\n",
      "          [ 6.5000e+00,  6.7578e+00,  5.7891e+00]],\n",
      "\n",
      "         [[-1.2900e+00, -1.9551e+00, -2.4883e+00],\n",
      "          [-2.9707e+00, -4.9180e+00, -4.8555e+00],\n",
      "          [-2.1738e+00, -3.8652e+00, -3.5527e+00]],\n",
      "\n",
      "         [[-2.5371e+00, -1.5527e+00, -1.1992e+00],\n",
      "          [-3.7988e+00, -4.1836e+00, -3.0566e+00],\n",
      "          [-4.6406e+00, -3.5547e+00, -2.5703e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "for i in (model_n.parameters()):\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5176ba2c-1b22-4deb-9e28-207787056e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7f1e173f-d85c-4a75-8ab9-64db55321d8b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 6.4258e+00, -2.4766e+00,  2.6973e+00],\n",
      "          [ 2.9336e+00, -1.9578e+01, -6.4404e-01],\n",
      "          [ 5.9531e+00, -1.8320e+00,  5.4219e+00]],\n",
      "\n",
      "         [[ 7.5898e+00, -3.4043e+00,  5.2656e+00],\n",
      "          [ 8.5645e-01, -2.4359e+01,  1.3779e+00],\n",
      "          [ 5.9336e+00, -1.3379e+00,  8.2656e+00]],\n",
      "\n",
      "         [[ 1.8193e+00, -3.2617e-01,  2.2734e+00],\n",
      "          [-3.2324e-01, -8.5391e+00,  1.2676e+00],\n",
      "          [ 7.6562e-01, -2.8418e-01,  2.8613e+00]]],\n",
      "\n",
      "\n",
      "        [[[-4.4141e+00, -5.3594e+00, -1.1846e+00],\n",
      "          [-1.2844e+01, -1.3219e+01, -4.8984e+00],\n",
      "          [-1.6219e+01, -1.1250e+01, -4.8008e+00]],\n",
      "\n",
      "         [[ 1.7080e+00,  6.2500e-01, -1.7188e+00],\n",
      "          [ 4.8398e+00,  2.9902e+00, -2.4585e-01],\n",
      "          [ 6.2305e+00,  2.3594e+00, -1.8860e-02]],\n",
      "\n",
      "         [[ 1.4053e+00,  3.7676e+00,  1.7725e+00],\n",
      "          [ 8.1875e+00,  1.0406e+01,  5.7461e+00],\n",
      "          [ 8.5000e+00,  7.1055e+00,  3.8672e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.9844e+01,  3.6113e+00,  2.1125e+01],\n",
      "          [-3.9781e+01,  3.5332e+00,  3.5156e+01],\n",
      "          [-2.4219e+01,  6.0498e-01,  2.0719e+01]],\n",
      "\n",
      "         [[ 8.9219e+00, -4.2891e+00, -4.3359e+00],\n",
      "          [ 1.3023e+01, -2.0234e+00, -1.0859e+01],\n",
      "          [ 6.3398e+00, -7.2705e-01, -6.2656e+00]],\n",
      "\n",
      "         [[ 1.3633e+01, -3.8613e+00, -1.5125e+01],\n",
      "          [ 2.9312e+01, -1.1162e+00, -2.7750e+01],\n",
      "          [ 1.6906e+01,  2.2852e+00, -1.5547e+01]]],\n",
      "\n",
      "\n",
      "        [[[ 7.3203e+00, -1.0439e+00, -5.7188e+00],\n",
      "          [ 1.3367e+01, -4.8047e-01, -1.4320e+01],\n",
      "          [ 8.3672e+00,  1.7249e-01, -7.6953e+00]],\n",
      "\n",
      "         [[ 6.6133e+00, -4.0479e-01, -5.4258e+00],\n",
      "          [ 1.3688e+01,  1.6998e-02, -1.5961e+01],\n",
      "          [ 6.8281e+00,  9.6875e-01, -7.0469e+00]],\n",
      "\n",
      "         [[ 9.4482e-01, -1.9348e-01, -3.3252e-01],\n",
      "          [ 3.5781e+00,  4.2017e-01, -5.5078e+00],\n",
      "          [ 8.4570e-01,  7.6270e-01, -1.3154e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8975e+00, -2.4258e+00, -3.9336e+00],\n",
      "          [ 1.6670e+00, -2.0918e+00, -2.4023e+00],\n",
      "          [-1.7761e-01,  1.4150e+00,  3.4355e+00]],\n",
      "\n",
      "         [[ 2.2598e+00,  1.0034e-01, -2.5117e+00],\n",
      "          [ 6.5381e-01, -4.3896e-01, -4.5581e-01],\n",
      "          [-2.4043e+00,  1.0762e+00,  6.0645e-01]],\n",
      "\n",
      "         [[-1.8867e+00,  8.0643e-03, -1.9873e-01],\n",
      "          [-6.0645e-01, -5.1562e-01, -1.0488e+00],\n",
      "          [-6.6846e-01, -1.7896e-01, -3.0176e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 5.1875e+00,  3.8281e+00,  3.1602e+00],\n",
      "          [ 6.5781e+00,  6.9688e+00,  6.1875e+00],\n",
      "          [ 4.3008e+00,  3.1621e+00,  4.5078e+00]],\n",
      "\n",
      "         [[-1.1922e+01, -1.3070e+01, -1.0906e+01],\n",
      "          [-1.6844e+01, -1.9781e+01, -1.7625e+01],\n",
      "          [-1.3117e+01, -1.3828e+01, -1.3328e+01]],\n",
      "\n",
      "         [[ 6.1680e+00,  8.7500e+00,  7.2852e+00],\n",
      "          [ 9.8047e+00,  1.3156e+01,  1.0227e+01],\n",
      "          [ 9.5156e+00,  1.0016e+01,  9.3203e+00]]],\n",
      "\n",
      "\n",
      "        [[[-3.5410e+00, -1.3418e+00,  5.2461e+00],\n",
      "          [-9.9453e+00, -1.7793e+00,  1.1727e+01],\n",
      "          [-4.7344e+00, -1.7500e+00,  6.1641e+00]],\n",
      "\n",
      "         [[-5.6211e+00, -1.6279e+00,  7.2070e+00],\n",
      "          [-1.4219e+01, -1.8633e+00,  1.5391e+01],\n",
      "          [-6.1016e+00, -1.4355e+00,  7.5625e+00]],\n",
      "\n",
      "         [[-2.0117e+00, -8.9502e-01,  2.9180e+00],\n",
      "          [-7.0898e+00, -6.3916e-01,  6.9570e+00],\n",
      "          [-3.3750e+00, -3.1274e-01,  3.9492e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 6.5820e+00,  1.1969e+01,  6.2734e+00],\n",
      "          [ 3.4375e-01,  3.2373e-01, -4.1870e-01],\n",
      "          [-6.8281e+00, -1.2297e+01, -5.9531e+00]],\n",
      "\n",
      "         [[ 6.3906e+00,  1.3391e+01,  6.9336e+00],\n",
      "          [ 3.1403e-02, -7.9248e-01, -1.0078e+00],\n",
      "          [-6.6602e+00, -1.3664e+01, -5.3516e+00]],\n",
      "\n",
      "         [[ 1.0088e+00,  4.4336e+00,  1.6943e+00],\n",
      "          [ 2.4573e-01,  3.4302e-01, -6.1572e-01],\n",
      "          [-1.7461e+00, -4.8477e+00, -1.0488e+00]]],\n",
      "\n",
      "\n",
      "        [[[-5.6094e+00, -1.1164e+01, -4.3906e+00],\n",
      "          [-1.3242e+00, -7.9932e-01, -2.8247e-01],\n",
      "          [ 6.8047e+00,  1.1445e+01,  5.4062e+00]],\n",
      "\n",
      "         [[-5.3086e+00, -1.2523e+01, -4.4219e+00],\n",
      "          [-7.0654e-01, -1.6113e-01,  8.4863e-01],\n",
      "          [ 6.1055e+00,  1.0758e+01,  4.5625e+00]],\n",
      "\n",
      "         [[-2.2148e+00, -6.2617e+00, -1.9199e+00],\n",
      "          [-9.6533e-01, -3.0981e-01,  5.1123e-01],\n",
      "          [ 2.6934e+00,  5.3438e+00,  2.4121e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 6.0742e+00, -9.0078e+00, -2.6738e+00],\n",
      "          [ 1.3109e+01, -4.0469e+00,  4.9062e+00],\n",
      "          [ 3.5957e+00, -8.5078e+00,  4.6094e-01]],\n",
      "\n",
      "         [[ 3.2617e+00, -1.1484e+01, -1.3513e-01],\n",
      "          [ 1.0133e+01, -5.0273e+00,  1.1039e+01],\n",
      "          [-9.9854e-01, -1.0359e+01,  4.0938e+00]],\n",
      "\n",
      "         [[-3.1006e-01, -3.4980e+00,  1.3887e+00],\n",
      "          [ 1.4463e+00, -1.9092e+00,  5.9727e+00],\n",
      "          [-2.3945e+00, -3.8340e+00,  2.1816e+00]]],\n",
      "\n",
      "\n",
      "        [[[-5.5000e+00,  1.3609e+01, -7.7461e+00],\n",
      "          [-8.3750e+00,  2.1125e+01, -1.3641e+01],\n",
      "          [-4.2031e+00,  1.1555e+01, -7.1719e+00]],\n",
      "\n",
      "         [[-9.4219e+00,  1.6438e+01, -7.0000e+00],\n",
      "          [-1.5180e+01,  2.5891e+01, -1.0648e+01],\n",
      "          [-7.3281e+00,  1.2383e+01, -5.3203e+00]],\n",
      "\n",
      "         [[-2.3516e+00,  4.1211e+00, -1.5283e+00],\n",
      "          [-6.7773e+00,  8.6797e+00, -2.5781e+00],\n",
      "          [-2.2852e+00,  3.4238e+00, -1.1104e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 2.6348e+00, -1.0008e+01, -1.7547e+01],\n",
      "          [ 5.5742e+00, -5.4453e+00, -9.6172e+00],\n",
      "          [ 6.5469e+00,  4.9170e-01,  8.2471e-01]],\n",
      "\n",
      "         [[ 8.0371e-01,  8.3984e+00,  1.1836e+01],\n",
      "          [-2.2871e+00,  3.8574e+00,  5.7969e+00],\n",
      "          [-2.1406e+00,  4.9390e-01,  1.5127e+00]],\n",
      "\n",
      "         [[-3.0703e+00,  2.0078e+00,  7.2109e+00],\n",
      "          [-3.6855e+00,  1.3516e+00,  2.8809e+00],\n",
      "          [-3.1465e+00,  6.3965e-01,  4.4604e-01]]],\n",
      "\n",
      "\n",
      "        [[[-7.7500e+00, -7.7969e+00, -3.8828e+00],\n",
      "          [-6.9727e+00, -6.1602e+00, -1.9277e+00],\n",
      "          [-6.6797e+00, -5.0156e+00, -2.6367e+00]],\n",
      "\n",
      "         [[ 1.5000e+01,  1.4219e+01,  1.5562e+01],\n",
      "          [ 1.2523e+01,  1.3562e+01,  1.2422e+01],\n",
      "          [ 9.4375e+00,  8.0312e+00,  8.8203e+00]],\n",
      "\n",
      "         [[-7.4219e+00, -7.6719e+00, -1.1055e+01],\n",
      "          [-5.1602e+00, -8.1484e+00, -9.8594e+00],\n",
      "          [-3.4941e+00, -4.7930e+00, -6.7227e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 3.8984e+00, -1.4131e+00,  5.1328e+00],\n",
      "          [-4.2422e+00, -2.5172e+01, -4.5547e+00],\n",
      "          [ 3.2363e+00, -5.4766e+00,  2.2832e+00]],\n",
      "\n",
      "         [[ 3.3418e+00,  2.0215e+00,  5.6875e+00],\n",
      "          [-3.0469e-01, -2.0891e+01, -4.1772e-01],\n",
      "          [ 4.1289e+00, -1.5889e+00,  4.6328e+00]],\n",
      "\n",
      "         [[-4.5264e-01, -2.7129e+00, -8.3191e-02],\n",
      "          [-1.8457e+00, -1.1242e+01, -2.7539e-01],\n",
      "          [ 7.9834e-01, -3.6426e+00,  2.2969e+00]]],\n",
      "\n",
      "\n",
      "        [[[-8.7812e+00,  3.0273e+00,  7.1211e+00],\n",
      "          [ 2.2773e+00, -6.5273e+00,  5.5713e-01],\n",
      "          [ 1.0344e+01,  1.3594e+00, -9.4453e+00]],\n",
      "\n",
      "         [[-1.3375e+01,  2.2148e+00,  1.3477e+01],\n",
      "          [ 6.0254e-01, -8.7812e+00,  2.1953e+00],\n",
      "          [ 1.5352e+01,  7.6953e-01, -1.2203e+01]],\n",
      "\n",
      "         [[-6.2070e+00,  3.2695e+00,  5.1641e+00],\n",
      "          [-1.6777e+00, -4.9961e+00,  5.2227e+00],\n",
      "          [ 4.7305e+00,  1.0242e-01, -5.4414e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 3.8340e+00,  4.4922e+00,  5.1992e+00],\n",
      "          [ 6.3867e+00,  9.5938e+00,  7.6094e+00],\n",
      "          [ 6.5000e+00,  6.7578e+00,  5.7891e+00]],\n",
      "\n",
      "         [[-1.2900e+00, -1.9551e+00, -2.4883e+00],\n",
      "          [-2.9707e+00, -4.9180e+00, -4.8555e+00],\n",
      "          [-2.1738e+00, -3.8652e+00, -3.5527e+00]],\n",
      "\n",
      "         [[-2.5371e+00, -1.5527e+00, -1.1992e+00],\n",
      "          [-3.7988e+00, -4.1836e+00, -3.0566e+00],\n",
      "          [-4.6406e+00, -3.5547e+00, -2.5703e+00]]]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "for i in model.to(torch.float16).parameters():\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "69ced134-3c27-4859-93f5-a9cf3ea4ccaa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): SegmentationModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): C2PSA(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): Sequential(\n",
       "          (0): PSABlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): Conv(\n",
       "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (proj): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (pe): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (ffn): Sequential(\n",
       "              (0): Conv(\n",
       "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (12): Concat()\n",
       "      (13): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (15): Concat()\n",
       "      (16): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (18): Concat()\n",
       "      (19): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (21): Concat()\n",
       "      (22): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): Segment(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (proto): Proto(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (upsample): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (cv4): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model16.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cf4e43ce-f953-4be7-90f9-b2aa7b163830",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): SegmentationModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): C2PSA(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): Sequential(\n",
       "          (0): PSABlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): Conv(\n",
       "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (proj): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (pe): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (ffn): Sequential(\n",
       "              (0): Conv(\n",
       "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (12): Concat()\n",
       "      (13): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (15): Concat()\n",
       "      (16): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (18): Concat()\n",
       "      (19): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (21): Concat()\n",
       "      (22): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): Segment(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (proto): Proto(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (upsample): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (cv4): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_n.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a14c648e-7800-4bff-997e-d99733fb4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "model16 = model.to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7b3068af-02d5-4189-9235-2705378e5314",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): SegmentationModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): C2PSA(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): Sequential(\n",
       "          (0): PSABlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): Conv(\n",
       "                (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (proj): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (pe): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (ffn): Sequential(\n",
       "              (0): Conv(\n",
       "                (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (12): Concat()\n",
       "      (13): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (15): Concat()\n",
       "      (16): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (18): Concat()\n",
       "      (19): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (21): Concat()\n",
       "      (22): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): Segment(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (proto): Proto(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (upsample): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (cv4): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(512, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5ce39f9c-5286-4e65-bef9-ce246a522bf6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): SegmentationModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): C2PSA(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): Sequential(\n",
       "          (0): PSABlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): Conv(\n",
       "                (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (proj): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (pe): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (ffn): Sequential(\n",
       "              (0): Conv(\n",
       "                (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (12): Concat()\n",
       "      (13): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (15): Concat()\n",
       "      (16): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (18): Concat()\n",
       "      (19): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (21): Concat()\n",
       "      (22): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): Segment(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (proto): Proto(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (upsample): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (cv4): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(512, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model16.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b57e36-6d58-4458-8342-a1dc565a389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "98ee4f2d-5a9c-4323-9068-ff0d5b46ed9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected m1 and m2 to have the same dtype, but got: c10::Half != float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/images/train/1710275253_0.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/ultralytics/engine/model.py:180\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    153\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    154\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/ultralytics/engine/model.py:551\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor:\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor \u001b[38;5;241m=\u001b[39m (predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictor\u001b[39m\u001b[38;5;124m\"\u001b[39m))(overrides\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[0;32m--> 551\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_cli\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# only update args if predictor is already setup\u001b[39;00m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m get_cfg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs, args)\n",
      "File \u001b[0;32m~/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/ultralytics/engine/predictor.py:308\u001b[0m, in \u001b[0;36mBasePredictor.setup_model\u001b[0;34m(self, model, verbose)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    307\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize YOLO model with given parameters and set it to evaluation mode.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoBackend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice  \u001b[38;5;66;03m# update device\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfp16  \u001b[38;5;66;03m# update half\u001b[39;00m\n",
      "File \u001b[0;32m~/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/ultralytics/nn/autobackend.py:148\u001b[0m, in \u001b[0;36mAutoBackend.__init__\u001b[0;34m(self, weights, device, dnn, data, fp16, batch, fuse, verbose)\u001b[0m\n\u001b[1;32m    146\u001b[0m model \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fuse:\n\u001b[0;32m--> 148\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkpt_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    150\u001b[0m     kpt_shape \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mkpt_shape  \u001b[38;5;66;03m# pose-only\u001b[39;00m\n",
      "File \u001b[0;32m~/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/ultralytics/nn/tasks.py:207\u001b[0m, in \u001b[0;36mBaseModel.fuse\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, Conv2):\n\u001b[1;32m    206\u001b[0m     m\u001b[38;5;241m.\u001b[39mfuse_convs()\n\u001b[0;32m--> 207\u001b[0m m\u001b[38;5;241m.\u001b[39mconv \u001b[38;5;241m=\u001b[39m \u001b[43mfuse_conv_and_bn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# update conv\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mdelattr\u001b[39m(m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbn\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# remove batchnorm\u001b[39;00m\n\u001b[1;32m    209\u001b[0m m\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mforward_fuse  \u001b[38;5;66;03m# update forward\u001b[39;00m\n",
      "File \u001b[0;32m~/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/ultralytics/utils/torch_utils.py:267\u001b[0m, in \u001b[0;36mfuse_conv_and_bn\u001b[0;34m(conv, bn)\u001b[0m\n\u001b[1;32m    265\u001b[0m b_conv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(conv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39mconv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mif\u001b[39;00m conv\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m conv\u001b[38;5;241m.\u001b[39mbias\n\u001b[1;32m    266\u001b[0m b_bn \u001b[38;5;241m=\u001b[39m bn\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m-\u001b[39m bn\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mmul(bn\u001b[38;5;241m.\u001b[39mrunning_mean)\u001b[38;5;241m.\u001b[39mdiv(torch\u001b[38;5;241m.\u001b[39msqrt(bn\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;241m+\u001b[39m bn\u001b[38;5;241m.\u001b[39meps))\n\u001b[0;32m--> 267\u001b[0m fusedconv\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mcopy_(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw_bn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_conv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m b_bn)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fusedconv\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected m1 and m2 to have the same dtype, but got: c10::Half != float"
     ]
    }
   ],
   "source": [
    "model('/home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/images/train/1710275253_0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "123810f7-15c4-4b29-8b1d-f51430236484",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('/home/jovyan/nazar/123/123/misis_chill/train_dataset/cv_open_dataset/open_img/1710275253_0.jpg')\n",
    "image = add_sobel_as_fourth_channel(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "bf63e37e-bef8-4b46-82e1-d40b0793328f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1920, 4)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "08d14b06-9946-4828-8445-5d44ded0e3c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): SegmentationModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): C2PSA(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): Sequential(\n",
       "          (0): PSABlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): Conv(\n",
       "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (proj): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (pe): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (ffn): Sequential(\n",
       "              (0): Conv(\n",
       "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (12): Concat()\n",
       "      (13): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (15): Concat()\n",
       "      (16): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (18): Concat()\n",
       "      (19): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (21): Concat()\n",
       "      (22): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): Segment(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (proto): Proto(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (upsample): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (cv4): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3f1461eb-95d2-4ccc-830e-6db092d0a6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/images/train/1710275253_0.jpg: 384x640 (no detections), 225.3ms\n",
      "Speed: 2.4ms preprocess, 225.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       " obb: None\n",
       " orig_img: array([[[ 21,  24,  32],\n",
       "         [ 21,  24,  32],\n",
       "         [ 22,  24,  32],\n",
       "         ...,\n",
       "         [ 94,  82,  64],\n",
       "         [ 91,  79,  61],\n",
       "         [ 97,  85,  67]],\n",
       " \n",
       "        [[ 21,  24,  32],\n",
       "         [ 21,  24,  32],\n",
       "         [ 22,  24,  32],\n",
       "         ...,\n",
       "         [ 96,  84,  66],\n",
       "         [ 92,  80,  62],\n",
       "         [ 93,  81,  63]],\n",
       " \n",
       "        [[ 22,  24,  32],\n",
       "         [ 22,  24,  32],\n",
       "         [ 22,  24,  32],\n",
       "         ...,\n",
       "         [ 97,  85,  67],\n",
       "         [ 94,  82,  64],\n",
       "         [ 95,  83,  65]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[140, 123, 102],\n",
       "         [139, 123, 100],\n",
       "         [133, 116,  95],\n",
       "         ...,\n",
       "         [ 67,  65,  47],\n",
       "         [ 67,  64,  49],\n",
       "         [ 67,  65,  47]],\n",
       " \n",
       "        [[137, 122, 103],\n",
       "         [136, 122, 100],\n",
       "         [131, 116,  97],\n",
       "         ...,\n",
       "         [ 56,  56,  40],\n",
       "         [ 58,  55,  41],\n",
       "         [ 58,  55,  40]],\n",
       " \n",
       "        [[137, 122, 103],\n",
       "         [137, 122, 103],\n",
       "         [141, 127, 108],\n",
       "         ...,\n",
       "         [ 51,  50,  36],\n",
       "         [ 53,  50,  36],\n",
       "         [ 53,  50,  36]]], dtype=uint8)\n",
       " orig_shape: (1080, 1920)\n",
       " path: '/home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/images/train/1710275253_0.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs/segment/train62'\n",
       " speed: {'preprocess': 2.419710159301758, 'inference': 225.2960205078125, 'postprocess': 0.4367828369140625}]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model16('/home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/images/train/1710275253_0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc25c72-b65b-408a-b387-f148d92918a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-nmkarpov]",
   "language": "python",
   "name": "conda-env-.mlspace-nmkarpov-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
