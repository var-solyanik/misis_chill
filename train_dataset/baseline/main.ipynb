{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cfccd41-bc39-4548-8ca0-cb622267c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sobel_as_fourth_channel(image):\n",
    "    \"\"\"\n",
    "    Adds the Sobel operator (edge detection) result as the fourth channel to an input image.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image (H, W, 3) read by cv2.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Image with the Sobel operator as the fourth channel (H, W, 4).\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "        print(image)\n",
    "        raise ValueError(\"Input image is None. Please provide a valid image.\")\n",
    "    \n",
    "    if image.shape[-1] != 3:\n",
    "        raise ValueError(\"Input image must have 3 channels (H, W, 3).\")\n",
    "\n",
    "    # Convert to grayscale for edge detection\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Sobel operator (X and Y gradients)\n",
    "    sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)  # Sobel in X direction\n",
    "    sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)  # Sobel in Y direction\n",
    "\n",
    "    # Compute the gradient magnitude\n",
    "    sobel_magnitude = cv2.magnitude(sobel_x, sobel_y)\n",
    "\n",
    "    # Normalize to 8-bit (0-255)\n",
    "    sobel_normalized = cv2.normalize(sobel_magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    sobel_normalized = sobel_normalized.astype(np.uint8)\n",
    "\n",
    "    # Add Sobel as the fourth channel\n",
    "    sobel_channel = np.expand_dims(sobel_normalized, axis=-1)  # Shape (H, W, 1)\n",
    "    image_with_sobel = np.concatenate((image, sobel_channel), axis=-1)  # Shape (H, W, 4)\n",
    "\n",
    "    return image_with_sobel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e89357de-6d2d-4006-983d-8c821181e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_photos_with_sobel(input_photos_dir, output_photos_dir):\n",
    "    try:\n",
    "        files = os.listdir(input_photos_dir)  # Lists all items in the directory\n",
    "        files = [f for f in files if os.path.isfile(os.path.join(input_photos_dir, f))]  # Filter files only with asbolute path with it\n",
    "        print(\"files\", files)\n",
    "        for input_photo_name in files:\n",
    "            if input_photo_name != '.DS_Store':\n",
    "                image = cv2.imread(os.path.join(input_photos_dir, input_photo_name))\n",
    "                # print(input_photo_name)\n",
    "                image_with_sobel = add_sobel_as_fourth_channel(image)\n",
    "                cv2.imwrite(os.path.join(output_photos_dir, input_photo_name.replace(\".jpg\", \".png\").replace('.png', '.tiff')), image_with_sobel)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory not found: {input_photos_dir}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# print(process_photos_with_sobel('/Users/z.gabdrakhmanov/Downloads/train_dataset/cv_open_dataset/open_img', '/Users/z.gabdrakhmanov/vscode/hack/misis2024s-23-01-gabdrakhmanov-z-i/nazar'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f5ec2f4a-ddc4-44cc-851d-803247dcb534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files ['.DS_Store', '1710275253_0.jpg', '1710277054_0.jpg', '1710322071_0.jpg', '1710370672_0.jpg', '1710381477_0.jpg', '1710390473_0.jpg', '1710399478_0.jpg', '1710430072_0.jpg', '1710458871_0.jpg', '1710584874_0.jpg', '1710604665_0.jpg', '1710809853_0.jpg', '1710811652_0.jpg', '1710813452_0.jpg', '1710815253_0.jpg', '1710835053_0.jpg', '1710847680_0.jpg', '1710917853_0.jpg', '1711009681_0.jpg', '1711090673_0.jpg', '1711119479_0.jpg', '1711177052_0.jpg', '1711178852_0.jpg', '1711178872_0.jpg', '1711180652_0.jpg', '1711182453_0.jpg', '1711184253_0.jpg', '1711186053_0.jpg', '1711187868_0.jpg', '1711256271_0.jpg', '1711268872_0.jpg', '1711268876_0.jpg', '1711330053_0.jpg', '1711393084_0.jpg', '1711412853_0.jpg', '1711421860_0.jpg', '1711430853_0.jpg', '1711448853_0.jpg', '1711450664_0.jpg', '1711456067_0.jpg', '1711459680_0.jpg', '1711510053_0.jpg', '1712555862_0.jpg', '1712564893_0.jpg', '1712577488_0.jpg', '1712741269_0.jpg', '1712788070_0.jpg', '1713302111_0.jpg', '1713350757_0.jpg', '1713767248_0.jpg', '1715041648_0.jpg', '1715074048_0.jpg', '1715112967_0.jpg', '1715158095_0.jpg', '1716017248_0.jpg', '1717022950_0.jpg', '1717058869_0.jpg', '1717059450_0.jpg', '1717060391_0.jpg', '1717061248_0.jpg', '1717063048_0.jpg', '1717068448_0.jpg', '1717390039_0.jpg', 'F1_1_1_1.ts_f_1000.jpg', 'F1_1_1_1.ts_f_500.jpg', 'F1_1_1_2.ts_f_1000.jpg', 'F1_1_1_2.ts_f_500.jpg', 'F1_1_2_1.ts_f_1000.jpg', 'F1_1_2_1.ts_f_500.jpg', 'F1_1_2_2.ts_f_1000.jpg', 'F1_1_2_2.ts_f_500.jpg', 'F1_1_3_1.ts_f_1000.jpg', 'F1_1_3_1.ts_f_500.jpg', 'F1_1_4_2.ts_f_1000.jpg', 'F1_1_4_2.ts_f_500.jpg', 'F1_1_5_1.ts_f_1000.jpg', 'F1_1_5_1.ts_f_500.jpg', 'F1_1_5_2.ts_f_1000.jpg', 'F1_1_5_2.ts_f_500.jpg', 'F1_2_2_1.ts_f_1000.jpg', 'F1_2_2_1.ts_f_500.jpg', 'F1_2_2_2.ts_f_1000.jpg', 'F1_2_2_2.ts_f_500.jpg', 'F1_2_3_1.ts_f_1000.jpg', 'F1_2_3_1.ts_f_500.jpg', 'F1_2_3_2.ts_f_1000.jpg', 'F1_2_3_2.ts_f_500.jpg', 'F1_2_4_1.ts_f_1000.jpg', 'F1_2_4_1.ts_f_500.jpg', 'F1_2_4_2.ts_f_1000.jpg', 'F1_2_4_2.ts_f_500.jpg', 'F1_2_5_1.ts_f_1000.jpg', 'F1_2_5_1.ts_f_500.jpg', 'F1_2_5_2.ts_f_1000.jpg', 'F1_2_5_2.ts_f_500.jpg', 'F2_1_1_1.ts_f_1000.jpg', 'F2_1_1_1.ts_f_500.jpg', 'F2_1_1_2.ts_f_1000.jpg', 'F2_1_1_2.ts_f_500.jpg', 'F2_1_2_2.ts_f_1000.jpg', 'F2_1_2_2.ts_f_500.jpg', 'F2_2_1_1.ts_f_1000.jpg', 'F2_2_1_1.ts_f_500.jpg', 'F2_2_1_2.ts_f_1000.jpg', 'F2_2_1_2.ts_f_500.jpg', 'F2_2_2_1.ts_f_1000.jpg', 'F2_2_2_1.ts_f_500.jpg', 'F2_2_2_2.ts_f_1000.jpg', 'F2_2_2_2.ts_f_500.jpg', 'F2_2_3_1.ts_f_1000.jpg', 'F2_2_3_1.ts_f_500.jpg', 'F2_2_3_2.ts_f_1000.jpg', 'F2_2_3_2.ts_f_500.jpg', 'F4_1_1_1.ts_f_1000.jpg', 'F4_1_1_1.ts_f_500.jpg', 'F4_1_1_2.ts_f_1000.jpg', 'F4_1_1_2.ts_f_500.jpg', 'F4_1_2_1.ts_f_1000.jpg', 'F4_1_2_1.ts_f_500.jpg', 'F4_1_2_2.ts_f_1000.jpg', 'F4_1_2_2.ts_f_500.jpg', 'F4_1_3_1.ts_f_1000.jpg', 'F4_1_3_1.ts_f_500.jpg', 'F4_1_3_2.ts_f_1000.jpg', 'F4_1_3_2.ts_f_500.jpg', 'F4_2_2_1.ts_f_1000.jpg', 'F4_2_2_1.ts_f_500.jpg', 'F4_2_2_2.ts_f_1000.jpg', 'F4_2_2_2.ts_f_500.jpg', 'F4_2_3_1.ts_f_1000.jpg', 'F4_2_3_1.ts_f_500.jpg', 'F4_2_3_2.ts_f_1000.jpg', 'F4_2_3_2.ts_f_500.jpg', 'F5_1_1_1.ts_f_1000.jpg', 'F5_1_1_1.ts_f_500.jpg', 'F5_1_1_2.ts_f_1000.jpg', 'F5_1_1_2.ts_f_500.jpg', 'F5_1_2_1.ts_f_1000.jpg', 'F5_1_2_1.ts_f_500.jpg', 'F5_1_2_2.ts_f_1000.jpg', 'F5_1_2_2.ts_f_500.jpg', 'F5_1_3_1.ts_f_1000.jpg', 'F5_1_3_1.ts_f_500.jpg', 'F5_1_3_2.ts_f_1000.jpg', 'F5_1_3_2.ts_f_500.jpg', 'F5_2_1_1.ts_f_1000.jpg', 'F5_2_1_1.ts_f_500.jpg', 'F5_2_1_2.ts_f_1000.jpg', 'F5_2_1_2.ts_f_500.jpg', 'F5_2_2_1.ts_f_1000.jpg', 'F5_2_2_1.ts_f_500.jpg', 'F5_2_2_2.ts_f_1000.jpg', 'F5_2_2_2.ts_f_500.jpg', 'F5_2_3_1.ts_f_1000.jpg', 'F5_2_3_1.ts_f_500.jpg', 'F5_2_3_2.ts_f_1000.jpg', 'F5_2_3_2.ts_f_500.jpg', 'F7_1_1_1.ts_f_1000.jpg', 'F7_1_1_1.ts_f_500.jpg', 'F7_1_1_2.ts_f_1000.jpg', 'F7_1_1_2.ts_f_500.jpg', 'F7_1_2_1.ts_f_1000.jpg', 'F7_1_2_1.ts_f_500.jpg', 'F7_1_2_2.ts_f_1000.jpg', 'F7_1_2_2.ts_f_500.jpg', 'F7_2_1_1.ts_f_1000.jpg', 'F7_2_1_1.ts_f_500.jpg', 'F7_2_1_2.ts_f_1000.jpg', 'F7_2_1_2.ts_f_500.jpg', 'gr1.jpg']\n"
     ]
    }
   ],
   "source": [
    "process_photos_with_sobel('/home/jovyan/nazar/123/123/misis_chill/train_dataset/cv_open_dataset/open_img', '/home/jovyan/nazar/123/123/misis_chill/train_dataset/cv_open_dataset/open_img_tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "b38bf9d8-bf5a-46b5-89a3-5d6aa2baa2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files ['100_1710813452_0.png', '101_1710835053_0.png', '103_1711186053_0.png', '105_1712788070_0.png', '106_1715112967_0.png', '107_1717060391_0.png', '10_1709255042_0.png', '11_1709260082_0.png', '12_1709260681_0.png', '13_1709261462_0.png', '14_1709262422_0.png', '15_1709265902_0.png', '16_1709265962_0.png', '17_1709276582_0.png', '18_1709310482_0.png', '1_1709104681_0.png', '20_1709344621_0.png', '21_1709388062_0.png', '22_1709859454_0.png', '23_1709859466_0.png', '256_1709104681_0.png', '257_1709109361_0.png', '258_1709116441_0.png', '259_1709186522_0.png', '25_1709888254_0.png', '260_1709187722_0.png', '261_1709191202_0.png', '262_1709232001_0.png', '263_1709253121_0.png', '264_1709254201_0.png', '265_1709255042_0.png', '266_1709260082_0.png', '267_1709260681_0.png', '268_1709261462_0.png', '269_1709262422_0.png', '26_1710079053_0.png', '270_1709265902_0.png', '271_1709265962_0.png', '272_1709276582_0.png', '273_1709310482_0.png', '274_1709339881_0.png', '275_1709344621_0.png', '276_1709388062_0.png', '277_1709859454_0.png', '278_1709859466_0.png', '27_1710253681_0.png', '280_1709888254_0.png', '282_1710253681_0.png', '283_1710260853_0.png', '284_1710262654_0.png', '285_1710266253_0.png', '287_1710809853_0.png', '288_1710813452_0.png', '28_1710260853_0.png', '291_1711186053_0.png', '293_1712788070_0.png', '294_1715112967_0.png', '295_1717060391_0.png', '296_dirty1.png', '29_1710262654_0.png', '2_1709109361_0.png', '30_1710266253_0.png', '31_1710458871_0.png', '32_1710809853_0.png', '33_1710813452_0.png', '34_1710835053_0.png', '36_1711186053_0.png', '38_1712788070_0.png', '39_1715112967_0.png', '3_1709116441_0.png', '401_1709104681_0.png', '402_1709109361_0.png', '403_1709116441_0.png', '404_1709186522_0.png', '405_1709187722_0.png', '406_1709191202_0.png', '407_1709232001_0.png', '408_1709253121_0.png', '409_1709254201_0.png', '40_1717060391_0.png', '410_1709255042_0.png', '411_1709260082_0.png', '412_1709260681_0.png', '413_1709261462_0.png', '414_1709262422_0.png', '415_1709265902_0.png', '416_1709265962_0.png', '417_1709276582_0.png', '418_1709310482_0.png', '419_1709339881_0.png', '41_dirty1.png', '420_1709344621_0.png', '421_1709388062_0.png', '422_1709859454_0.png', '423_1709859466_0.png', '425_1709888254_0.png', '426_1710079053_0.png', '427_1710253681_0.png', '428_1710260853_0.png', '429_1710262654_0.png', '430_1710266253_0.png', '431_1710458871_0.png', '432_1710809853_0.png', '433_1710813452_0.png', '434_1710835053_0.png', '436_1711186053_0.png', '438_1712788070_0.png', '439_1715112967_0.png', '440_1717060391_0.png', '441_dirty1.png', '4_1709186522_0.png', '5_1709187722_0.png', '68_1709104681_0.png', '69_1709109361_0.png', '6_1709191202_0.png', '70_1709116441_0.png', '71_1709186522_0.png', '72_1709187722_0.png', '73_1709191202_0.png', '74_1709232001_0.png', '75_1709253121_0.png', '76_1709254201_0.png', '77_1709255042_0.png', '78_1709260082_0.png', '79_1709260681_0.png', '7_1709232001_0.png', '80_1709261462_0.png', '81_1709262422_0.png', '82_1709265902_0.png', '83_1709265962_0.png', '84_1709276582_0.png', '85_1709310482_0.png', '86_1709339881_0.png', '87_1709344621_0.png', '88_1709388062_0.png', '89_1709859454_0.png', '90_1709859466_0.png', '91_1709886453_0.png', '92_1709888254_0.png', '93_1710079053_0.png', '94_1710253681_0.png', '95_1710260853_0.png', '96_1710262654_0.png', '97_1710266253_0.png', '98_1710458871_0.png', '99_1710809853_0.png', '9_1709254201_0.png']\n"
     ]
    }
   ],
   "source": [
    "process_photos_with_sobel('/home/jovyan/nazar/123/123/misis_chill/train_dataset/cv_synt_dataset/synt_img', '/home/jovyan/nazar/123/123/misis_chill/train_dataset/cv_synt_dataset/synt_img_tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac56f6e6ea79ce6e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "IMAGES_DIR = '/home/jovyan/nazar/123/123/misis_chill/new_photos/cv_open_dataset/open_img'  # Путь к вашему датасету с изображениями\n",
    "MASKS_DIR = '/home/jovyan/nazar/123/123/misis_chill/new_photos/cv_open_dataset/open_msk'  # Путь к вашему датасету с масками\n",
    "OUTPUT_DIR = './datasets/train_data'  # Путь к выходной директории\n",
    "TRAIN_SIZE = 0.8  # Процент обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "406044194f9535d5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(OUTPUT_DIR, 'images/train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'images/val'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'labels/train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'labels/val'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61ba29c30ff4a2dc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_files = [f for f in os.listdir(IMAGES_DIR) if f.endswith(('.jpg', '.png', '.tiff'))]\n",
    "mask_files = [f for f in os.listdir(MASKS_DIR) if f.endswith('.png')]\n",
    "if len(image_files) != len(mask_files):\n",
    "    print(len(image_files))\n",
    "    print(len(mask_files))\n",
    "    print(\"Количество изображений и масок не совпадает.\")\n",
    "    for mask in mask_files:\n",
    "        if mask.replace(\".png\", \".jpg\") not in image_files:\n",
    "            print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "835ee47d-c89d-409a-a645-266ab030cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_DIR2 = '/home/jovyan/nazar/123/123/misis_chill/new_photos/cv_synt_dataset/synt_img'  # Путь к вашему датасету с изображениями\n",
    "MASKS_DIR2 = '/home/jovyan/nazar/123/123/misis_chill/new_photos/cv_synt_dataset/synt_msk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b85c9706-c257-4469-832c-5998629422df",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files += [\"synt\" + f for f in os.listdir(IMAGES_DIR2) if f.endswith(('.jpg', '.png', '.tiff'))]\n",
    "mask_files += [f for f in os.listdir(MASKS_DIR2) if f.endswith('.png')]\n",
    "if len(image_files) != len(mask_files):\n",
    "    print(len(image_files))\n",
    "    print(len(mask_files))\n",
    "    print(\"Количество изображений и масок не совпадает.\")\n",
    "    for mask in mask_files:\n",
    "        if mask.replace(\".png\", \".jpg\") not in image_files:\n",
    "            print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e9828bb-10b2-4bcf-ad6a-43b0d7bb5882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1709046321_0.jpg',\n",
       " '1709046334_0.jpg',\n",
       " '1709080522_0.jpg',\n",
       " '1709101222_0.jpg',\n",
       " '1709103022_0.jpg',\n",
       " '1709104681_0.jpg',\n",
       " '1709109361_0.jpg',\n",
       " '1709116441_0.jpg',\n",
       " '1709117582_0.jpg',\n",
       " '1709143887_0.jpg',\n",
       " '1709186522_0.jpg',\n",
       " '1709187722_0.jpg',\n",
       " '1709191202_0.jpg',\n",
       " '1709232001_0.jpg',\n",
       " '1709253121_0.jpg',\n",
       " '1709254201_0.jpg',\n",
       " '1709255042_0.jpg',\n",
       " '1709255402_0.jpg',\n",
       " '1709260082_0.jpg',\n",
       " '1709260681_0.jpg',\n",
       " '1709261462_0.jpg',\n",
       " '1709262422_0.jpg',\n",
       " '1709263081_0.jpg',\n",
       " '1709265902_0.jpg',\n",
       " '1709265962_0.jpg',\n",
       " '1709269442_0.jpg',\n",
       " '1709271842_0.jpg',\n",
       " '1709276582_0.jpg',\n",
       " '1709278599_0.jpg',\n",
       " '1709278661_0.jpg',\n",
       " '1709283396_0.jpg',\n",
       " '1709310482_0.jpg',\n",
       " '1709339881_0.jpg',\n",
       " '1709344621_0.jpg',\n",
       " '1709388062_0.jpg',\n",
       " '1709520200_0.jpg',\n",
       " '1709550918_0.jpg',\n",
       " '1709587678_0.jpg',\n",
       " '1709807287_0.jpg',\n",
       " '1709827069_0.jpg',\n",
       " '1709855872_0.jpg',\n",
       " '1709857666_0.jpg',\n",
       " '1709859454_0.jpg',\n",
       " '1709859466_0.jpg',\n",
       " '1709863065_0.jpg',\n",
       " '1709864860_0.jpg',\n",
       " '1709864863_0.jpg',\n",
       " '1709866664_0.jpg',\n",
       " '1709868465_0.jpg',\n",
       " '1709870253_0.jpg',\n",
       " '1709870265_0.jpg',\n",
       " '1709872064_0.jpg',\n",
       " '1709875661_0.jpg',\n",
       " '1709875666_0.jpg',\n",
       " '1709877464_0.jpg',\n",
       " '1709879265_0.jpg',\n",
       " '1709881065_0.jpg',\n",
       " '1709882867_0.jpg',\n",
       " '1709884664_0.jpg',\n",
       " '1709886453_0.jpg',\n",
       " '1709888254_0.jpg',\n",
       " '1709893685_0.jpg',\n",
       " '1710079053_0.jpg',\n",
       " '1710147478_0.jpg',\n",
       " '1710253681_0.jpg',\n",
       " '1710260853_0.jpg',\n",
       " '1710262654_0.jpg',\n",
       " '1710262674_0.jpg',\n",
       " '1710264453_0.jpg',\n",
       " '1710264471_0.jpg',\n",
       " '1710266253_0.jpg',\n",
       " '1710268053_0.jpg',\n",
       " '1710269853_0.jpg',\n",
       " '1710271653_0.jpg',\n",
       " '1710273453_0.jpg',\n",
       " '1710273470_0.jpg',\n",
       " '1710275253_0.jpg',\n",
       " '1710277054_0.jpg',\n",
       " '1710322071_0.jpg',\n",
       " '1710370672_0.jpg',\n",
       " '1710381477_0.jpg',\n",
       " '1710390473_0.jpg',\n",
       " '1710399478_0.jpg',\n",
       " '1710430072_0.jpg',\n",
       " '1710458871_0.jpg',\n",
       " '1710584874_0.jpg',\n",
       " '1710604665_0.jpg',\n",
       " '1710809853_0.jpg',\n",
       " '1710811652_0.jpg',\n",
       " '1710813452_0.jpg',\n",
       " '1710815253_0.jpg',\n",
       " '1710835053_0.jpg',\n",
       " '1710847680_0.jpg',\n",
       " '1710917853_0.jpg',\n",
       " '1711009681_0.jpg',\n",
       " '1711090673_0.jpg',\n",
       " '1711119479_0.jpg',\n",
       " '1711177052_0.jpg',\n",
       " '1711178852_0.jpg',\n",
       " '1711178872_0.jpg',\n",
       " '1711180652_0.jpg',\n",
       " '1711182453_0.jpg',\n",
       " '1711184253_0.jpg',\n",
       " '1711186053_0.jpg',\n",
       " '1711187868_0.jpg',\n",
       " '1711256271_0.jpg',\n",
       " '1711268872_0.jpg',\n",
       " '1711268876_0.jpg',\n",
       " '1711330053_0.jpg',\n",
       " '1711393084_0.jpg',\n",
       " '1711412853_0.jpg',\n",
       " '1711421860_0.jpg',\n",
       " '1711430853_0.jpg',\n",
       " '1711448853_0.jpg',\n",
       " '1711450664_0.jpg',\n",
       " '1711456067_0.jpg',\n",
       " '1711459680_0.jpg',\n",
       " '1711510053_0.jpg',\n",
       " '1712555862_0.jpg',\n",
       " '1712564893_0.jpg',\n",
       " '1712577488_0.jpg',\n",
       " '1712741269_0.jpg',\n",
       " '1712788070_0.jpg',\n",
       " '1713302111_0.jpg',\n",
       " '1713350757_0.jpg',\n",
       " '1713767248_0.jpg',\n",
       " '1715041648_0.jpg',\n",
       " '1715074048_0.jpg',\n",
       " '1715112967_0.jpg',\n",
       " '1715158095_0.jpg',\n",
       " '1716017248_0.jpg',\n",
       " '1717022950_0.jpg',\n",
       " '1717058869_0.jpg',\n",
       " '1717059450_0.jpg',\n",
       " '1717060391_0.jpg',\n",
       " '1717061248_0.jpg',\n",
       " '1717063048_0.jpg',\n",
       " '1717068448_0.jpg',\n",
       " '1717390039_0.jpg',\n",
       " '17_1.png',\n",
       " '17_2.png',\n",
       " '29_1.png',\n",
       " '29_2.png',\n",
       " 'F1_1_1_1.ts_f_1000.jpg',\n",
       " 'F1_1_1_1.ts_f_500.jpg',\n",
       " 'F1_1_1_2.ts_f_1000.jpg',\n",
       " 'F1_1_1_2.ts_f_500.jpg',\n",
       " 'F1_1_2_1.ts_f_1000.jpg',\n",
       " 'F1_1_2_1.ts_f_500.jpg',\n",
       " 'F1_1_2_2.ts_f_1000.jpg',\n",
       " 'F1_1_2_2.ts_f_500.jpg',\n",
       " 'F1_1_3_1.ts_f_1000.jpg',\n",
       " 'F1_1_3_1.ts_f_500.jpg',\n",
       " 'F1_1_4_2.ts_f_1000.jpg',\n",
       " 'F1_1_4_2.ts_f_500.jpg',\n",
       " 'F1_1_5_1.ts_f_1000.jpg',\n",
       " 'F1_1_5_1.ts_f_500.jpg',\n",
       " 'F1_1_5_2.ts_f_1000.jpg',\n",
       " 'F1_1_5_2.ts_f_500.jpg',\n",
       " 'F1_2_2_1.ts_f_1000.jpg',\n",
       " 'F1_2_2_1.ts_f_500.jpg',\n",
       " 'F1_2_2_2.ts_f_1000.jpg',\n",
       " 'F1_2_2_2.ts_f_500.jpg',\n",
       " 'F1_2_3_1.ts_f_1000.jpg',\n",
       " 'F1_2_3_1.ts_f_500.jpg',\n",
       " 'F1_2_3_2.ts_f_1000.jpg',\n",
       " 'F1_2_3_2.ts_f_500.jpg',\n",
       " 'F1_2_4_1.ts_f_1000.jpg',\n",
       " 'F1_2_4_1.ts_f_500.jpg',\n",
       " 'F1_2_4_2.ts_f_1000.jpg',\n",
       " 'F1_2_4_2.ts_f_500.jpg',\n",
       " 'F1_2_5_1.ts_f_1000.jpg',\n",
       " 'F1_2_5_1.ts_f_500.jpg',\n",
       " 'F1_2_5_2.ts_f_1000.jpg',\n",
       " 'F1_2_5_2.ts_f_500.jpg',\n",
       " 'F2_1_1_1.ts_f_1000.jpg',\n",
       " 'F2_1_1_1.ts_f_500.jpg',\n",
       " 'F2_1_1_2.ts_f_1000.jpg',\n",
       " 'F2_1_1_2.ts_f_500.jpg',\n",
       " 'F2_1_2_2.ts_f_1000.jpg',\n",
       " 'F2_1_2_2.ts_f_500.jpg',\n",
       " 'F2_2_1_1.ts_f_1000.jpg',\n",
       " 'F2_2_1_1.ts_f_500.jpg',\n",
       " 'F2_2_1_2.ts_f_1000.jpg',\n",
       " 'F2_2_1_2.ts_f_500.jpg',\n",
       " 'F2_2_2_1.ts_f_1000.jpg',\n",
       " 'F2_2_2_1.ts_f_500.jpg',\n",
       " 'F2_2_2_2.ts_f_1000.jpg',\n",
       " 'F2_2_2_2.ts_f_500.jpg',\n",
       " 'F2_2_3_1.ts_f_1000.jpg',\n",
       " 'F2_2_3_1.ts_f_500.jpg',\n",
       " 'F2_2_3_2.ts_f_1000.jpg',\n",
       " 'F2_2_3_2.ts_f_500.jpg',\n",
       " 'F4_1_1_1.ts_f_1000.jpg',\n",
       " 'F4_1_1_1.ts_f_500.jpg',\n",
       " 'F4_1_1_2.ts_f_1000.jpg',\n",
       " 'F4_1_1_2.ts_f_500.jpg',\n",
       " 'F4_1_2_1.ts_f_1000.jpg',\n",
       " 'F4_1_2_1.ts_f_500.jpg',\n",
       " 'F4_1_2_2.ts_f_1000.jpg',\n",
       " 'F4_1_2_2.ts_f_500.jpg',\n",
       " 'F4_1_3_1.ts_f_1000.jpg',\n",
       " 'F4_1_3_1.ts_f_500.jpg',\n",
       " 'F4_1_3_2.ts_f_1000.jpg',\n",
       " 'F4_1_3_2.ts_f_500.jpg',\n",
       " 'F4_2_2_1.ts_f_1000.jpg',\n",
       " 'F4_2_2_1.ts_f_500.jpg',\n",
       " 'F4_2_2_2.ts_f_1000.jpg',\n",
       " 'F4_2_2_2.ts_f_500.jpg',\n",
       " 'F4_2_3_1.ts_f_1000.jpg',\n",
       " 'F4_2_3_1.ts_f_500.jpg',\n",
       " 'F4_2_3_2.ts_f_1000.jpg',\n",
       " 'F4_2_3_2.ts_f_500.jpg',\n",
       " 'F5_1_1_1.ts_f_1000.jpg',\n",
       " 'F5_1_1_1.ts_f_500.jpg',\n",
       " 'F5_1_1_2.ts_f_1000.jpg',\n",
       " 'F5_1_1_2.ts_f_500.jpg',\n",
       " 'F5_1_2_1.ts_f_1000.jpg',\n",
       " 'F5_1_2_1.ts_f_500.jpg',\n",
       " 'F5_1_2_2.ts_f_1000.jpg',\n",
       " 'F5_1_2_2.ts_f_500.jpg',\n",
       " 'F5_1_3_1.ts_f_1000.jpg',\n",
       " 'F5_1_3_1.ts_f_500.jpg',\n",
       " 'F5_1_3_2.ts_f_1000.jpg',\n",
       " 'F5_1_3_2.ts_f_500.jpg',\n",
       " 'F5_2_1_1.ts_f_1000.jpg',\n",
       " 'F5_2_1_1.ts_f_500.jpg',\n",
       " 'F5_2_1_2.ts_f_1000.jpg',\n",
       " 'F5_2_1_2.ts_f_500.jpg',\n",
       " 'F5_2_2_1.ts_f_1000.jpg',\n",
       " 'F5_2_2_1.ts_f_500.jpg',\n",
       " 'F5_2_2_2.ts_f_1000.jpg',\n",
       " 'F5_2_2_2.ts_f_500.jpg',\n",
       " 'F5_2_3_1.ts_f_1000.jpg',\n",
       " 'F5_2_3_1.ts_f_500.jpg',\n",
       " 'F5_2_3_2.ts_f_1000.jpg',\n",
       " 'F5_2_3_2.ts_f_500.jpg',\n",
       " 'F7_1_1_1.ts_f_1000.jpg',\n",
       " 'F7_1_1_1.ts_f_500.jpg',\n",
       " 'F7_1_1_2.ts_f_1000.jpg',\n",
       " 'F7_1_1_2.ts_f_500.jpg',\n",
       " 'F7_1_2_1.ts_f_1000.jpg',\n",
       " 'F7_1_2_1.ts_f_500.jpg',\n",
       " 'F7_1_2_2.ts_f_1000.jpg',\n",
       " 'F7_1_2_2.ts_f_500.jpg',\n",
       " 'F7_2_1_1.ts_f_1000.jpg',\n",
       " 'F7_2_1_1.ts_f_500.jpg',\n",
       " 'F7_2_1_2.ts_f_1000.jpg',\n",
       " 'F7_2_1_2.ts_f_500.jpg',\n",
       " 'gr1.jpg',\n",
       " 'synt100_1710813452_0.png',\n",
       " 'synt101_1710835053_0.png',\n",
       " 'synt103_1711186053_0.png',\n",
       " 'synt105_1712788070_0.png',\n",
       " 'synt106_1715112967_0.png',\n",
       " 'synt107_1717060391_0.png',\n",
       " 'synt10_1709255042_0.png',\n",
       " 'synt11_1709260082_0.png',\n",
       " 'synt12_1709260681_0.png',\n",
       " 'synt13_1709261462_0.png',\n",
       " 'synt14_1709262422_0.png',\n",
       " 'synt15_1709265902_0.png',\n",
       " 'synt16_1709265962_0.png',\n",
       " 'synt17_1709276582_0.png',\n",
       " 'synt18_1709310482_0.png',\n",
       " 'synt1_1709104681_0.png',\n",
       " 'synt20_1709344621_0.png',\n",
       " 'synt21_1709388062_0.png',\n",
       " 'synt22_1709859454_0.png',\n",
       " 'synt23_1709859466_0.png',\n",
       " 'synt256_1709104681_0.png',\n",
       " 'synt257_1709109361_0.png',\n",
       " 'synt258_1709116441_0.png',\n",
       " 'synt259_1709186522_0.png',\n",
       " 'synt25_1709888254_0.png',\n",
       " 'synt260_1709187722_0.png',\n",
       " 'synt261_1709191202_0.png',\n",
       " 'synt262_1709232001_0.png',\n",
       " 'synt263_1709253121_0.png',\n",
       " 'synt264_1709254201_0.png',\n",
       " 'synt265_1709255042_0.png',\n",
       " 'synt266_1709260082_0.png',\n",
       " 'synt267_1709260681_0.png',\n",
       " 'synt268_1709261462_0.png',\n",
       " 'synt269_1709262422_0.png',\n",
       " 'synt26_1710079053_0.png',\n",
       " 'synt270_1709265902_0.png',\n",
       " 'synt271_1709265962_0.png',\n",
       " 'synt272_1709276582_0.png',\n",
       " 'synt273_1709310482_0.png',\n",
       " 'synt274_1709339881_0.png',\n",
       " 'synt275_1709344621_0.png',\n",
       " 'synt276_1709388062_0.png',\n",
       " 'synt277_1709859454_0.png',\n",
       " 'synt278_1709859466_0.png',\n",
       " 'synt27_1710253681_0.png',\n",
       " 'synt280_1709888254_0.png',\n",
       " 'synt282_1710253681_0.png',\n",
       " 'synt283_1710260853_0.png',\n",
       " 'synt284_1710262654_0.png',\n",
       " 'synt285_1710266253_0.png',\n",
       " 'synt287_1710809853_0.png',\n",
       " 'synt288_1710813452_0.png',\n",
       " 'synt28_1710260853_0.png',\n",
       " 'synt291_1711186053_0.png',\n",
       " 'synt293_1712788070_0.png',\n",
       " 'synt294_1715112967_0.png',\n",
       " 'synt295_1717060391_0.png',\n",
       " 'synt296_dirty1.png',\n",
       " 'synt29_1710262654_0.png',\n",
       " 'synt2_1709109361_0.png',\n",
       " 'synt30_1710266253_0.png',\n",
       " 'synt31_1710458871_0.png',\n",
       " 'synt32_1710809853_0.png',\n",
       " 'synt33_1710813452_0.png',\n",
       " 'synt34_1710835053_0.png',\n",
       " 'synt36_1711186053_0.png',\n",
       " 'synt38_1712788070_0.png',\n",
       " 'synt39_1715112967_0.png',\n",
       " 'synt3_1709116441_0.png',\n",
       " 'synt401_1709104681_0.png',\n",
       " 'synt402_1709109361_0.png',\n",
       " 'synt403_1709116441_0.png',\n",
       " 'synt404_1709186522_0.png',\n",
       " 'synt405_1709187722_0.png',\n",
       " 'synt406_1709191202_0.png',\n",
       " 'synt407_1709232001_0.png',\n",
       " 'synt408_1709253121_0.png',\n",
       " 'synt409_1709254201_0.png',\n",
       " 'synt40_1717060391_0.png',\n",
       " 'synt410_1709255042_0.png',\n",
       " 'synt411_1709260082_0.png',\n",
       " 'synt412_1709260681_0.png',\n",
       " 'synt413_1709261462_0.png',\n",
       " 'synt414_1709262422_0.png',\n",
       " 'synt415_1709265902_0.png',\n",
       " 'synt416_1709265962_0.png',\n",
       " 'synt417_1709276582_0.png',\n",
       " 'synt418_1709310482_0.png',\n",
       " 'synt419_1709339881_0.png',\n",
       " 'synt41_dirty1.png',\n",
       " 'synt420_1709344621_0.png',\n",
       " 'synt421_1709388062_0.png',\n",
       " 'synt422_1709859454_0.png',\n",
       " 'synt423_1709859466_0.png',\n",
       " 'synt425_1709888254_0.png',\n",
       " 'synt426_1710079053_0.png',\n",
       " 'synt427_1710253681_0.png',\n",
       " 'synt428_1710260853_0.png',\n",
       " 'synt429_1710262654_0.png',\n",
       " 'synt430_1710266253_0.png',\n",
       " 'synt431_1710458871_0.png',\n",
       " 'synt432_1710809853_0.png',\n",
       " 'synt433_1710813452_0.png',\n",
       " 'synt434_1710835053_0.png',\n",
       " 'synt436_1711186053_0.png',\n",
       " 'synt438_1712788070_0.png',\n",
       " 'synt439_1715112967_0.png',\n",
       " 'synt440_1717060391_0.png',\n",
       " 'synt441_dirty1.png',\n",
       " 'synt4_1709186522_0.png',\n",
       " 'synt5_1709187722_0.png',\n",
       " 'synt68_1709104681_0.png',\n",
       " 'synt69_1709109361_0.png',\n",
       " 'synt6_1709191202_0.png',\n",
       " 'synt70_1709116441_0.png',\n",
       " 'synt71_1709186522_0.png',\n",
       " 'synt72_1709187722_0.png',\n",
       " 'synt73_1709191202_0.png',\n",
       " 'synt74_1709232001_0.png',\n",
       " 'synt75_1709253121_0.png',\n",
       " 'synt76_1709254201_0.png',\n",
       " 'synt77_1709255042_0.png',\n",
       " 'synt78_1709260082_0.png',\n",
       " 'synt79_1709260681_0.png',\n",
       " 'synt7_1709232001_0.png',\n",
       " 'synt80_1709261462_0.png',\n",
       " 'synt81_1709262422_0.png',\n",
       " 'synt82_1709265902_0.png',\n",
       " 'synt83_1709265962_0.png',\n",
       " 'synt84_1709276582_0.png',\n",
       " 'synt85_1709310482_0.png',\n",
       " 'synt86_1709339881_0.png',\n",
       " 'synt87_1709344621_0.png',\n",
       " 'synt88_1709388062_0.png',\n",
       " 'synt89_1709859454_0.png',\n",
       " 'synt90_1709859466_0.png',\n",
       " 'synt91_1709886453_0.png',\n",
       " 'synt92_1709888254_0.png',\n",
       " 'synt93_1710079053_0.png',\n",
       " 'synt94_1710253681_0.png',\n",
       " 'synt95_1710260853_0.png',\n",
       " 'synt96_1710262654_0.png',\n",
       " 'synt97_1710266253_0.png',\n",
       " 'synt98_1710458871_0.png',\n",
       " 'synt99_1710809853_0.png',\n",
       " 'synt9_1709254201_0.png']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1e867c40-f98c-4e4a-bfee-50e643b15433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1710275253_0.png',\n",
       " '1710277054_0.png',\n",
       " '1710322071_0.png',\n",
       " '1710370672_0.png',\n",
       " '1710381477_0.png',\n",
       " '1710390473_0.png',\n",
       " '1710399478_0.png',\n",
       " '1710430072_0.png',\n",
       " '1710458871_0.png',\n",
       " '1710584874_0.png',\n",
       " '1710604665_0.png',\n",
       " '1710809853_0.png',\n",
       " '1710811652_0.png',\n",
       " '1710813452_0.png',\n",
       " '1710815253_0.png',\n",
       " '1710835053_0.png',\n",
       " '1710847680_0.png',\n",
       " '1710917853_0.png',\n",
       " '1711009681_0.png',\n",
       " '1711090673_0.png',\n",
       " '1711119479_0.png',\n",
       " '1711177052_0.png',\n",
       " '1711178852_0.png',\n",
       " '1711178872_0.png',\n",
       " '1711180652_0.png',\n",
       " '1711182453_0.png',\n",
       " '1711184253_0.png',\n",
       " '1711186053_0.png',\n",
       " '1711187868_0.png',\n",
       " '1711256271_0.png',\n",
       " '1711268872_0.png',\n",
       " '1711268876_0.png',\n",
       " '1711330053_0.png',\n",
       " '1711393084_0.png',\n",
       " '1711412853_0.png',\n",
       " '1711421860_0.png',\n",
       " '1711430853_0.png',\n",
       " '1711448853_0.png',\n",
       " '1711450664_0.png',\n",
       " '1711456067_0.png',\n",
       " '1711459680_0.png',\n",
       " '1711510053_0.png',\n",
       " '1712555862_0.png',\n",
       " '1712564893_0.png',\n",
       " '1712577488_0.png',\n",
       " '1712741269_0.png',\n",
       " '1712788070_0.png',\n",
       " '1713302111_0.png',\n",
       " '1713350757_0.png',\n",
       " '1713767248_0.png',\n",
       " '1715041648_0.png',\n",
       " '1715074048_0.png',\n",
       " '1715112967_0.png',\n",
       " '1715158095_0.png',\n",
       " '1716017248_0.png',\n",
       " '1717022950_0.png',\n",
       " '1717058869_0.png',\n",
       " '1717059450_0.png',\n",
       " '1717060391_0.png',\n",
       " '1717061248_0.png',\n",
       " '1717063048_0.png',\n",
       " '1717068448_0.png',\n",
       " '1717390039_0.png',\n",
       " 'F1_1_1_1.ts_f_1000.png',\n",
       " 'F1_1_1_1.ts_f_500.png',\n",
       " 'F1_1_1_2.ts_f_1000.png',\n",
       " 'F1_1_1_2.ts_f_500.png',\n",
       " 'F1_1_2_1.ts_f_1000.png',\n",
       " 'F1_1_2_1.ts_f_500.png',\n",
       " 'F1_1_2_2.ts_f_1000.png',\n",
       " 'F1_1_2_2.ts_f_500.png',\n",
       " 'F1_1_3_1.ts_f_1000.png',\n",
       " 'F1_1_3_1.ts_f_500.png',\n",
       " 'F1_1_4_2.ts_f_1000.png',\n",
       " 'F1_1_4_2.ts_f_500.png',\n",
       " 'F1_1_5_1.ts_f_1000.png',\n",
       " 'F1_1_5_1.ts_f_500.png',\n",
       " 'F1_1_5_2.ts_f_1000.png',\n",
       " 'F1_1_5_2.ts_f_500.png',\n",
       " 'F1_2_2_1.ts_f_1000.png',\n",
       " 'F1_2_2_1.ts_f_500.png',\n",
       " 'F1_2_2_2.ts_f_1000.png',\n",
       " 'F1_2_2_2.ts_f_500.png',\n",
       " 'F1_2_3_1.ts_f_1000.png',\n",
       " 'F1_2_3_1.ts_f_500.png',\n",
       " 'F1_2_3_2.ts_f_1000.png',\n",
       " 'F1_2_3_2.ts_f_500.png',\n",
       " 'F1_2_4_1.ts_f_1000.png',\n",
       " 'F1_2_4_1.ts_f_500.png',\n",
       " 'F1_2_4_2.ts_f_1000.png',\n",
       " 'F1_2_4_2.ts_f_500.png',\n",
       " 'F1_2_5_1.ts_f_1000.png',\n",
       " 'F1_2_5_1.ts_f_500.png',\n",
       " 'F1_2_5_2.ts_f_1000.png',\n",
       " 'F1_2_5_2.ts_f_500.png',\n",
       " 'F2_1_1_1.ts_f_1000.png',\n",
       " 'F2_1_1_1.ts_f_500.png',\n",
       " 'F2_1_1_2.ts_f_1000.png',\n",
       " 'F2_1_1_2.ts_f_500.png',\n",
       " 'F2_1_2_2.ts_f_1000.png',\n",
       " 'F2_1_2_2.ts_f_500.png',\n",
       " 'F2_2_1_1.ts_f_1000.png',\n",
       " 'F2_2_1_1.ts_f_500.png',\n",
       " 'F2_2_1_2.ts_f_1000.png',\n",
       " 'F2_2_1_2.ts_f_500.png',\n",
       " 'F2_2_2_1.ts_f_1000.png',\n",
       " 'F2_2_2_1.ts_f_500.png',\n",
       " 'F2_2_2_2.ts_f_1000.png',\n",
       " 'F2_2_2_2.ts_f_500.png',\n",
       " 'F2_2_3_1.ts_f_1000.png',\n",
       " 'F2_2_3_1.ts_f_500.png',\n",
       " 'F2_2_3_2.ts_f_1000.png',\n",
       " 'F2_2_3_2.ts_f_500.png',\n",
       " 'F4_1_1_1.ts_f_1000.png',\n",
       " 'F4_1_1_1.ts_f_500.png',\n",
       " 'F4_1_1_2.ts_f_1000.png',\n",
       " 'F4_1_1_2.ts_f_500.png',\n",
       " 'F4_1_2_1.ts_f_1000.png',\n",
       " 'F4_1_2_1.ts_f_500.png',\n",
       " 'F4_1_2_2.ts_f_1000.png',\n",
       " 'F4_1_2_2.ts_f_500.png',\n",
       " 'F4_1_3_1.ts_f_1000.png',\n",
       " 'F4_1_3_1.ts_f_500.png',\n",
       " 'F4_1_3_2.ts_f_1000.png',\n",
       " 'F4_1_3_2.ts_f_500.png',\n",
       " 'F4_2_2_1.ts_f_1000.png',\n",
       " 'F4_2_2_1.ts_f_500.png',\n",
       " 'F4_2_2_2.ts_f_1000.png',\n",
       " 'F4_2_2_2.ts_f_500.png',\n",
       " 'F4_2_3_1.ts_f_1000.png',\n",
       " 'F4_2_3_1.ts_f_500.png',\n",
       " 'F4_2_3_2.ts_f_1000.png',\n",
       " 'F4_2_3_2.ts_f_500.png',\n",
       " 'F5_1_1_1.ts_f_1000.png',\n",
       " 'F5_1_1_1.ts_f_500.png',\n",
       " 'F5_1_1_2.ts_f_1000.png',\n",
       " 'F5_1_1_2.ts_f_500.png',\n",
       " 'F5_1_2_1.ts_f_1000.png',\n",
       " 'F5_1_2_1.ts_f_500.png',\n",
       " 'F5_1_2_2.ts_f_1000.png',\n",
       " 'F5_1_2_2.ts_f_500.png',\n",
       " 'F5_1_3_1.ts_f_1000.png',\n",
       " 'F5_1_3_1.ts_f_500.png',\n",
       " 'F5_1_3_2.ts_f_1000.png',\n",
       " 'F5_1_3_2.ts_f_500.png',\n",
       " 'F5_2_1_1.ts_f_1000.png',\n",
       " 'F5_2_1_1.ts_f_500.png',\n",
       " 'F5_2_1_2.ts_f_1000.png',\n",
       " 'F5_2_1_2.ts_f_500.png',\n",
       " 'F5_2_2_1.ts_f_1000.png',\n",
       " 'F5_2_2_1.ts_f_500.png',\n",
       " 'F5_2_2_2.ts_f_1000.png',\n",
       " 'F5_2_2_2.ts_f_500.png',\n",
       " 'F5_2_3_1.ts_f_1000.png',\n",
       " 'F5_2_3_1.ts_f_500.png',\n",
       " 'F5_2_3_2.ts_f_1000.png',\n",
       " 'F5_2_3_2.ts_f_500.png',\n",
       " 'F7_1_1_1.ts_f_1000.png',\n",
       " 'F7_1_1_1.ts_f_500.png',\n",
       " 'F7_1_1_2.ts_f_1000.png',\n",
       " 'F7_1_1_2.ts_f_500.png',\n",
       " 'F7_1_2_1.ts_f_1000.png',\n",
       " 'F7_1_2_1.ts_f_500.png',\n",
       " 'F7_1_2_2.ts_f_1000.png',\n",
       " 'F7_1_2_2.ts_f_500.png',\n",
       " 'F7_2_1_1.ts_f_1000.png',\n",
       " 'F7_2_1_1.ts_f_500.png',\n",
       " 'F7_2_1_2.ts_f_1000.png',\n",
       " 'F7_2_1_2.ts_f_500.png',\n",
       " 'gr1.png',\n",
       " '100_1710813452_0.png',\n",
       " '101_1710835053_0.png',\n",
       " '103_1711186053_0.png',\n",
       " '105_1712788070_0.png',\n",
       " '106_1715112967_0.png',\n",
       " '107_1717060391_0.png',\n",
       " '10_1709255042_0.png',\n",
       " '11_1709260082_0.png',\n",
       " '12_1709260681_0.png',\n",
       " '13_1709261462_0.png',\n",
       " '14_1709262422_0.png',\n",
       " '15_1709265902_0.png',\n",
       " '16_1709265962_0.png',\n",
       " '17_1709276582_0.png',\n",
       " '18_1709310482_0.png',\n",
       " '1_1709104681_0.png',\n",
       " '20_1709344621_0.png',\n",
       " '21_1709388062_0.png',\n",
       " '22_1709859454_0.png',\n",
       " '23_1709859466_0.png',\n",
       " '256_1709104681_0.png',\n",
       " '257_1709109361_0.png',\n",
       " '258_1709116441_0.png',\n",
       " '259_1709186522_0.png',\n",
       " '25_1709888254_0.png',\n",
       " '260_1709187722_0.png',\n",
       " '261_1709191202_0.png',\n",
       " '262_1709232001_0.png',\n",
       " '263_1709253121_0.png',\n",
       " '264_1709254201_0.png',\n",
       " '265_1709255042_0.png',\n",
       " '266_1709260082_0.png',\n",
       " '267_1709260681_0.png',\n",
       " '268_1709261462_0.png',\n",
       " '269_1709262422_0.png',\n",
       " '26_1710079053_0.png',\n",
       " '270_1709265902_0.png',\n",
       " '271_1709265962_0.png',\n",
       " '272_1709276582_0.png',\n",
       " '273_1709310482_0.png',\n",
       " '274_1709339881_0.png',\n",
       " '275_1709344621_0.png',\n",
       " '276_1709388062_0.png',\n",
       " '277_1709859454_0.png',\n",
       " '278_1709859466_0.png',\n",
       " '27_1710253681_0.png',\n",
       " '280_1709888254_0.png',\n",
       " '282_1710253681_0.png',\n",
       " '283_1710260853_0.png',\n",
       " '284_1710262654_0.png',\n",
       " '285_1710266253_0.png',\n",
       " '287_1710809853_0.png',\n",
       " '288_1710813452_0.png',\n",
       " '28_1710260853_0.png',\n",
       " '291_1711186053_0.png',\n",
       " '293_1712788070_0.png',\n",
       " '294_1715112967_0.png',\n",
       " '295_1717060391_0.png',\n",
       " '296_dirty1.png',\n",
       " '29_1710262654_0.png',\n",
       " '2_1709109361_0.png',\n",
       " '30_1710266253_0.png',\n",
       " '31_1710458871_0.png',\n",
       " '32_1710809853_0.png',\n",
       " '33_1710813452_0.png',\n",
       " '34_1710835053_0.png',\n",
       " '36_1711186053_0.png',\n",
       " '38_1712788070_0.png',\n",
       " '39_1715112967_0.png',\n",
       " '3_1709116441_0.png',\n",
       " '401_1709104681_0.png',\n",
       " '402_1709109361_0.png',\n",
       " '403_1709116441_0.png',\n",
       " '404_1709186522_0.png',\n",
       " '405_1709187722_0.png',\n",
       " '406_1709191202_0.png',\n",
       " '407_1709232001_0.png',\n",
       " '408_1709253121_0.png',\n",
       " '409_1709254201_0.png',\n",
       " '40_1717060391_0.png',\n",
       " '410_1709255042_0.png',\n",
       " '411_1709260082_0.png',\n",
       " '412_1709260681_0.png',\n",
       " '413_1709261462_0.png',\n",
       " '414_1709262422_0.png',\n",
       " '415_1709265902_0.png',\n",
       " '416_1709265962_0.png',\n",
       " '417_1709276582_0.png',\n",
       " '418_1709310482_0.png',\n",
       " '419_1709339881_0.png',\n",
       " '41_dirty1.png',\n",
       " '420_1709344621_0.png',\n",
       " '421_1709388062_0.png',\n",
       " '422_1709859454_0.png',\n",
       " '423_1709859466_0.png',\n",
       " '425_1709888254_0.png',\n",
       " '426_1710079053_0.png',\n",
       " '427_1710253681_0.png',\n",
       " '428_1710260853_0.png',\n",
       " '429_1710262654_0.png',\n",
       " '430_1710266253_0.png',\n",
       " '431_1710458871_0.png',\n",
       " '432_1710809853_0.png',\n",
       " '433_1710813452_0.png',\n",
       " '434_1710835053_0.png',\n",
       " '436_1711186053_0.png',\n",
       " '438_1712788070_0.png',\n",
       " '439_1715112967_0.png',\n",
       " '440_1717060391_0.png',\n",
       " '441_dirty1.png',\n",
       " '4_1709186522_0.png',\n",
       " '5_1709187722_0.png',\n",
       " '68_1709104681_0.png',\n",
       " '69_1709109361_0.png',\n",
       " '6_1709191202_0.png',\n",
       " '70_1709116441_0.png',\n",
       " '71_1709186522_0.png',\n",
       " '72_1709187722_0.png',\n",
       " '73_1709191202_0.png',\n",
       " '74_1709232001_0.png',\n",
       " '75_1709253121_0.png',\n",
       " '76_1709254201_0.png',\n",
       " '77_1709255042_0.png',\n",
       " '78_1709260082_0.png',\n",
       " '79_1709260681_0.png',\n",
       " '7_1709232001_0.png',\n",
       " '80_1709261462_0.png',\n",
       " '81_1709262422_0.png',\n",
       " '82_1709265902_0.png',\n",
       " '83_1709265962_0.png',\n",
       " '84_1709276582_0.png',\n",
       " '85_1709310482_0.png',\n",
       " '86_1709339881_0.png',\n",
       " '87_1709344621_0.png',\n",
       " '88_1709388062_0.png',\n",
       " '89_1709859454_0.png',\n",
       " '90_1709859466_0.png',\n",
       " '91_1709886453_0.png',\n",
       " '92_1709888254_0.png',\n",
       " '93_1710079053_0.png',\n",
       " '94_1710253681_0.png',\n",
       " '95_1710260853_0.png',\n",
       " '96_1710262654_0.png',\n",
       " '97_1710266253_0.png',\n",
       " '98_1710458871_0.png',\n",
       " '99_1710809853_0.png',\n",
       " '9_1709254201_0.png']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fd72f0f-c6d4-4fc6-b015-07601a47a9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len((mask_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d00ab007653eda31",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_images, val_images = train_test_split(image_files, train_size=TRAIN_SIZE, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4cd15221eb3f864",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def convert_mask_to_yolo(mask_path, out):\n",
    "    # Открываем изображение\n",
    "    image = cv2.imread(mask_path)\n",
    "\n",
    "    if image is None:\n",
    "        print(f\"Не удалось открыть изображение: {mask_path}\")\n",
    "        return\n",
    "    \n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Создаем маску для черного цвета\n",
    "    black_mask = cv2.inRange(image, (0, 0, 0), (50, 50, 50))\n",
    "\n",
    "    # Создаем новое изображение, где черный цвет остается, а остальные цвета становятся белыми\n",
    "    new_image = np.ones_like(image) * 255  # Начинаем с белого изображения\n",
    "    new_image[black_mask > 0] = [0, 0, 0]  # Заменяем черные пиксели\n",
    "\n",
    "    # Преобразуем в градации серого для нахождения контуров\n",
    "    gray_image = cv2.cvtColor(new_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Находим контуры\n",
    "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Сохраняем контуры в текстовом формате\n",
    "    output_file_name = os.path.splitext(os.path.basename(mask_path))[0] + '.txt'\n",
    "    output_file_path = os.path.join(OUTPUT_DIR, out, output_file_name)\n",
    "\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        for index, contour in enumerate(contours):\n",
    "            # Получаем координаты всех точек контура\n",
    "            contour_points = contour.reshape(-1, 2)\n",
    "            # Нормализуем координаты\n",
    "            normalized_points = [(x / width, y / height) for x, y in contour_points]\n",
    "            points_str = ' '.join(f\"{x:.3f} {y:.3f}\" for x, y in normalized_points)\n",
    "            f.write(f\"0 {points_str}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b4e5c-e87c-4798-80b6-ab9cc4bcf6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7922a992-3e79-43dd-af5c-40c236e8f5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['synt16_1709265962_0.png',\n",
       " '1711448853_0.jpg',\n",
       " 'F2_1_1_1.ts_f_500.jpg',\n",
       " '1710147478_0.jpg',\n",
       " 'F7_2_1_1.ts_f_1000.jpg',\n",
       " '1710917853_0.jpg',\n",
       " 'synt97_1710266253_0.png',\n",
       " '1713350757_0.jpg',\n",
       " 'synt73_1709191202_0.png',\n",
       " 'F4_1_1_2.ts_f_1000.jpg',\n",
       " '1711459680_0.jpg',\n",
       " 'synt262_1709232001_0.png',\n",
       " 'synt258_1709116441_0.png',\n",
       " 'synt425_1709888254_0.png',\n",
       " 'synt75_1709253121_0.png',\n",
       " 'synt107_1717060391_0.png',\n",
       " 'synt269_1709262422_0.png',\n",
       " '1710273470_0.jpg',\n",
       " 'synt413_1709261462_0.png',\n",
       " '1711330053_0.jpg',\n",
       " '1709255042_0.jpg',\n",
       " '1710262654_0.jpg',\n",
       " 'synt83_1709265962_0.png',\n",
       " '1709116441_0.jpg',\n",
       " '1709260681_0.jpg',\n",
       " 'F5_1_2_2.ts_f_500.jpg',\n",
       " 'synt432_1710809853_0.png',\n",
       " 'synt426_1710079053_0.png',\n",
       " '1711412853_0.jpg',\n",
       " 'F2_2_1_1.ts_f_1000.jpg',\n",
       " 'synt274_1709339881_0.png',\n",
       " 'synt87_1709344621_0.png',\n",
       " 'synt438_1712788070_0.png',\n",
       " '1709888254_0.jpg',\n",
       " '1710370672_0.jpg',\n",
       " 'synt268_1709261462_0.png',\n",
       " 'synt7_1709232001_0.png',\n",
       " 'synt105_1712788070_0.png',\n",
       " 'synt411_1709260082_0.png',\n",
       " '1711393084_0.jpg',\n",
       " '1709255402_0.jpg',\n",
       " 'F1_2_4_1.ts_f_500.jpg',\n",
       " '1709265962_0.jpg',\n",
       " '1715041648_0.jpg',\n",
       " 'synt99_1710809853_0.png',\n",
       " 'synt284_1710262654_0.png',\n",
       " '1717068448_0.jpg',\n",
       " 'synt30_1710266253_0.png',\n",
       " 'synt33_1710813452_0.png',\n",
       " 'F1_1_5_1.ts_f_1000.jpg',\n",
       " 'F4_2_3_1.ts_f_1000.jpg',\n",
       " 'F7_1_1_2.ts_f_1000.jpg',\n",
       " 'F2_1_1_1.ts_f_1000.jpg',\n",
       " '1712564893_0.jpg',\n",
       " 'synt79_1709260681_0.png',\n",
       " 'synt407_1709232001_0.png',\n",
       " '29_1.png',\n",
       " 'F1_1_3_1.ts_f_500.jpg',\n",
       " 'synt293_1712788070_0.png',\n",
       " '1709550918_0.jpg',\n",
       " '17_1.png',\n",
       " 'synt273_1709310482_0.png',\n",
       " 'synt20_1709344621_0.png',\n",
       " 'F4_1_1_1.ts_f_1000.jpg',\n",
       " 'F4_1_1_1.ts_f_500.jpg',\n",
       " '1709886453_0.jpg',\n",
       " '1711421860_0.jpg',\n",
       " 'F1_2_3_2.ts_f_1000.jpg',\n",
       " 'F1_2_5_1.ts_f_500.jpg',\n",
       " '1709109361_0.jpg',\n",
       " 'synt270_1709265902_0.png',\n",
       " 'synt429_1710262654_0.png',\n",
       " 'F1_1_2_2.ts_f_500.jpg',\n",
       " '1709186522_0.jpg',\n",
       " 'F5_2_2_1.ts_f_1000.jpg',\n",
       " '1711186053_0.jpg',\n",
       " '1710390473_0.jpg',\n",
       " 'F4_1_3_2.ts_f_1000.jpg',\n",
       " 'F7_1_1_1.ts_f_500.jpg',\n",
       " 'synt418_1709310482_0.png',\n",
       " 'F4_2_3_1.ts_f_500.jpg',\n",
       " 'synt430_1710266253_0.png',\n",
       " 'F5_1_2_1.ts_f_500.jpg',\n",
       " '1710813452_0.jpg',\n",
       " 'synt95_1710260853_0.png',\n",
       " 'F1_2_3_1.ts_f_1000.jpg',\n",
       " 'F1_1_2_1.ts_f_1000.jpg',\n",
       " 'F5_2_1_1.ts_f_1000.jpg',\n",
       " '1710847680_0.jpg',\n",
       " '1710264471_0.jpg',\n",
       " '1713302111_0.jpg',\n",
       " '1711119479_0.jpg',\n",
       " 'F1_1_1_1.ts_f_1000.jpg',\n",
       " 'synt287_1710809853_0.png',\n",
       " '1711177052_0.jpg',\n",
       " 'F5_2_1_2.ts_f_1000.jpg',\n",
       " '1710264453_0.jpg',\n",
       " '1709265902_0.jpg',\n",
       " '1709587678_0.jpg',\n",
       " 'F1_1_1_1.ts_f_500.jpg',\n",
       " '1712788070_0.jpg',\n",
       " '1710262674_0.jpg',\n",
       " 'F5_1_3_2.ts_f_1000.jpg',\n",
       " 'F4_2_3_2.ts_f_1000.jpg',\n",
       " 'synt72_1709187722_0.png',\n",
       " 'synt427_1710253681_0.png',\n",
       " 'F2_2_3_2.ts_f_500.jpg',\n",
       " '1713767248_0.jpg',\n",
       " 'F1_1_1_2.ts_f_500.jpg',\n",
       " 'F5_2_2_2.ts_f_500.jpg',\n",
       " '1710604665_0.jpg',\n",
       " 'synt414_1709262422_0.png',\n",
       " 'F4_1_3_1.ts_f_500.jpg',\n",
       " 'synt415_1709265902_0.png',\n",
       " 'F4_1_2_1.ts_f_500.jpg',\n",
       " 'synt81_1709262422_0.png',\n",
       " 'synt295_1717060391_0.png',\n",
       " 'F2_2_1_2.ts_f_1000.jpg',\n",
       " 'synt276_1709388062_0.png',\n",
       " 'synt266_1709260082_0.png',\n",
       " 'F4_2_2_2.ts_f_500.jpg',\n",
       " 'synt265_1709255042_0.png',\n",
       " '1715158095_0.jpg',\n",
       " '1709807287_0.jpg',\n",
       " '1709187722_0.jpg',\n",
       " 'synt22_1709859454_0.png',\n",
       " 'F2_2_2_1.ts_f_1000.jpg',\n",
       " 'F5_1_2_2.ts_f_1000.jpg',\n",
       " '1711430853_0.jpg',\n",
       " 'F2_1_2_2.ts_f_1000.jpg',\n",
       " 'synt275_1709344621_0.png',\n",
       " 'synt12_1709260681_0.png',\n",
       " '1711510053_0.jpg',\n",
       " 'synt70_1709116441_0.png',\n",
       " 'synt260_1709187722_0.png',\n",
       " 'synt85_1709310482_0.png',\n",
       " 'synt26_1710079053_0.png',\n",
       " 'synt86_1709339881_0.png',\n",
       " 'F1_2_3_1.ts_f_500.jpg',\n",
       " 'synt100_1710813452_0.png',\n",
       " '1717063048_0.jpg',\n",
       " 'F1_1_4_2.ts_f_500.jpg',\n",
       " 'F4_1_2_2.ts_f_1000.jpg',\n",
       " 'F4_1_2_1.ts_f_1000.jpg',\n",
       " 'synt93_1710079053_0.png',\n",
       " '1709080522_0.jpg',\n",
       " 'F4_1_3_2.ts_f_500.jpg',\n",
       " '1711456067_0.jpg',\n",
       " 'F5_2_2_2.ts_f_1000.jpg',\n",
       " 'F5_2_3_1.ts_f_500.jpg',\n",
       " '1709271842_0.jpg',\n",
       " '1712577488_0.jpg',\n",
       " 'synt291_1711186053_0.png',\n",
       " 'F5_1_3_2.ts_f_500.jpg',\n",
       " 'synt406_1709191202_0.png',\n",
       " 'synt1_1709104681_0.png',\n",
       " 'synt76_1709254201_0.png',\n",
       " 'synt420_1709344621_0.png',\n",
       " '1715074048_0.jpg',\n",
       " '1710273453_0.jpg',\n",
       " 'synt401_1709104681_0.png',\n",
       " '1709278661_0.jpg',\n",
       " '1710430072_0.jpg',\n",
       " 'synt431_1710458871_0.png',\n",
       " '1711268876_0.jpg',\n",
       " 'F7_2_1_1.ts_f_500.jpg',\n",
       " 'synt78_1709260082_0.png',\n",
       " '1717059450_0.jpg',\n",
       " 'F7_1_2_2.ts_f_500.jpg',\n",
       " 'F2_2_1_2.ts_f_500.jpg',\n",
       " 'synt283_1710260853_0.png',\n",
       " '1709863065_0.jpg',\n",
       " 'synt6_1709191202_0.png',\n",
       " '1710260853_0.jpg',\n",
       " 'synt278_1709859466_0.png',\n",
       " '1710584874_0.jpg',\n",
       " 'F7_1_2_1.ts_f_500.jpg',\n",
       " 'F2_2_2_1.ts_f_500.jpg',\n",
       " 'F1_2_2_1.ts_f_1000.jpg',\n",
       " '1709191202_0.jpg',\n",
       " '1709520200_0.jpg',\n",
       " '1709278599_0.jpg',\n",
       " 'F1_2_4_2.ts_f_500.jpg',\n",
       " '29_2.png',\n",
       " 'synt36_1711186053_0.png',\n",
       " 'synt404_1709186522_0.png',\n",
       " 'F5_1_3_1.ts_f_1000.jpg',\n",
       " '1711090673_0.jpg',\n",
       " '1709872064_0.jpg',\n",
       " 'F7_1_1_2.ts_f_500.jpg',\n",
       " 'synt280_1709888254_0.png',\n",
       " 'synt288_1710813452_0.png',\n",
       " 'F2_1_1_2.ts_f_500.jpg',\n",
       " '1709857666_0.jpg',\n",
       " 'synt82_1709265902_0.png',\n",
       " 'F4_2_2_1.ts_f_500.jpg',\n",
       " 'synt106_1715112967_0.png',\n",
       " 'synt40_1717060391_0.png',\n",
       " '1709103022_0.jpg',\n",
       " 'synt10_1709255042_0.png',\n",
       " 'synt436_1711186053_0.png',\n",
       " '1711180652_0.jpg',\n",
       " 'F5_2_1_1.ts_f_500.jpg',\n",
       " 'synt421_1709388062_0.png',\n",
       " 'F5_1_1_1.ts_f_1000.jpg',\n",
       " 'F1_2_5_1.ts_f_1000.jpg',\n",
       " '1711178852_0.jpg',\n",
       " 'F5_1_1_2.ts_f_1000.jpg',\n",
       " '1709893685_0.jpg',\n",
       " '1709866664_0.jpg',\n",
       " '1709339881_0.jpg',\n",
       " 'synt21_1709388062_0.png',\n",
       " 'F4_1_2_2.ts_f_500.jpg',\n",
       " 'synt433_1710813452_0.png',\n",
       " '1709276582_0.jpg',\n",
       " 'synt2_1709109361_0.png',\n",
       " 'F5_2_2_1.ts_f_500.jpg',\n",
       " 'synt14_1709262422_0.png',\n",
       " 'synt272_1709276582_0.png',\n",
       " 'F1_2_2_2.ts_f_500.jpg',\n",
       " 'synt417_1709276582_0.png',\n",
       " '1717390039_0.jpg',\n",
       " '1710079053_0.jpg',\n",
       " '1717061248_0.jpg',\n",
       " '1715112967_0.jpg',\n",
       " '1709117582_0.jpg',\n",
       " '1710253681_0.jpg',\n",
       " 'synt285_1710266253_0.png',\n",
       " '1709253121_0.jpg',\n",
       " 'F1_1_5_1.ts_f_500.jpg',\n",
       " '1709855872_0.jpg',\n",
       " 'synt69_1709109361_0.png',\n",
       " 'synt402_1709109361_0.png',\n",
       " 'F5_1_1_2.ts_f_500.jpg',\n",
       " 'synt264_1709254201_0.png',\n",
       " 'synt80_1709261462_0.png',\n",
       " 'F5_2_3_2.ts_f_500.jpg',\n",
       " 'F4_2_2_2.ts_f_1000.jpg',\n",
       " 'F4_2_3_2.ts_f_500.jpg',\n",
       " 'synt27_1710253681_0.png',\n",
       " 'synt5_1709187722_0.png',\n",
       " 'synt101_1710835053_0.png',\n",
       " 'synt91_1709886453_0.png',\n",
       " 'synt68_1709104681_0.png',\n",
       " 'synt28_1710260853_0.png',\n",
       " 'synt23_1709859466_0.png',\n",
       " 'F4_1_3_1.ts_f_1000.jpg',\n",
       " 'F1_2_2_2.ts_f_1000.jpg',\n",
       " '1709859466_0.jpg',\n",
       " 'F5_1_2_1.ts_f_1000.jpg',\n",
       " 'F2_2_3_1.ts_f_500.jpg',\n",
       " 'synt29_1710262654_0.png',\n",
       " 'synt13_1709261462_0.png',\n",
       " '1711256271_0.jpg',\n",
       " '1709875666_0.jpg',\n",
       " '1709046334_0.jpg',\n",
       " '1709870253_0.jpg',\n",
       " '1710381477_0.jpg',\n",
       " 'F4_2_2_1.ts_f_1000.jpg',\n",
       " '1709388062_0.jpg',\n",
       " 'synt17_1709276582_0.png',\n",
       " '1710835053_0.jpg',\n",
       " 'synt419_1709339881_0.png',\n",
       " '1709875661_0.jpg',\n",
       " 'synt18_1709310482_0.png',\n",
       " 'F7_1_2_1.ts_f_1000.jpg',\n",
       " '1709232001_0.jpg',\n",
       " 'synt34_1710835053_0.png',\n",
       " '1710811652_0.jpg',\n",
       " 'synt259_1709186522_0.png',\n",
       " 'F1_2_3_2.ts_f_500.jpg',\n",
       " 'synt409_1709254201_0.png',\n",
       " 'synt94_1710253681_0.png',\n",
       " '1717060391_0.jpg',\n",
       " 'synt294_1715112967_0.png',\n",
       " 'synt84_1709276582_0.png',\n",
       " 'synt3_1709116441_0.png',\n",
       " 'F7_1_2_2.ts_f_1000.jpg',\n",
       " '1709877464_0.jpg',\n",
       " '1709870265_0.jpg',\n",
       " 'F1_2_5_2.ts_f_500.jpg',\n",
       " 'F2_2_3_1.ts_f_1000.jpg',\n",
       " 'synt98_1710458871_0.png',\n",
       " 'F2_2_2_2.ts_f_1000.jpg',\n",
       " 'F1_2_4_2.ts_f_1000.jpg',\n",
       " '1709884664_0.jpg',\n",
       " '1709868465_0.jpg',\n",
       " 'synt423_1709859466_0.png',\n",
       " 'F5_2_3_2.ts_f_1000.jpg',\n",
       " 'synt103_1711186053_0.png',\n",
       " '1709262422_0.jpg',\n",
       " 'synt32_1710809853_0.png',\n",
       " 'F1_2_2_1.ts_f_500.jpg',\n",
       " 'synt261_1709191202_0.png',\n",
       " 'F2_2_3_2.ts_f_1000.jpg',\n",
       " 'synt277_1709859454_0.png',\n",
       " 'synt422_1709859454_0.png',\n",
       " 'synt11_1709260082_0.png',\n",
       " 'synt296_dirty1.png',\n",
       " 'F1_1_2_2.ts_f_1000.jpg',\n",
       " '1716017248_0.jpg',\n",
       " 'F1_1_3_1.ts_f_1000.jpg',\n",
       " 'synt441_dirty1.png',\n",
       " '1711178872_0.jpg',\n",
       " 'synt77_1709255042_0.png',\n",
       " '1710809853_0.jpg',\n",
       " 'synt410_1709255042_0.png',\n",
       " 'F5_1_1_1.ts_f_500.jpg',\n",
       " '1712741269_0.jpg',\n",
       " 'synt9_1709254201_0.png',\n",
       " '1709261462_0.jpg',\n",
       " 'F2_2_2_2.ts_f_500.jpg',\n",
       " '1710268053_0.jpg',\n",
       " '1711268872_0.jpg',\n",
       " 'synt256_1709104681_0.png',\n",
       " 'synt428_1710260853_0.png',\n",
       " '1711184253_0.jpg']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdf99ded756441b8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Копирование изображений и масок в соответствующие папки\n",
    "for img in train_images:\n",
    "    if 'synt' not in img:\n",
    "        shutil.copy(os.path.join(IMAGES_DIR, img), os.path.join(OUTPUT_DIR, 'images/train', img))\n",
    "        mask_name = img.replace('.jpg', '.png')\n",
    "        convert_mask_to_yolo(os.path.join(MASKS_DIR, mask_name), 'labels/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a80bd15-a3ef-4e36-8e81-ba2c1c2f0155",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F1_1_1_2.ts_f_500.jpg',\n",
       " 'F4_1_1_1.ts_f_500.jpg',\n",
       " '1710430072_0.jpg',\n",
       " 'F5_1_2_1.ts_f_1000.jpg',\n",
       " 'syntF7_1_1_1.ts_f_500.jpg',\n",
       " 'syntF4_2_3_2.ts_f_500.jpg',\n",
       " 'syntF4_2_2_2.ts_f_500.jpg',\n",
       " 'F1_1_5_2.ts_f_1000.jpg',\n",
       " 'F5_2_1_1.ts_f_1000.jpg',\n",
       " 'synt1711187868_0.jpg',\n",
       " 'syntF1_2_3_1.ts_f_500.jpg',\n",
       " '1717063048_0.jpg',\n",
       " 'F2_2_1_1.ts_f_1000.jpg',\n",
       " 'syntF2_2_1_1.ts_f_500.jpg',\n",
       " 'F1_2_5_2.ts_f_500.jpg',\n",
       " 'synt1711256271_0.jpg',\n",
       " 'F5_2_1_2.ts_f_1000.jpg',\n",
       " '1710917853_0.jpg',\n",
       " '1711180652_0.jpg',\n",
       " 'syntF2_2_2_1.ts_f_1000.jpg',\n",
       " 'F5_1_3_2.ts_f_1000.jpg',\n",
       " 'synt1715112967_0.jpg',\n",
       " 'syntF1_2_2_1.ts_f_500.jpg',\n",
       " 'syntF2_1_1_2.ts_f_500.jpg',\n",
       " 'synt1711510053_0.jpg',\n",
       " 'syntF1_1_5_2.ts_f_1000.jpg',\n",
       " 'F5_2_2_2.ts_f_500.jpg',\n",
       " 'synt1711456067_0.jpg',\n",
       " '1711090673_0.jpg',\n",
       " 'synt1711180652_0.jpg',\n",
       " 'F1_2_2_1.ts_f_1000.jpg',\n",
       " 'syntF5_2_2_2.ts_f_1000.jpg',\n",
       " '1711009681_0.jpg',\n",
       " 'F5_1_2_2.ts_f_500.jpg',\n",
       " 'syntF5_2_2_2.ts_f_500.jpg',\n",
       " '1711268876_0.jpg',\n",
       " 'syntF1_1_3_1.ts_f_500.jpg',\n",
       " 'syntF1_1_1_1.ts_f_500.jpg',\n",
       " 'F7_1_2_2.ts_f_1000.jpg',\n",
       " 'syntF1_1_5_1.ts_f_500.jpg',\n",
       " 'F5_2_1_2.ts_f_500.jpg',\n",
       " 'synt1710430072_0.jpg',\n",
       " '1717061248_0.jpg',\n",
       " 'F2_2_3_2.ts_f_1000.jpg',\n",
       " 'F4_2_3_2.ts_f_500.jpg',\n",
       " '1710835053_0.jpg',\n",
       " '1710399478_0.jpg',\n",
       " 'syntF4_2_2_2.ts_f_1000.jpg',\n",
       " 'synt1713767248_0.jpg',\n",
       " 'F1_2_5_2.ts_f_1000.jpg',\n",
       " '1711268872_0.jpg',\n",
       " '1711178852_0.jpg',\n",
       " 'syntF4_1_2_2.ts_f_1000.jpg',\n",
       " 'syntF4_2_3_1.ts_f_1000.jpg',\n",
       " '1717058869_0.jpg',\n",
       " 'synt1710835053_0.jpg',\n",
       " '1710604665_0.jpg',\n",
       " 'synt1717061248_0.jpg',\n",
       " 'syntF1_1_2_2.ts_f_1000.jpg',\n",
       " 'F1_2_2_2.ts_f_1000.jpg',\n",
       " 'syntF5_2_3_1.ts_f_1000.jpg',\n",
       " 'synt1711330053_0.jpg',\n",
       " 'synt1717390039_0.jpg',\n",
       " 'F7_2_1_2.ts_f_1000.jpg',\n",
       " 'synt1715041648_0.jpg',\n",
       " 'synt1710584874_0.jpg',\n",
       " 'F1_2_5_1.ts_f_500.jpg',\n",
       " 'F1_1_2_2.ts_f_1000.jpg',\n",
       " 'syntF1_1_2_1.ts_f_1000.jpg',\n",
       " 'F2_1_1_1.ts_f_500.jpg',\n",
       " 'syntF4_1_1_2.ts_f_1000.jpg',\n",
       " 'F2_1_1_2.ts_f_1000.jpg',\n",
       " 'synt1710604665_0.jpg',\n",
       " 'F1_1_2_1.ts_f_500.jpg',\n",
       " '1711178872_0.jpg',\n",
       " '1711448853_0.jpg',\n",
       " 'F4_1_3_1.ts_f_500.jpg',\n",
       " 'F1_1_2_1.ts_f_1000.jpg',\n",
       " 'syntF2_2_1_1.ts_f_1000.jpg',\n",
       " 'F5_2_3_1.ts_f_1000.jpg',\n",
       " 'F4_2_2_1.ts_f_1000.jpg',\n",
       " 'F5_2_1_1.ts_f_500.jpg',\n",
       " 'synt1711182453_0.jpg',\n",
       " 'F1_2_3_2.ts_f_500.jpg',\n",
       " 'syntF4_1_2_1.ts_f_1000.jpg',\n",
       " 'syntF1_1_5_2.ts_f_500.jpg',\n",
       " 'syntF4_1_2_1.ts_f_500.jpg',\n",
       " 'F7_1_1_1.ts_f_1000.jpg',\n",
       " 'syntF1_2_4_1.ts_f_500.jpg',\n",
       " 'F7_2_1_2.ts_f_500.jpg',\n",
       " 'syntF1_1_1_1.ts_f_1000.jpg',\n",
       " 'syntF4_2_3_2.ts_f_1000.jpg',\n",
       " 'syntF1_2_3_2.ts_f_500.jpg',\n",
       " 'F4_2_3_1.ts_f_1000.jpg',\n",
       " '1711450664_0.jpg',\n",
       " 'syntF1_2_3_2.ts_f_1000.jpg',\n",
       " 'syntF1_1_2_1.ts_f_500.jpg',\n",
       " 'synt1710390473_0.jpg',\n",
       " 'F2_2_3_2.ts_f_500.jpg',\n",
       " 'syntF2_2_1_2.ts_f_500.jpg',\n",
       " 'synt1717059450_0.jpg',\n",
       " 'F4_1_2_1.ts_f_1000.jpg',\n",
       " 'syntF5_2_1_2.ts_f_500.jpg',\n",
       " 'syntF7_1_1_1.ts_f_1000.jpg',\n",
       " 'syntF1_1_2_2.ts_f_500.jpg',\n",
       " 'syntF5_1_2_1.ts_f_1000.jpg',\n",
       " 'F7_1_2_2.ts_f_500.jpg',\n",
       " 'synt1711184253_0.jpg',\n",
       " 'synt1715158095_0.jpg',\n",
       " 'F5_2_3_1.ts_f_500.jpg',\n",
       " 'synt1717022950_0.jpg',\n",
       " '1710322071_0.jpg',\n",
       " 'F4_1_1_2.ts_f_1000.jpg',\n",
       " 'synt1710813452_0.jpg',\n",
       " 'synt1716017248_0.jpg',\n",
       " 'F4_1_2_2.ts_f_500.jpg',\n",
       " 'syntF1_2_5_1.ts_f_500.jpg',\n",
       " 'syntF4_1_3_2.ts_f_500.jpg',\n",
       " 'F4_2_2_2.ts_f_1000.jpg',\n",
       " 'F1_1_4_2.ts_f_500.jpg',\n",
       " '1711256271_0.jpg',\n",
       " 'F1_2_3_1.ts_f_1000.jpg',\n",
       " 'syntF2_2_3_2.ts_f_500.jpg',\n",
       " 'F2_2_2_2.ts_f_1000.jpg',\n",
       " 'synt1711178872_0.jpg',\n",
       " 'syntF2_2_3_1.ts_f_1000.jpg',\n",
       " 'F5_1_1_1.ts_f_1000.jpg',\n",
       " 'synt1710815253_0.jpg',\n",
       " 'syntF1_1_4_2.ts_f_500.jpg',\n",
       " '1712577488_0.jpg',\n",
       " 'syntF5_1_3_1.ts_f_500.jpg',\n",
       " 'F1_1_1_2.ts_f_1000.jpg',\n",
       " 'syntF1_2_3_1.ts_f_1000.jpg',\n",
       " 'F1_2_3_2.ts_f_1000.jpg',\n",
       " 'synt1710847680_0.jpg',\n",
       " 'F1_1_2_2.ts_f_500.jpg',\n",
       " 'F7_1_1_2.ts_f_1000.jpg',\n",
       " '1710811652_0.jpg',\n",
       " '1711421860_0.jpg',\n",
       " '1711187868_0.jpg',\n",
       " 'synt1710275253_0.jpg',\n",
       " 'F5_1_3_1.ts_f_500.jpg',\n",
       " 'syntF1_1_5_1.ts_f_1000.jpg',\n",
       " 'F4_2_3_2.ts_f_1000.jpg',\n",
       " 'syntF2_2_2_2.ts_f_1000.jpg',\n",
       " '1715074048_0.jpg',\n",
       " 'F2_1_1_1.ts_f_1000.jpg',\n",
       " 'syntF1_2_4_2.ts_f_500.jpg',\n",
       " 'synt1710458871_0.jpg',\n",
       " 'synt1711412853_0.jpg',\n",
       " 'F4_1_3_2.ts_f_1000.jpg',\n",
       " '1711510053_0.jpg',\n",
       " 'F1_2_4_2.ts_f_1000.jpg',\n",
       " 'syntF2_1_1_1.ts_f_1000.jpg',\n",
       " 'synt1711430853_0.jpg',\n",
       " 'F5_1_1_2.ts_f_500.jpg',\n",
       " '1711184253_0.jpg',\n",
       " 'syntF7_1_2_2.ts_f_1000.jpg',\n",
       " 'F5_1_3_1.ts_f_1000.jpg',\n",
       " 'syntF4_1_1_1.ts_f_1000.jpg',\n",
       " '1710381477_0.jpg',\n",
       " '1710275253_0.jpg',\n",
       " 'syntF5_1_1_1.ts_f_500.jpg',\n",
       " 'syntF4_1_3_1.ts_f_1000.jpg',\n",
       " 'syntF5_2_3_1.ts_f_500.jpg',\n",
       " 'F2_1_2_2.ts_f_500.jpg',\n",
       " 'synt1717058869_0.jpg',\n",
       " 'synt1711178852_0.jpg',\n",
       " 'syntF4_1_3_1.ts_f_500.jpg',\n",
       " 'synt1713350757_0.jpg',\n",
       " 'synt1712564893_0.jpg',\n",
       " 'F2_2_1_2.ts_f_1000.jpg',\n",
       " 'synt1710277054_0.jpg',\n",
       " 'F2_1_1_2.ts_f_500.jpg',\n",
       " '1711430853_0.jpg',\n",
       " 'synt1712741269_0.jpg',\n",
       " '1717068448_0.jpg',\n",
       " 'F5_2_2_1.ts_f_500.jpg',\n",
       " '1713302111_0.jpg',\n",
       " '1711330053_0.jpg',\n",
       " 'syntF2_2_3_2.ts_f_1000.jpg',\n",
       " '1710809853_0.jpg',\n",
       " 'syntF7_2_1_1.ts_f_500.jpg',\n",
       " 'synt1711268872_0.jpg',\n",
       " 'syntF5_1_1_1.ts_f_1000.jpg',\n",
       " '1711186053_0.jpg',\n",
       " 'syntF2_1_1_2.ts_f_1000.jpg',\n",
       " 'syntF1_2_2_1.ts_f_1000.jpg',\n",
       " 'synt1717063048_0.jpg',\n",
       " 'F7_1_2_1.ts_f_500.jpg',\n",
       " 'syntF4_1_2_2.ts_f_500.jpg',\n",
       " 'F5_1_2_1.ts_f_500.jpg',\n",
       " '1717390039_0.jpg',\n",
       " 'F5_1_1_2.ts_f_1000.jpg',\n",
       " 'F4_2_2_2.ts_f_500.jpg',\n",
       " '1710458871_0.jpg',\n",
       " 'F1_1_1_1.ts_f_500.jpg',\n",
       " '1710815253_0.jpg',\n",
       " 'F5_2_3_2.ts_f_500.jpg',\n",
       " '1711459680_0.jpg',\n",
       " 'syntF5_1_3_1.ts_f_1000.jpg',\n",
       " 'syntF5_2_2_1.ts_f_500.jpg',\n",
       " 'synt1712788070_0.jpg',\n",
       " 'syntF4_2_2_1.ts_f_1000.jpg',\n",
       " 'syntF1_1_1_2.ts_f_500.jpg',\n",
       " 'synt1711448853_0.jpg',\n",
       " 'synt1712555862_0.jpg',\n",
       " 'syntF5_1_2_2.ts_f_1000.jpg',\n",
       " 'syntF1_2_2_2.ts_f_1000.jpg',\n",
       " 'syntF7_1_2_1.ts_f_1000.jpg',\n",
       " 'syntF5_1_2_2.ts_f_500.jpg',\n",
       " 'syntF2_1_2_2.ts_f_1000.jpg',\n",
       " 'synt1711268876_0.jpg',\n",
       " 'F7_1_2_1.ts_f_1000.jpg',\n",
       " '1712564893_0.jpg',\n",
       " 'synt1713302111_0.jpg',\n",
       " 'synt1711119479_0.jpg',\n",
       " 'syntF1_2_4_2.ts_f_1000.jpg',\n",
       " 'F2_2_2_1.ts_f_1000.jpg',\n",
       " '1715158095_0.jpg',\n",
       " '1710277054_0.jpg',\n",
       " '1713767248_0.jpg',\n",
       " 'F1_2_2_1.ts_f_500.jpg',\n",
       " 'synt1711421860_0.jpg',\n",
       " '1711412853_0.jpg',\n",
       " 'syntF1_2_5_2.ts_f_1000.jpg',\n",
       " 'F1_2_5_1.ts_f_1000.jpg',\n",
       " '1715112967_0.jpg',\n",
       " 'syntF1_2_5_2.ts_f_500.jpg',\n",
       " 'syntF1_1_3_1.ts_f_1000.jpg',\n",
       " '1710813452_0.jpg',\n",
       " 'F1_2_4_1.ts_f_500.jpg',\n",
       " 'syntF2_2_1_2.ts_f_1000.jpg',\n",
       " 'F7_2_1_1.ts_f_500.jpg',\n",
       " 'syntF7_1_2_2.ts_f_500.jpg',\n",
       " 'F5_1_1_1.ts_f_500.jpg',\n",
       " 'syntF5_2_3_2.ts_f_500.jpg',\n",
       " 'syntF1_1_4_2.ts_f_1000.jpg',\n",
       " '1716017248_0.jpg',\n",
       " '1715041648_0.jpg',\n",
       " 'synt1710381477_0.jpg',\n",
       " 'synt1711090673_0.jpg',\n",
       " 'syntF7_2_1_2.ts_f_500.jpg',\n",
       " 'synt1710917853_0.jpg',\n",
       " 'gr1.jpg',\n",
       " '1717060391_0.jpg',\n",
       " '1713350757_0.jpg',\n",
       " 'syntF1_1_1_2.ts_f_1000.jpg',\n",
       " 'syntF1_2_2_2.ts_f_500.jpg',\n",
       " '1711177052_0.jpg',\n",
       " 'syntF5_1_3_2.ts_f_1000.jpg',\n",
       " 'F7_1_1_2.ts_f_500.jpg',\n",
       " 'syntF2_2_2_1.ts_f_500.jpg',\n",
       " 'synt1711177052_0.jpg',\n",
       " 'syntF4_1_3_2.ts_f_1000.jpg',\n",
       " 'syntF1_2_4_1.ts_f_1000.jpg',\n",
       " 'syntF5_1_2_1.ts_f_500.jpg',\n",
       " 'F5_2_2_1.ts_f_1000.jpg',\n",
       " 'F4_2_3_1.ts_f_500.jpg',\n",
       " 'F5_2_2_2.ts_f_1000.jpg',\n",
       " 'F2_1_2_2.ts_f_1000.jpg',\n",
       " 'F1_2_4_1.ts_f_1000.jpg',\n",
       " 'syntF7_1_1_2.ts_f_500.jpg',\n",
       " 'synt1712577488_0.jpg',\n",
       " 'F4_1_3_1.ts_f_1000.jpg',\n",
       " 'syntgr1.jpg',\n",
       " '1711119479_0.jpg',\n",
       " 'synt1711009681_0.jpg',\n",
       " 'F1_1_3_1.ts_f_1000.jpg',\n",
       " 'F2_2_2_1.ts_f_500.jpg',\n",
       " 'syntF2_1_2_2.ts_f_500.jpg',\n",
       " 'F2_2_1_1.ts_f_500.jpg']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "089793a9-27c9-4801-833a-49bab279f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for synt_img in train_images:\n",
    "    img = synt_img[4:]\n",
    "    if 'synt' in synt_img:\n",
    "        shutil.copy(os.path.join(IMAGES_DIR2, img), os.path.join(OUTPUT_DIR, 'images/train', img))\n",
    "        mask_name = img.replace('.jpg', '.png')\n",
    "        convert_mask_to_yolo(os.path.join(MASKS_DIR2, mask_name), 'labels/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b2fd246643ce5cf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for img in val_images:\n",
    "    if 'synt' not in img:\n",
    "        shutil.copy(os.path.join(IMAGES_DIR, img), os.path.join(OUTPUT_DIR, 'images/val', img))\n",
    "        mask_name = img.replace('.jpg', '.png')\n",
    "        convert_mask_to_yolo(os.path.join(MASKS_DIR, mask_name), 'labels/val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f5f9c58b-e536-472d-ae27-7dfab27121e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F1_1_4_2.ts_f_1000.jpg',\n",
       " 'synt4_1709186522_0.png',\n",
       " '1711182453_0.jpg',\n",
       " 'synt415_1709265902_0.png',\n",
       " '1710584874_0.jpg',\n",
       " 'F2_2_1_1.ts_f_1000.jpg',\n",
       " 'synt10_1709255042_0.png',\n",
       " 'synt20_1709344621_0.png',\n",
       " 'F1_1_1_1.ts_f_1000.jpg',\n",
       " 'F4_1_1_2.ts_f_500.jpg',\n",
       " 'synt3_1709116441_0.png',\n",
       " 'F1_1_5_1.ts_f_1000.jpg',\n",
       " 'F7_1_2_2.ts_f_500.jpg',\n",
       " 'synt105_1712788070_0.png',\n",
       " 'synt274_1709339881_0.png',\n",
       " 'F1_2_3_1.ts_f_500.jpg',\n",
       " 'synt1_1709104681_0.png',\n",
       " 'F4_2_3_2.ts_f_500.jpg',\n",
       " 'synt96_1710262654_0.png',\n",
       " 'synt406_1709191202_0.png',\n",
       " 'synt267_1709260681_0.png',\n",
       " 'synt11_1709260082_0.png',\n",
       " 'synt262_1709232001_0.png',\n",
       " 'F2_2_2_2.ts_f_500.jpg',\n",
       " '1712555862_0.jpg',\n",
       " 'F4_1_2_1.ts_f_500.jpg',\n",
       " 'synt79_1709260681_0.png',\n",
       " '1711393084_0.jpg',\n",
       " 'F4_1_1_1.ts_f_500.jpg',\n",
       " 'synt16_1709265962_0.png',\n",
       " 'F4_2_2_1.ts_f_500.jpg',\n",
       " '1710390473_0.jpg',\n",
       " 'F7_2_1_2.ts_f_1000.jpg',\n",
       " 'synt40_1717060391_0.png',\n",
       " 'synt81_1709262422_0.png',\n",
       " 'F1_1_5_2.ts_f_1000.jpg',\n",
       " 'synt107_1717060391_0.png',\n",
       " 'synt25_1709888254_0.png',\n",
       " 'F5_1_2_1.ts_f_1000.jpg',\n",
       " 'F7_2_1_2.ts_f_500.jpg',\n",
       " '1710430072_0.jpg',\n",
       " 'F5_2_1_2.ts_f_1000.jpg',\n",
       " 'synt68_1709104681_0.png',\n",
       " 'synt440_1717060391_0.png',\n",
       " '1712788070_0.jpg',\n",
       " '1710370672_0.jpg',\n",
       " 'F5_1_2_2.ts_f_500.jpg',\n",
       " 'synt401_1709104681_0.png',\n",
       " 'synt28_1710260853_0.png',\n",
       " 'F1_2_2_2.ts_f_500.jpg',\n",
       " '1717059450_0.jpg',\n",
       " 'synt30_1710266253_0.png',\n",
       " 'F7_2_1_1.ts_f_1000.jpg',\n",
       " '1712741269_0.jpg',\n",
       " '1710917853_0.jpg',\n",
       " 'synt18_1709310482_0.png',\n",
       " 'F2_2_1_2.ts_f_500.jpg',\n",
       " '1717063048_0.jpg',\n",
       " '1711180652_0.jpg',\n",
       " 'synt284_1710262654_0.png',\n",
       " 'synt272_1709276582_0.png',\n",
       " 'F2_2_3_1.ts_f_1000.jpg',\n",
       " 'F1_2_5_2.ts_f_1000.jpg',\n",
       " 'F4_1_3_2.ts_f_500.jpg']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "728b00f2-960b-4a91-9ea3-30c9ab79cc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for synt_img in val_images:\n",
    "    img = synt_img[4:]\n",
    "    if 'synt' in synt_img:\n",
    "        shutil.copy(os.path.join(IMAGES_DIR2, img), os.path.join(OUTPUT_DIR, 'images/val', img))\n",
    "        mask_name = img.replace('.jpg', '.png')\n",
    "        convert_mask_to_yolo(os.path.join(MASKS_DIR2, mask_name), 'labels/val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d7767e7853d7b98",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасет успешно разбит и сохранен в структуре проекта.\n"
     ]
    }
   ],
   "source": [
    "data_yaml_content = f\"\"\"\n",
    "train: train_data/images/train\n",
    "val: train_data/images/val\n",
    "\n",
    "nc: 1  # Обновите количество классов (1 для загрязнения)\n",
    "names: ['contaminated']  # Обновите названия классов\n",
    "\"\"\"\n",
    "\n",
    "with open('data.yaml', 'w') as f:\n",
    "    f.write(data_yaml_content)\n",
    "\n",
    "print(\"Датасет успешно разбит и сохранен в структуре проекта.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a73b014c45fcc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo11s-seg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4d7d986b-fc45-4bf7-8b52-d84560535820",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/home/jovyan/nazar/123/123/misis_chill/custom.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e09be0ae-0ae7-4847-9a62-4187bf99d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.model.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "113b1560-dc6c-4e7a-bec6-a9ff032a1331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ade5ec11-9c95-4af8-9e2d-f19775fd8e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jovyan/nazar/123/123/misis_chill/custom_config.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "f3c5e120-87cd-452b-a685-81a11164ccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ no model scale passed. Assuming scale='l'.\n",
      "Transferred 134/1077 items from pretrained weights\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('/home/jovyan/nazar/123/123/misis_chill/custom_config.yaml').load('/home/jovyan/nazar/123/123/misis_chill/custom.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23721b-1a73-44d5-8815-4bce1bb91e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d9841482-ab69-45b3-a57f-2cd4db100937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c09a256a-d522-43a8-ae46-c642bea92df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.model[0].conv = nn.Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "375731ea-4c42-496a-bcc1-75e45ac58bb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): SegmentationModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(8, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): C2PSA(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): Sequential(\n",
       "          (0): PSABlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): Conv(\n",
       "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (proj): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (pe): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (ffn): Sequential(\n",
       "              (0): Conv(\n",
       "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (12): Concat()\n",
       "      (13): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (15): Concat()\n",
       "      (16): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (18): Concat()\n",
       "      (19): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (21): Concat()\n",
       "      (22): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): Segment(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (proto): Proto(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (upsample): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (cv4): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d85abfbc10ae302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.47 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.44 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (A100 80GB PCIe, 81252MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=segment, mode=train, model=yolo11s-seg.pt, data=./data.yaml, epochs=20, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=train10, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/segment/train10\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
      " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
      " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 23        [16, 19, 22]  1   1474291  ultralytics.nn.modules.head.Segment          [1, 32, 128, [128, 256, 512]] \n",
      "YOLO11s-seg summary: 355 layers, 10,082,675 parameters, 10,082,659 gradients, 35.6 GFLOPs\n",
      "\n",
      "Transferred 555/561 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/segment/train10', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/labels/train... 641 images, 511 backgrounds, 0 corrupt: 100%|██████████| 797/797 [00:01<00:00, 487.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/labels/train.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/labels/val... 208 images, 114 backgrounds, 0 corrupt: 100%|██████████| 208/208 [00:00<00:00, 301.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/labels/val.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/segment/train10/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 90 weight(decay=0.0), 101 weight(decay=0.0005), 100 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/segment/train10\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20      5.66G      1.522      2.951      3.384      1.422        117        640: 100%|██████████| 50/50 [00:07<00:00,  6.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:01<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.511       0.49      0.449      0.229      0.502      0.479      0.438      0.209\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20      5.38G      1.387      2.309      1.415      1.287        115        640: 100%|██████████| 50/50 [00:06<00:00,  8.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.436      0.539      0.392      0.224      0.432      0.525      0.375      0.199\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20      5.45G      1.348      2.188      1.254      1.249         69        640: 100%|██████████| 50/50 [00:06<00:00,  8.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.628      0.618      0.572      0.327      0.633      0.617      0.568      0.307\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20      5.45G      1.333      2.114      1.233       1.26         94        640: 100%|██████████| 50/50 [00:05<00:00,  8.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.783      0.677      0.728      0.473      0.796      0.677      0.732      0.437\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20      5.38G      1.308      2.042      1.189      1.244         92        640: 100%|██████████| 50/50 [00:05<00:00,  8.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  7.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.818      0.661      0.751      0.467      0.805      0.656      0.731      0.374\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20      5.32G      1.238       1.95      1.062      1.211        151        640: 100%|██████████| 50/50 [00:06<00:00,  8.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  7.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.792      0.765      0.763      0.506      0.796      0.765      0.764      0.466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20      5.52G      1.136      1.859     0.9505      1.158         50        640: 100%|██████████| 50/50 [00:06<00:00,  8.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.768      0.739      0.773      0.512       0.81      0.711      0.767      0.438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20      5.49G      1.138      1.821     0.9833      1.167         61        640: 100%|██████████| 50/50 [00:06<00:00,  8.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  7.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.846      0.746      0.779      0.528      0.839      0.739      0.761      0.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20      5.43G      1.122      1.823      0.916      1.138         95        640: 100%|██████████| 50/50 [00:05<00:00,  8.42it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  8.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954       0.81      0.779      0.805      0.514      0.811      0.772      0.793      0.426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20      5.42G      1.076      1.718     0.8887      1.122        106        640: 100%|██████████| 50/50 [00:05<00:00,  8.42it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  7.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.883      0.787       0.84      0.582      0.886      0.788      0.826      0.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20      5.34G      1.092       1.71      1.049      1.142         49        640: 100%|██████████| 50/50 [00:06<00:00,  8.05it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  7.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.857      0.733      0.796      0.555       0.85       0.73      0.781      0.466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20      5.45G      1.138       1.75      1.038       1.18        105        640: 100%|██████████| 50/50 [00:05<00:00,  9.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.823      0.735      0.775      0.535      0.828      0.729      0.756      0.441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20      5.34G        1.1      1.704     0.9696       1.15         55        640: 100%|██████████| 50/50 [00:05<00:00,  9.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  8.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.873      0.718      0.819      0.574      0.872      0.715      0.806      0.513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20      5.46G      1.037       1.64     0.9051      1.121         33        640: 100%|██████████| 50/50 [00:05<00:00,  9.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  8.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.855      0.809      0.854      0.611      0.862      0.776      0.829      0.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20       5.4G      1.021      1.601      0.843      1.097         55        640: 100%|██████████| 50/50 [00:05<00:00,  9.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  8.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.845       0.79      0.839      0.592      0.901       0.74      0.814      0.498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20      5.32G     0.9606      1.555     0.9769      1.102          4        640: 100%|██████████| 50/50 [00:05<00:00,  9.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.806       0.78      0.826      0.608      0.877      0.717      0.821      0.557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20      5.37G     0.9491      1.525     0.8296       1.08         21        640: 100%|██████████| 50/50 [00:05<00:00,  9.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.855      0.829      0.858      0.623      0.854      0.819       0.84      0.518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20      5.45G     0.9197      1.449     0.7579      1.053         24        640: 100%|██████████| 50/50 [00:05<00:00,  9.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  8.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.902      0.787      0.865      0.641      0.899      0.781      0.846      0.551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20      5.47G     0.9029      1.473     0.7457      1.059         61        640: 100%|██████████| 50/50 [00:05<00:00,  9.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.873      0.835      0.869       0.67      0.869      0.821      0.856      0.574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20       5.3G     0.8687      1.378     0.6855      1.045         15        640: 100%|██████████| 50/50 [00:05<00:00,  9.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.892      0.834      0.881      0.681      0.887      0.829      0.864      0.583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 epochs completed in 0.044 hours.\n",
      "Optimizer stripped from runs/segment/train10/weights/last.pt, 20.5MB\n",
      "Optimizer stripped from runs/segment/train10/weights/best.pt, 20.5MB\n",
      "\n",
      "Validating runs/segment/train10/weights/best.pt...\n",
      "Ultralytics 8.3.44 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (A100 80GB PCIe, 81252MiB)\n",
      "YOLO11s-seg summary (fused): 265 layers, 10,067,203 parameters, 0 gradients, 35.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:01<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.894      0.835      0.881      0.681      0.887      0.829      0.864      0.583\n",
      "Speed: 0.2ms preprocess, 0.9ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/train10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_results = model.train(\n",
    "    data=\"./data.yaml\",  # path to dataset YAML\n",
    "    epochs=20,  # number of training epochs\n",
    "    imgsz=640,  # training image size\n",
    "    device=\"0\",  # device to run on, i.e. device=0 or device=0,1,2,3 or device=cpu\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2660d21c659b40fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.44 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (A100 80GB PCIe, 81252MiB)\n",
      "YOLO11s-seg summary (fused): 265 layers, 10,067,203 parameters, 0 gradients, 35.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/labels/val.cache... 208 images, 114 backgrounds, 0 corrupt: 100%|██████████| 208/208 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:03<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        208        954      0.893      0.833      0.881      0.686      0.884      0.824       0.86      0.578\n",
      "Speed: 0.2ms preprocess, 2.0ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/train102\u001b[0m\n",
      "ultralytics.utils.metrics.SegmentMetrics object with attributes:\n",
      "\n",
      "ap_class_index: array([0])\n",
      "box: ultralytics.utils.metrics.Metric object\n",
      "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7fdf9d344700>\n",
      "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)', 'Precision-Recall(M)', 'F1-Confidence(M)', 'Precision-Confidence(M)', 'Recall-Confidence(M)']\n",
      "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,\n",
      "            0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,\n",
      "            0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,\n",
      "            0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,\n",
      "            0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,\n",
      "            0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,\n",
      "            0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,\n",
      "            0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,\n",
      "            0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,\n",
      "            0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,\n",
      "            0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,\n",
      "            0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,\n",
      "            0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,\n",
      "            0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,\n",
      "            0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,\n",
      "            0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,\n",
      "            0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.99171,     0.98901,     0.98663,     0.98663,     0.98663,     0.98663,     0.98663,     0.98663,     0.98663,     0.98663,     0.98663,     0.98663,     0.98477,     0.98477,     0.98477,     0.98477,\n",
      "            0.98477,     0.98477,     0.98477,     0.98477,     0.98477,     0.98477,     0.98477,     0.98477,     0.98477,     0.98477,     0.98477,     0.98477,     0.98477,     0.98477,     0.98477,     0.98477,     0.98284,     0.98284,     0.98284,     0.98284,     0.98284,     0.98284,     0.98284,\n",
      "            0.98284,     0.98284,     0.98284,     0.98284,     0.98284,     0.98284,     0.98049,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,\n",
      "            0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,     0.97991,\n",
      "             0.9784,      0.9784,      0.9784,      0.9784,      0.9784,      0.9784,      0.9784,      0.9784,      0.9784,      0.9784,      0.9784,      0.9784,      0.9784,      0.9784,      0.9784,     0.97674,     0.97674,     0.97674,     0.97674,     0.97674,     0.97674,     0.97674,     0.97674,\n",
      "            0.97674,     0.97556,     0.97556,     0.97556,     0.97556,     0.97556,     0.97556,     0.97556,     0.97556,     0.97556,     0.97556,     0.97556,     0.97556,     0.97556,     0.97556,     0.97556,     0.97556,     0.97556,     0.97556,      0.9739,      0.9739,      0.9739,      0.9739,\n",
      "             0.9739,      0.9739,     0.97318,     0.97318,     0.97318,     0.97318,     0.97318,     0.97318,     0.97318,     0.97318,     0.97318,     0.97318,     0.97318,     0.97318,     0.97318,     0.97318,     0.97318,     0.97318,     0.97318,     0.97318,     0.97318,     0.97318,     0.97318,\n",
      "            0.97318,     0.97318,     0.97318,     0.97243,     0.97243,     0.97243,     0.97243,     0.97243,     0.97243,     0.97243,     0.97243,     0.97243,     0.97243,     0.97243,     0.97243,     0.97243,     0.97243,     0.97243,     0.97243,     0.97243,     0.97243,     0.97243,     0.97243,\n",
      "            0.97243,     0.97243,     0.97178,     0.97178,     0.97178,     0.97178,     0.97178,     0.97178,     0.97178,     0.97178,     0.97178,     0.97178,     0.97178,     0.97178,     0.97178,     0.97178,     0.97178,     0.97178,     0.97178,     0.97178,     0.97178,     0.97178,     0.97178,\n",
      "            0.97178,     0.97178,     0.97028,     0.97028,     0.97028,     0.97028,     0.97028,     0.96944,     0.96944,     0.96944,     0.96944,     0.96944,     0.96944,     0.96944,     0.96944,     0.96944,     0.96944,     0.96944,     0.96944,     0.96944,     0.96944,     0.96944,     0.96944,\n",
      "            0.96817,     0.96817,     0.96817,     0.96817,     0.96817,     0.96817,     0.96817,     0.96817,     0.96678,     0.96678,     0.96678,     0.96678,     0.96523,      0.9648,      0.9648,      0.9648,      0.9648,      0.9648,      0.9648,      0.9648,      0.9648,      0.9648,      0.9648,\n",
      "             0.9648,      0.9648,      0.9648,      0.9648,      0.9648,      0.9648,      0.9648,      0.9648,      0.9648,      0.9648,      0.9648,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,\n",
      "            0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,\n",
      "            0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96407,     0.96131,     0.96131,     0.96018,     0.96018,     0.96018,     0.96018,     0.96018,     0.95912,     0.95912,     0.95912,     0.95912,     0.95912,     0.95912,     0.95803,     0.95803,\n",
      "            0.95803,     0.95803,     0.95803,     0.95803,     0.95677,     0.95677,     0.95429,     0.95429,     0.95429,     0.95429,     0.95319,     0.95319,     0.95319,     0.95319,     0.95211,     0.95211,     0.95211,     0.95211,     0.94979,     0.94979,     0.94979,     0.94979,     0.94979,\n",
      "            0.94979,     0.94875,     0.94875,     0.94875,     0.94875,     0.94751,      0.9465,      0.9465,      0.9465,      0.9465,     0.94536,     0.94536,     0.94489,     0.94489,     0.94489,     0.94489,     0.94489,     0.94489,     0.94489,     0.94489,     0.94489,     0.94489,     0.94489,\n",
      "            0.94489,     0.94407,     0.94407,     0.94407,     0.94407,     0.94407,     0.94407,     0.94327,     0.94327,     0.94327,     0.94327,     0.94327,     0.94327,     0.94256,     0.94256,     0.94256,     0.94256,     0.94256,     0.94256,     0.94256,     0.94256,     0.94171,     0.94171,\n",
      "            0.94171,     0.94171,     0.94171,     0.93959,     0.93959,     0.93959,     0.93959,     0.93614,     0.93614,     0.93384,     0.93291,     0.93291,     0.93291,     0.93182,     0.93082,     0.93082,     0.92974,     0.92875,     0.92875,     0.92875,     0.92777,     0.92777,     0.92327,\n",
      "             0.9227,      0.9227,      0.9227,      0.9227,      0.9227,      0.9227,     0.92054,      0.9201,      0.9201,      0.9201,      0.9201,      0.9201,      0.9201,      0.9201,     0.91697,     0.91697,     0.91597,     0.91497,     0.91497,     0.91398,     0.91201,     0.91201,     0.91156,\n",
      "            0.91156,     0.91156,     0.91156,     0.91156,     0.91156,     0.91059,     0.90962,     0.90877,     0.90877,     0.90782,     0.90581,      0.9052,      0.9052,      0.9052,      0.9052,     0.90323,     0.90252,     0.90252,     0.90252,     0.90252,     0.90171,     0.90171,      0.9008,\n",
      "                0.9,         0.9,     0.89909,     0.89729,     0.89729,     0.89262,     0.89262,     0.89262,     0.89075,        0.89,        0.89,     0.88423,     0.88048,     0.87895,     0.87895,     0.87895,     0.87895,     0.87813,     0.86975,      0.8671,     0.86538,     0.86214,     0.86214,\n",
      "            0.86214,     0.86138,     0.85519,      0.8546,      0.8546,     0.84855,     0.84783,     0.84449,     0.83947,     0.83792,     0.83047,     0.82981,     0.82665,     0.82517,     0.82488,     0.82488,     0.82488,     0.82488,     0.80739,     0.79294,     0.79238,     0.79183,     0.78531,\n",
      "            0.77965,     0.77695,      0.7693,       0.766,     0.74978,     0.74205,     0.73989,     0.73989,     0.73583,     0.73583,     0.73478,     0.73438,      0.7327,     0.69451,     0.69419,     0.69419,      0.6905,     0.68135,     0.66175,     0.66175,     0.66099,     0.65972,     0.63928,\n",
      "            0.62237,     0.62066,     0.62066,     0.61472,     0.58215,       0.572,     0.56075,     0.55742,     0.55413,     0.54984,     0.54086,     0.53837,     0.53837,     0.53837,     0.53837,     0.51296,     0.46857,     0.40361,     0.37171,     0.37171,     0.34611,     0.29578,     0.28232,\n",
      "            0.28003,     0.24768,     0.20527,     0.20488,     0.18335,     0.17833,     0.15903,     0.15558,     0.12551,      0.1073,      0.1071,    0.090756,    0.089421,    0.088087,    0.086752,    0.085417,    0.084083,    0.082748,    0.081413,    0.080079,    0.078744,    0.077409,    0.076075,\n",
      "            0.07474,    0.073406,    0.072071,    0.070736,    0.069402,    0.068067,    0.066732,    0.065398,    0.064063,    0.062728,    0.061394,    0.060059,    0.058724,     0.05739,    0.056055,     0.05472,    0.053386,    0.052051,    0.050717,    0.049382,    0.048047,    0.046713,    0.045378,\n",
      "           0.044043,    0.042709,    0.041374,    0.040039,    0.038705,     0.03737,    0.036035,    0.034701,    0.033366,    0.032032,    0.030697,    0.029362,    0.028028,    0.026693,    0.025358,    0.024024,    0.022689,    0.021354,     0.02002,    0.018685,     0.01735,    0.016016,    0.014681,\n",
      "           0.013346,    0.012012,    0.010677,   0.0093425,   0.0080079,   0.0066732,   0.0053386,   0.0040039,   0.0026693,   0.0013346,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.16555,     0.16566,     0.26482,     0.33576,     0.38737,     0.42868,     0.45888,      0.4901,     0.51477,     0.53489,     0.54954,     0.56785,     0.57965,     0.59191,     0.60267,     0.61443,      0.6215,     0.63331,      0.6427,     0.64996,     0.65609,      0.6612,     0.66771,\n",
      "            0.67573,     0.67967,     0.68398,     0.68745,     0.69188,     0.69481,     0.69971,     0.70365,     0.70718,      0.7099,      0.7139,      0.7169,     0.71921,     0.72243,     0.72637,     0.72996,     0.73083,     0.73384,     0.73517,     0.73681,     0.74022,     0.74222,     0.74579,\n",
      "            0.74643,     0.74839,     0.74983,     0.75131,     0.75285,     0.75415,     0.75632,     0.75969,     0.75971,     0.76023,     0.75967,     0.76135,     0.76218,     0.76421,     0.76618,     0.76791,     0.76911,     0.77095,     0.77252,     0.77443,     0.77524,     0.77645,     0.77759,\n",
      "            0.77906,     0.77959,     0.77971,     0.78074,     0.78258,     0.78492,     0.78545,     0.78621,     0.78767,     0.78833,     0.78861,     0.78991,     0.79162,       0.794,     0.79476,     0.79545,     0.79658,     0.79883,     0.79945,     0.80031,     0.80116,     0.80186,     0.80306,\n",
      "             0.8031,     0.80293,     0.80254,     0.80353,     0.80425,     0.80407,     0.80479,     0.80527,     0.80608,     0.80796,     0.80858,     0.80916,     0.80927,     0.80938,     0.80938,     0.80912,     0.80895,     0.80973,     0.80993,     0.81022,     0.81106,     0.81184,     0.81255,\n",
      "            0.81314,     0.81398,     0.81429,     0.81597,     0.81618,     0.81641,     0.81676,     0.81702,     0.81766,     0.81811,     0.81809,     0.81975,     0.81929,      0.8201,      0.8217,     0.82216,     0.82257,     0.82279,     0.82348,     0.82377,     0.82443,     0.82553,      0.8261,\n",
      "            0.82738,     0.82693,     0.82726,     0.82751,     0.82872,     0.82948,     0.83015,     0.83041,     0.83012,     0.83026,     0.83011,     0.82996,     0.82981,     0.82982,     0.83014,     0.83054,     0.83081,     0.83106,     0.83168,     0.83199,     0.83265,     0.83362,     0.83438,\n",
      "            0.83515,     0.83633,     0.83662,     0.83751,      0.8371,     0.83709,      0.8373,     0.83746,      0.8376,     0.83773,     0.83852,     0.83891,     0.83923,     0.83955,     0.83999,     0.84014,      0.8403,     0.84075,     0.84107,     0.84171,     0.84259,     0.84276,     0.84298,\n",
      "            0.84341,      0.8442,     0.84438,     0.84456,     0.84513,      0.8453,     0.84548,     0.84573,     0.84609,     0.84511,      0.8449,     0.84469,     0.84473,     0.84459,     0.84498,     0.84515,     0.84527,     0.84515,     0.84503,     0.84491,     0.84479,     0.84495,     0.84552,\n",
      "            0.84643,     0.84609,     0.84619,     0.84581,     0.84642,     0.84833,      0.8494,     0.84928,      0.8497,     0.84986,     0.85008,     0.85032,     0.85057,      0.8516,     0.85193,     0.85186,     0.85204,     0.85222,     0.85267,     0.85294,     0.85296,     0.85257,     0.85283,\n",
      "            0.85291,     0.85278,     0.85265,     0.85251,     0.85238,     0.85288,     0.85295,     0.85302,      0.8531,     0.85317,     0.85324,     0.85336,     0.85349,     0.85362,      0.8538,     0.85411,      0.8554,     0.85451,     0.85459,     0.85415,     0.85444,     0.85481,     0.85533,\n",
      "            0.85614,     0.85669,     0.85701,     0.85674,     0.85702,     0.85696,     0.85662,     0.85642,     0.85626,      0.8561,     0.85594,     0.85544,     0.85597,     0.85622,     0.85638,     0.85653,     0.85669,     0.85687,     0.85705,       0.857,     0.85671,     0.85669,     0.85744,\n",
      "            0.85719,     0.85695,     0.85743,     0.85766,     0.85787,     0.85807,     0.85814,     0.85793,     0.85772,     0.85772,     0.85802,     0.85816,     0.85827,     0.85839,     0.85851,     0.85994,     0.86031,     0.86089,     0.86133,     0.86145,     0.86158,     0.86171,     0.86154,\n",
      "            0.86134,     0.86115,     0.86124,     0.86141,     0.86157,     0.86083,     0.86065,     0.86048,     0.86025,      0.8599,     0.86016,     0.86057,     0.86073,     0.86088,     0.86132,     0.86173,     0.86185,     0.86174,     0.86163,     0.86152,     0.86141,      0.8613,     0.86202,\n",
      "             0.8623,      0.8625,     0.86273,     0.86324,     0.86382,      0.8639,     0.86361,      0.8633,     0.86294,     0.86277,     0.86292,     0.86306,     0.86324,      0.8635,     0.86292,      0.8627,     0.86247,       0.862,     0.86236,     0.86254,     0.86271,     0.86367,     0.86403,\n",
      "            0.86316,     0.86337,     0.86352,     0.86367,     0.86382,     0.86364,     0.86343,     0.86322,     0.86343,     0.86364,     0.86332,     0.86297,     0.86277,     0.86256,     0.86253,     0.86273,     0.86288,     0.86246,     0.86263,     0.86237,     0.86175,      0.8605,     0.86014,\n",
      "            0.86023,     0.86031,      0.8604,     0.86048,     0.86057,     0.86051,     0.86064,     0.86077,     0.86089,     0.86004,     0.85965,      0.8595,     0.85935,      0.8592,     0.85905,     0.85885,     0.85864,     0.85843,     0.85849,     0.85858,     0.85868,     0.85878,     0.85886,\n",
      "            0.85841,     0.85863,       0.859,     0.85862,     0.85893,     0.85836,     0.85821,     0.85806,     0.85792,     0.85791,     0.85764,     0.85779,     0.85794,     0.85809,      0.8576,     0.85778,      0.8575,     0.85713,     0.85664,     0.85647,     0.85631,     0.85614,      0.8552,\n",
      "            0.85436,     0.85427,     0.85338,     0.85349,     0.85361,     0.85372,     0.85425,     0.85414,     0.85404,     0.85393,     0.85382,     0.85372,     0.85368,     0.85398,       0.854,     0.85381,     0.85362,     0.85392,     0.85363,     0.85331,     0.85367,     0.85322,     0.85316,\n",
      "            0.85353,     0.85362,     0.85371,     0.85379,     0.85388,     0.85319,     0.85294,     0.85269,     0.85251,     0.85235,      0.8522,     0.85204,     0.85188,     0.85172,     0.85156,     0.85139,       0.851,     0.85066,     0.85055,     0.85044,     0.85034,     0.85023,     0.85012,\n",
      "            0.84998,     0.84982,     0.84966,     0.84949,     0.84987,      0.8494,     0.84969,     0.85017,     0.84997,     0.84977,     0.84958,     0.84863,     0.84723,     0.84663,     0.84667,     0.84615,     0.84628,     0.84642,     0.84655,     0.84715,     0.84732,     0.84749,     0.84765,\n",
      "            0.84781,     0.84797,     0.84794,     0.84777,      0.8476,     0.84743,     0.84701,     0.84678,     0.84689,     0.84701,     0.84713,     0.84713,      0.8469,     0.84667,     0.84652,     0.84647,     0.84642,     0.84636,     0.84631,     0.84625,      0.8462,     0.84615,     0.84609,\n",
      "            0.84604,     0.84598,     0.84593,     0.84592,     0.84602,     0.84612,     0.84622,     0.84632,     0.84613,     0.84601,     0.84582,     0.84562,     0.84495,     0.84526,     0.84471,     0.84483,     0.84494,     0.84505,     0.84517,     0.84491,     0.84459,     0.84429,     0.84399,\n",
      "            0.84329,     0.84351,     0.84362,     0.84332,     0.84303,     0.84339,     0.84396,     0.84355,     0.84277,     0.84321,      0.8436,     0.84375,     0.84388,     0.84401,     0.84404,     0.84379,     0.84353,     0.84277,     0.84264,     0.84252,     0.84239,     0.84226,     0.84214,\n",
      "            0.84146,     0.84163,      0.8418,     0.84203,     0.84239,     0.84179,      0.8414,     0.84039,     0.84028,     0.84018,     0.84007,     0.83996,     0.83986,     0.83975,     0.83947,     0.83911,     0.83923,     0.83842,     0.83775,     0.83709,     0.83671,      0.8365,     0.83629,\n",
      "            0.83612,     0.83602,     0.83592,     0.83582,     0.83572,     0.83562,     0.83552,     0.83538,      0.8352,     0.83501,     0.83483,      0.8352,     0.83512,     0.83492,     0.83473,     0.83389,     0.83357,     0.83325,     0.83167,     0.83129,     0.83132,     0.83152,     0.83147,\n",
      "            0.83073,     0.82958,     0.82945,     0.82932,      0.8292,     0.82907,     0.82894,     0.82824,     0.82762,     0.82777,     0.82801,     0.82725,     0.82712,       0.827,     0.82688,     0.82675,      0.8266,     0.82631,     0.82602,     0.82523,     0.82479,     0.82442,     0.82409,\n",
      "            0.82366,     0.82171,     0.82073,     0.82077,     0.81949,     0.81985,     0.81898,     0.81828,     0.81773,     0.81745,     0.81717,     0.81739,     0.81708,     0.81677,     0.81509,     0.81466,     0.81489,     0.81497,     0.81416,     0.81248,     0.81196,     0.81154,      0.8117,\n",
      "            0.81185,     0.81201,     0.81199,     0.81127,     0.81086,     0.81055,     0.80975,     0.81009,     0.80908,     0.80871,      0.8073,     0.80765,     0.80585,     0.80535,     0.80503,     0.80526,     0.80512,     0.80469,     0.80447,     0.80482,     0.80266,     0.80213,      0.8015,\n",
      "            0.79943,     0.79917,     0.79892,      0.7981,     0.79685,      0.7949,     0.79399,      0.7935,     0.79313,     0.79377,     0.79405,     0.79289,     0.79095,     0.79036,     0.78852,     0.78684,     0.78586,     0.78489,     0.78463,     0.78437,     0.78261,     0.78218,     0.77966,\n",
      "            0.77806,     0.77746,     0.77684,     0.77595,     0.77565,     0.77534,     0.77432,     0.77413,      0.7719,     0.77141,     0.77017,     0.76714,     0.76606,     0.76435,     0.76375,     0.76336,     0.76236,     0.75987,     0.75571,     0.75369,     0.75114,     0.74996,     0.74877,\n",
      "            0.74833,     0.74795,     0.74763,     0.74669,     0.74544,     0.74505,      0.7434,     0.74241,     0.74008,     0.73884,     0.73657,     0.73598,     0.73564,     0.73529,     0.73177,     0.72983,     0.72844,     0.72805,     0.72725,     0.72639,     0.72585,     0.72509,     0.72444,\n",
      "            0.72424,      0.7243,     0.72385,     0.72141,      0.7188,     0.71782,     0.71574,     0.71465,     0.71376,     0.71105,     0.70945,     0.70776,     0.70627,     0.70541,      0.7045,     0.70371,     0.69984,     0.69878,     0.69674,     0.69578,      0.6952,     0.69423,     0.69114,\n",
      "             0.6895,     0.68829,     0.68631,     0.68285,     0.67992,     0.67844,     0.67687,     0.67176,     0.66777,     0.66438,     0.66273,     0.65922,     0.65619,     0.65205,     0.64715,     0.64716,      0.6421,     0.63974,     0.63913,     0.63533,      0.6338,     0.62762,     0.62594,\n",
      "            0.62369,     0.61861,     0.61602,     0.61425,       0.613,      0.6097,     0.60457,     0.60096,     0.59364,      0.5894,     0.58736,     0.58383,     0.58181,     0.57761,     0.57528,     0.57286,     0.57018,     0.56265,     0.56123,     0.55734,     0.55135,     0.54654,     0.54613,\n",
      "            0.54538,     0.54406,     0.54242,     0.54126,     0.53589,     0.52926,     0.52398,     0.51777,     0.50939,     0.50606,     0.49875,     0.49459,     0.48599,     0.47989,     0.47541,     0.46598,     0.45602,     0.44864,     0.43941,     0.43306,     0.43026,     0.41532,     0.40832,\n",
      "            0.39682,     0.38988,     0.38365,     0.37911,     0.37281,     0.36337,     0.35546,     0.35178,     0.34188,     0.32842,     0.31755,     0.31001,     0.29734,     0.28571,     0.27764,     0.27368,      0.2654,     0.25008,     0.23762,     0.23216,     0.21836,     0.20763,     0.20005,\n",
      "            0.19254,     0.18353,     0.17278,     0.16531,     0.15225,     0.14438,     0.13246,     0.12827,     0.12095,     0.10791,     0.10209,    0.090083,    0.079806,    0.077055,    0.067418,    0.063896,    0.055963,    0.051611,    0.043675,    0.036622,    0.033868,    0.028916,    0.023978,\n",
      "           0.019155,    0.015808,    0.014155,    0.013257,    0.010353,   0.0098686,   0.0093843,   0.0088998,   0.0084151,   0.0071697,   0.0059457,    0.004992,   0.0040552,   0.0032157,   0.0023755,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.090844,    0.090908,     0.15443,     0.20518,     0.24516,     0.27932,     0.30587,     0.33425,     0.35778,     0.37788,     0.39267,     0.41186,     0.42439,     0.43766,     0.44954,     0.46275,     0.47109,      0.4848,     0.49589,     0.50459,     0.51202,      0.5186,     0.52666,\n",
      "            0.53671,     0.54319,     0.54871,     0.55397,     0.56055,     0.56441,     0.57091,     0.57659,     0.58135,     0.58548,     0.59094,     0.59506,     0.59826,     0.60272,     0.60823,     0.61328,       0.615,     0.62027,     0.62267,     0.62503,     0.62996,     0.63286,     0.63807,\n",
      "            0.63954,     0.64241,     0.64455,     0.64674,     0.64902,     0.65095,      0.6542,     0.65925,     0.65985,     0.66088,     0.66151,     0.66406,     0.66532,     0.66842,     0.67145,      0.6741,     0.67597,     0.67881,     0.68187,     0.68484,      0.6861,       0.688,     0.69042,\n",
      "            0.69274,     0.69359,     0.69442,     0.69669,     0.69963,     0.70338,     0.70424,     0.70546,     0.70781,     0.70889,     0.70933,     0.71144,     0.71421,      0.7181,     0.71935,     0.72048,     0.72234,     0.72604,     0.72706,     0.72849,     0.72991,     0.73107,     0.73377,\n",
      "            0.73425,       0.735,      0.7358,     0.73746,     0.73868,     0.73953,     0.74106,     0.74189,       0.744,     0.74722,     0.74827,     0.74927,     0.74946,     0.74965,     0.74975,     0.74964,     0.74967,     0.75102,     0.75136,     0.75186,      0.7533,     0.75465,     0.75587,\n",
      "             0.7569,     0.75836,      0.7589,     0.76182,     0.76219,     0.76258,     0.76319,     0.76366,     0.76477,     0.76556,     0.76632,     0.76929,     0.76924,     0.77067,     0.77349,     0.77432,     0.77505,     0.77543,     0.77666,     0.77799,     0.78002,     0.78198,       0.783,\n",
      "            0.78531,     0.78534,     0.78594,      0.7864,     0.78858,     0.78995,     0.79118,     0.79165,     0.79198,     0.79235,      0.7923,     0.79225,      0.7922,     0.79282,     0.79375,     0.79449,     0.79499,     0.79544,     0.79657,     0.79715,     0.79835,     0.80014,     0.80155,\n",
      "            0.80297,     0.80516,     0.80569,     0.80738,     0.80725,     0.80746,     0.80786,     0.80816,     0.80841,     0.80865,     0.81012,     0.81087,     0.81145,     0.81205,     0.81288,     0.81317,     0.81345,      0.8143,      0.8149,     0.81611,     0.81777,     0.81809,      0.8185,\n",
      "             0.8193,      0.8208,     0.82113,     0.82147,     0.82256,     0.82289,     0.82322,      0.8237,     0.82438,     0.82451,     0.82445,     0.82438,     0.82464,     0.82505,     0.82607,     0.82639,     0.82665,     0.82661,     0.82658,     0.82654,     0.82651,     0.82696,     0.82806,\n",
      "            0.82981,     0.82971,     0.83045,     0.83034,     0.83173,     0.83544,     0.83751,     0.83827,     0.83942,      0.8404,     0.84083,      0.8413,     0.84179,      0.8438,     0.84449,     0.84533,     0.84568,     0.84603,     0.84693,     0.84747,     0.84778,     0.84776,     0.84827,\n",
      "            0.84853,      0.8485,     0.84846,     0.84843,     0.84839,     0.84939,     0.84953,     0.84968,     0.84982,     0.84997,     0.85011,     0.85034,      0.8506,     0.85086,     0.85123,     0.85183,      0.8544,     0.85472,     0.85515,     0.85504,     0.85563,     0.85638,     0.85742,\n",
      "            0.85905,     0.86016,     0.86131,     0.86134,     0.86189,      0.8621,     0.86201,     0.86197,     0.86193,     0.86189,     0.86185,     0.86173,       0.863,      0.8635,     0.86383,     0.86414,     0.86446,     0.86482,     0.86519,     0.86535,     0.86528,     0.86555,     0.86709,\n",
      "            0.86704,     0.86698,     0.86816,     0.86863,     0.86906,     0.86948,     0.86974,     0.86969,     0.86964,     0.86986,     0.87048,     0.87076,       0.871,     0.87125,     0.87149,     0.87445,      0.8752,     0.87642,     0.87731,     0.87758,     0.87784,     0.87811,     0.87809,\n",
      "            0.87805,       0.878,     0.87827,     0.87861,     0.87894,     0.87879,     0.87875,     0.87872,     0.87867,     0.87859,     0.88058,     0.88144,     0.88176,     0.88208,     0.88301,     0.88386,     0.88422,      0.8842,     0.88418,     0.88415,     0.88413,     0.88411,     0.88565,\n",
      "            0.88624,     0.88666,     0.88714,     0.88823,     0.88945,     0.88996,     0.88991,     0.88984,     0.88977,     0.89076,     0.89108,      0.8914,     0.89178,     0.89232,     0.89248,     0.89243,     0.89239,      0.8927,     0.89349,     0.89386,     0.89423,      0.8963,     0.89707,\n",
      "            0.89711,     0.89809,     0.89842,     0.89874,     0.89907,     0.89906,     0.89902,     0.89898,     0.89944,     0.89991,     0.89993,     0.89987,     0.89983,     0.89979,     0.89996,      0.9004,     0.90079,     0.90072,     0.90169,     0.90164,       0.902,     0.90225,     0.90224,\n",
      "            0.90242,     0.90261,      0.9028,     0.90298,     0.90317,     0.90432,      0.9046,     0.90488,     0.90516,     0.90505,     0.90498,     0.90495,     0.90493,      0.9049,     0.90488,     0.90484,      0.9048,     0.90477,     0.90495,     0.90516,     0.90538,      0.9056,     0.90581,\n",
      "            0.90606,     0.90656,     0.90778,     0.90772,     0.90853,     0.90866,     0.90863,     0.90861,     0.90858,     0.90886,     0.90956,      0.9099,     0.91024,     0.91058,     0.91081,     0.91153,     0.91148,     0.91142,     0.91134,     0.91132,     0.91129,     0.91126,      0.9111,\n",
      "            0.91097,     0.91196,     0.91194,      0.9122,     0.91246,     0.91272,     0.91397,     0.91396,     0.91394,     0.91392,     0.91391,     0.91389,       0.914,     0.91468,     0.91495,     0.91492,     0.91489,     0.91591,     0.91592,     0.91587,     0.91695,     0.91688,     0.91832,\n",
      "            0.91917,     0.91937,     0.91957,     0.91977,     0.91997,     0.91999,     0.91995,     0.91991,     0.91988,     0.91986,     0.91984,     0.91981,     0.91979,     0.91976,     0.91974,     0.91971,     0.91965,      0.9196,     0.91959,     0.91957,     0.91955,     0.91954,     0.91952,\n",
      "             0.9195,     0.91948,     0.91945,     0.91943,      0.9205,     0.92082,      0.9215,      0.9227,     0.92267,     0.92264,     0.92261,     0.92247,     0.92227,     0.92218,     0.92325,     0.92327,     0.92358,      0.9239,     0.92422,     0.92566,     0.92606,     0.92646,     0.92685,\n",
      "            0.92722,      0.9276,     0.92776,     0.92773,     0.92771,     0.92769,     0.92763,     0.92772,       0.928,     0.92828,     0.92856,     0.92874,     0.92871,     0.92868,     0.92866,     0.92865,     0.92864,     0.92864,     0.92863,     0.92862,     0.92861,     0.92861,      0.9286,\n",
      "            0.92859,     0.92858,     0.92858,     0.92863,     0.92888,     0.92913,     0.92937,     0.92962,     0.93064,     0.93079,     0.93077,     0.93074,     0.93082,     0.93158,     0.93177,     0.93205,     0.93232,      0.9326,     0.93287,     0.93288,     0.93284,      0.9328,     0.93276,\n",
      "            0.93289,     0.93342,     0.93383,      0.9338,     0.93376,     0.93469,     0.93614,     0.93609,     0.93628,     0.93735,     0.93833,     0.93869,     0.93902,     0.93935,     0.93958,     0.93955,     0.93952,     0.93943,     0.93942,      0.9394,     0.93939,     0.93937,     0.93936,\n",
      "            0.93933,     0.93976,     0.94018,     0.94077,     0.94171,     0.94164,      0.9416,     0.94148,     0.94147,     0.94146,     0.94145,     0.94143,     0.94142,     0.94141,     0.94138,     0.94134,     0.94252,     0.94243,     0.94236,     0.94229,     0.94224,     0.94222,      0.9422,\n",
      "            0.94218,     0.94217,     0.94216,     0.94214,     0.94213,     0.94212,     0.94211,     0.94209,     0.94207,     0.94205,     0.94203,     0.94308,     0.94325,     0.94323,     0.94321,     0.94312,     0.94308,     0.94305,     0.94287,     0.94283,     0.94318,     0.94368,     0.94405,\n",
      "            0.94397,     0.94385,     0.94383,     0.94382,      0.9438,     0.94379,     0.94378,      0.9437,     0.94363,      0.9442,     0.94484,     0.94481,     0.94479,     0.94478,     0.94477,     0.94475,     0.94474,     0.94471,     0.94468,     0.94459,     0.94454,      0.9445,     0.94447,\n",
      "            0.94442,     0.94421,      0.9441,     0.94507,     0.94523,     0.94619,      0.9464,     0.94633,     0.94627,     0.94624,     0.94621,     0.94875,     0.94872,     0.94869,     0.94852,     0.94866,     0.94928,     0.94978,      0.9497,     0.94953,     0.94948,     0.94947,     0.94991,\n",
      "            0.95034,     0.95077,     0.95206,       0.952,     0.95196,     0.95193,     0.95185,     0.95309,     0.95309,     0.95306,     0.95293,     0.95404,     0.95411,     0.95407,     0.95404,     0.95519,     0.95671,     0.95667,     0.95693,     0.95803,     0.95785,      0.9578,     0.95911,\n",
      "            0.95894,     0.95892,      0.9589,     0.95883,      0.9601,     0.95995,     0.96126,     0.96123,      0.9612,     0.96316,     0.96398,     0.96399,     0.96384,      0.9638,     0.96367,     0.96354,     0.96347,      0.9634,     0.96338,     0.96336,     0.96323,      0.9632,     0.96301,\n",
      "            0.96289,     0.96285,      0.9628,     0.96273,     0.96271,     0.96269,     0.96261,       0.964,      0.9639,     0.96386,     0.96377,     0.96355,     0.96347,     0.96334,     0.96329,     0.96326,      0.9647,     0.96452,     0.96421,     0.96407,     0.96388,     0.96379,      0.9637,\n",
      "            0.96502,     0.96636,     0.96675,     0.96668,      0.9666,     0.96723,     0.96805,     0.96798,     0.96783,     0.96936,     0.96921,     0.96918,     0.96915,     0.96913,      0.9689,     0.96878,     0.96869,     0.96866,     0.96861,     0.97022,     0.97018,     0.97014,      0.9701,\n",
      "            0.97074,     0.97177,     0.97174,      0.9716,     0.97144,     0.97138,     0.97125,     0.97119,     0.97113,     0.97097,     0.97087,     0.97076,     0.97243,     0.97238,     0.97232,     0.97228,     0.97205,     0.97198,     0.97186,      0.9718,     0.97177,     0.97171,     0.97152,\n",
      "            0.97142,     0.97296,     0.97306,     0.97286,     0.97269,      0.9726,     0.97251,      0.9722,     0.97273,     0.97368,     0.97359,     0.97536,     0.97519,     0.97496,     0.97469,     0.97673,     0.97646,     0.97634,     0.97725,      0.9782,     0.97813,     0.97782,     0.97842,\n",
      "            0.97979,     0.97956,     0.97944,     0.97935,     0.97929,     0.97913,     0.97888,     0.97871,     0.97834,     0.98049,     0.98278,     0.98264,     0.98255,     0.98238,     0.98476,     0.98467,     0.98457,     0.98428,     0.98423,     0.98408,     0.98649,     0.98632,     0.98804,\n",
      "            0.99034,     0.99168,     0.99165,     0.99162,     0.99151,     0.99136,     0.99125,     0.99111,     0.99091,     0.99083,     0.99065,     0.99055,     0.99033,     0.99342,     0.99334,     0.99316,     0.99297,     0.99282,     0.99262,     0.99249,     0.99243,     0.99208,     0.99191,\n",
      "            0.99162,     0.99143,     0.99126,     0.99113,     0.99095,     0.99067,     0.99041,     0.99029,     0.99495,      0.9947,     0.99448,     0.99432,     0.99404,     0.99376,     0.99355,     0.99344,      0.9932,     0.99273,     0.99229,     0.99209,     0.99153,     0.99104,     0.99067,\n",
      "            0.99027,     0.98974,     0.98905,     0.98851,     0.98745,     0.98672,     0.98545,     0.98495,     0.98399,     0.98197,      0.9809,     0.97828,     0.97542,     0.97453,     0.97085,     0.96924,     0.96489,     0.96196,     0.95518,     0.94682,     0.94265,     0.97692,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.93187,     0.93187,     0.92872,     0.92348,     0.92243,     0.92138,     0.91824,     0.91824,     0.91719,     0.91509,     0.91509,     0.91405,     0.91405,     0.91405,     0.91405,     0.91405,       0.913,       0.913,       0.913,       0.913,       0.913,     0.91195,     0.91195,\n",
      "            0.91195,     0.90776,     0.90776,     0.90566,     0.90356,     0.90356,     0.90356,     0.90252,     0.90252,     0.90147,     0.90147,     0.90147,     0.90147,     0.90147,     0.90147,     0.90147,     0.90042,     0.89832,     0.89727,     0.89727,     0.89727,     0.89727,     0.89727,\n",
      "            0.89623,     0.89623,     0.89623,     0.89623,     0.89623,     0.89623,     0.89623,     0.89623,     0.89518,     0.89473,     0.89203,     0.89203,     0.89203,     0.89203,     0.89203,     0.89203,     0.89203,     0.89203,     0.89099,     0.89099,     0.89099,     0.89099,     0.88994,\n",
      "            0.88994,     0.88994,     0.88889,     0.88784,     0.88784,     0.88784,     0.88784,     0.88784,     0.88784,     0.88784,     0.88784,     0.88784,     0.88784,     0.88784,     0.88784,     0.88784,     0.88784,     0.88784,     0.88784,     0.88784,     0.88784,     0.88784,     0.88679,\n",
      "            0.88621,      0.8847,      0.8826,      0.8826,      0.8826,     0.88094,      0.8805,      0.8805,     0.87945,     0.87945,     0.87945,     0.87945,     0.87945,     0.87945,     0.87931,     0.87884,     0.87841,     0.87841,     0.87841,     0.87841,     0.87841,     0.87841,     0.87841,\n",
      "            0.87841,     0.87841,     0.87841,     0.87841,     0.87841,     0.87841,     0.87841,     0.87841,     0.87841,     0.87841,     0.87736,     0.87729,     0.87631,     0.87631,     0.87631,     0.87631,     0.87631,     0.87631,     0.87631,     0.87526,     0.87421,     0.87421,     0.87421,\n",
      "            0.87421,     0.87317,     0.87317,     0.87317,     0.87317,     0.87317,     0.87317,     0.87317,     0.87212,     0.87197,      0.8717,     0.87144,     0.87117,     0.87045,     0.87002,     0.87002,     0.87002,     0.87002,     0.87002,     0.87002,     0.87002,     0.87002,     0.87002,\n",
      "            0.87002,     0.87002,     0.87002,     0.86997,     0.86924,     0.86897,     0.86897,     0.86897,     0.86897,     0.86897,     0.86897,     0.86897,     0.86897,     0.86897,     0.86897,     0.86897,     0.86897,     0.86897,     0.86897,     0.86897,     0.86897,     0.86897,     0.86897,\n",
      "            0.86897,     0.86897,     0.86897,     0.86897,     0.86897,     0.86897,     0.86897,     0.86897,     0.86897,     0.86676,     0.86639,     0.86603,     0.86583,     0.86508,     0.86478,     0.86478,     0.86475,     0.86454,     0.86433,     0.86412,     0.86391,     0.86373,     0.86373,\n",
      "            0.86373,     0.86313,     0.86254,     0.86186,     0.86164,     0.86164,     0.86164,     0.86059,     0.86025,     0.85954,     0.85954,     0.85954,     0.85954,     0.85954,     0.85951,     0.85849,     0.85849,     0.85849,     0.85849,     0.85849,     0.85819,     0.85744,     0.85744,\n",
      "            0.85733,      0.8571,     0.85687,     0.85664,     0.85641,     0.85639,     0.85639,     0.85639,     0.85639,     0.85639,     0.85639,     0.85639,     0.85639,     0.85639,     0.85639,     0.85639,     0.85639,      0.8543,     0.85402,     0.85325,     0.85325,     0.85325,     0.85325,\n",
      "            0.85325,     0.85325,     0.85276,      0.8522,      0.8522,     0.85188,     0.85129,     0.85094,     0.85066,     0.85038,      0.8501,     0.84924,     0.84906,     0.84906,     0.84906,     0.84906,     0.84906,     0.84906,     0.84906,     0.84881,      0.8483,     0.84801,       0.848,\n",
      "            0.84757,     0.84714,     0.84696,     0.84696,     0.84696,     0.84696,     0.84684,     0.84648,     0.84613,     0.84591,     0.84591,     0.84591,     0.84591,     0.84591,     0.84591,     0.84591,     0.84591,     0.84591,     0.84591,     0.84591,     0.84591,     0.84591,      0.8456,\n",
      "            0.84526,     0.84492,     0.84486,     0.84486,     0.84486,     0.84359,     0.84329,     0.84298,      0.8426,     0.84199,     0.84067,     0.84067,     0.84067,     0.84067,     0.84067,     0.84067,     0.84058,     0.84039,     0.84021,     0.84002,     0.83983,     0.83964,     0.83962,\n",
      "            0.83962,     0.83962,     0.83962,     0.83962,     0.83962,     0.83931,     0.83883,     0.83829,     0.83768,     0.83648,     0.83648,     0.83648,     0.83648,     0.83648,     0.83526,     0.83488,      0.8345,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,\n",
      "            0.83168,     0.83124,     0.83124,     0.83124,     0.83124,     0.83091,     0.83055,      0.8302,     0.83019,     0.83019,     0.82956,     0.82898,     0.82864,     0.82829,     0.82809,     0.82809,     0.82803,     0.82733,     0.82682,     0.82637,     0.82495,     0.82244,      0.8218,\n",
      "             0.8218,      0.8218,      0.8218,      0.8218,      0.8218,     0.82075,     0.82075,     0.82075,     0.82075,      0.8193,     0.81864,     0.81839,     0.81814,      0.8179,     0.81765,     0.81731,     0.81696,     0.81661,     0.81656,     0.81656,     0.81656,     0.81656,     0.81654,\n",
      "            0.81551,     0.81551,     0.81518,     0.81456,     0.81447,     0.81335,      0.8131,     0.81285,      0.8126,     0.81237,     0.81132,     0.81132,     0.81132,     0.81132,     0.81027,     0.81002,     0.80955,     0.80894,     0.80814,     0.80786,     0.80758,      0.8073,     0.80575,\n",
      "            0.80438,     0.80344,     0.80189,     0.80189,     0.80189,     0.80189,     0.80185,     0.80168,      0.8015,     0.80133,     0.80116,     0.80098,     0.80084,     0.80084,     0.80066,     0.80035,     0.80004,     0.79979,     0.79927,     0.79875,     0.79857,     0.79783,     0.79665,\n",
      "            0.79665,     0.79665,     0.79665,     0.79665,     0.79665,     0.79544,     0.79503,     0.79462,     0.79434,     0.79408,     0.79382,     0.79357,     0.79331,     0.79305,     0.79278,     0.79252,     0.79188,     0.79134,     0.79116,     0.79098,     0.79081,     0.79063,     0.79045,\n",
      "            0.79024,     0.78997,     0.78971,     0.78945,     0.78931,     0.78826,     0.78826,     0.78821,     0.78789,     0.78758,     0.78726,     0.78573,     0.78349,     0.78253,     0.78182,     0.78092,     0.78092,     0.78092,     0.78092,     0.78092,     0.78092,     0.78092,     0.78092,\n",
      "            0.78092,     0.78092,     0.78077,      0.7805,     0.78023,     0.77996,     0.77929,     0.77883,     0.77883,     0.77883,     0.77883,     0.77871,     0.77834,     0.77798,     0.77774,     0.77765,     0.77757,     0.77748,     0.77739,     0.77731,     0.77722,     0.77714,     0.77705,\n",
      "            0.77697,     0.77688,     0.77679,     0.77673,     0.77673,     0.77673,     0.77673,     0.77673,     0.77568,     0.77539,     0.77507,     0.77476,     0.77358,     0.77358,     0.77254,     0.77254,     0.77254,     0.77254,     0.77254,      0.7721,      0.7716,     0.77112,     0.77065,\n",
      "            0.76939,     0.76939,      0.7693,     0.76883,     0.76836,     0.76834,     0.76831,     0.76767,     0.76625,     0.76625,     0.76625,     0.76625,     0.76625,     0.76625,     0.76614,     0.76574,     0.76534,     0.76414,     0.76394,     0.76375,     0.76355,     0.76335,     0.76315,\n",
      "            0.76205,     0.76205,     0.76205,     0.76205,     0.76203,     0.76108,     0.76048,      0.7589,     0.75874,     0.75857,     0.75841,     0.75824,     0.75807,     0.75791,     0.75747,     0.75691,     0.75633,     0.75508,     0.75405,     0.75302,     0.75243,     0.75211,     0.75179,\n",
      "            0.75152,     0.75137,     0.75121,     0.75106,     0.75091,     0.75075,      0.7506,     0.75038,      0.7501,     0.74981,     0.74953,     0.74948,     0.74922,     0.74892,     0.74863,     0.74734,     0.74685,     0.74636,     0.74393,     0.74335,     0.74319,     0.74319,     0.74289,\n",
      "            0.74174,     0.73999,     0.73979,      0.7396,     0.73941,     0.73921,     0.73902,     0.73796,     0.73701,      0.7369,      0.7369,     0.73571,     0.73552,     0.73533,     0.73514,     0.73495,     0.73472,     0.73429,     0.73385,     0.73265,     0.73199,     0.73142,     0.73093,\n",
      "            0.73028,     0.72735,     0.72587,     0.72537,     0.72327,     0.72327,     0.72179,     0.72076,     0.71994,     0.71952,      0.7191,     0.71799,     0.71752,     0.71705,     0.71457,     0.71384,     0.71384,     0.71367,     0.71248,     0.71001,     0.70924,      0.7086,      0.7086,\n",
      "             0.7086,      0.7086,     0.70785,     0.70679,     0.70619,     0.70574,     0.70456,      0.7044,     0.70287,     0.70234,     0.70028,     0.70021,     0.69747,     0.69675,     0.69628,     0.69602,       0.695,     0.69437,     0.69392,     0.69386,     0.69075,     0.68999,     0.68837,\n",
      "            0.68541,     0.68505,     0.68469,     0.68353,     0.68105,     0.67829,     0.67631,     0.67562,     0.67509,     0.67505,     0.67505,     0.67338,     0.67064,     0.66982,     0.66724,     0.66491,     0.66354,     0.66219,     0.66183,     0.66147,     0.65904,     0.65843,     0.65496,\n",
      "            0.65276,     0.65194,     0.65109,     0.64987,     0.64945,     0.64903,     0.64764,     0.64675,     0.64368,     0.64302,     0.64134,     0.63725,     0.63579,      0.6335,     0.63269,     0.63217,     0.63019,     0.62686,     0.62135,     0.61868,     0.61533,     0.61378,     0.61223,\n",
      "            0.61111,     0.61006,     0.60949,     0.60827,     0.60664,     0.60587,     0.60338,      0.6021,      0.5991,     0.59689,     0.59399,     0.59324,      0.5928,     0.59236,     0.58789,     0.58543,     0.58367,     0.58319,     0.58218,      0.5805,     0.57983,     0.57887,     0.57806,\n",
      "            0.57757,     0.57729,     0.57672,     0.57368,     0.57044,     0.56923,     0.56666,     0.56532,     0.56423,     0.56091,     0.55895,     0.55688,      0.5545,     0.55346,     0.55235,     0.55139,     0.54674,     0.54547,     0.54302,     0.54187,     0.54118,     0.54002,     0.53636,\n",
      "            0.53441,     0.53249,     0.53009,     0.52604,     0.52262,      0.5209,     0.51908,     0.51318,     0.50839,     0.50421,     0.50234,     0.49785,     0.49445,     0.48982,     0.48438,     0.48389,     0.47831,     0.47573,     0.47484,     0.47043,     0.46877,     0.46211,     0.46017,\n",
      "            0.45744,     0.45204,     0.44931,     0.44744,     0.44613,     0.44268,     0.43733,     0.43361,      0.4261,     0.42134,     0.41884,     0.41528,     0.41326,     0.40906,     0.40633,     0.40393,     0.40128,     0.39391,     0.39253,     0.38876,     0.38259,       0.378,     0.37736,\n",
      "            0.37631,     0.37486,     0.37331,     0.37221,     0.36717,     0.36099,     0.35611,     0.35042,      0.3428,     0.33981,     0.33327,     0.32957,     0.32201,     0.31635,     0.31248,      0.3044,     0.29598,      0.2898,     0.28215,     0.27695,     0.27467,     0.26263,     0.25708,\n",
      "            0.24804,     0.24265,     0.23786,     0.23438,     0.22959,     0.22249,      0.2166,     0.21388,      0.2064,     0.19668,     0.18894,     0.18363,     0.17482,     0.16684,     0.16137,      0.1587,     0.15317,     0.14306,     0.13497,     0.13146,     0.12269,     0.11596,     0.11126,\n",
      "            0.10664,     0.10114,     0.09466,    0.090199,    0.082485,     0.07789,    0.071002,      0.0686,    0.064437,    0.057091,    0.053845,    0.047215,    0.041605,    0.040113,    0.034921,    0.033037,    0.028817,    0.026517,    0.022349,    0.018672,    0.017244,    0.014675,    0.012134,\n",
      "            0.00967,   0.0079668,   0.0071278,   0.0066726,   0.0052032,   0.0049588,   0.0047143,   0.0044698,   0.0042253,   0.0035977,   0.0029817,   0.0025022,   0.0020317,   0.0016104,   0.0011891,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,\n",
      "            0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,\n",
      "            0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,\n",
      "            0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,\n",
      "            0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,\n",
      "            0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,\n",
      "            0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,\n",
      "            0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,\n",
      "            0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,     0.99495,\n",
      "            0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,\n",
      "            0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,\n",
      "            0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,\n",
      "            0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,\n",
      "            0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99071,     0.99071,     0.99071,     0.99071,     0.99071,\n",
      "            0.99071,     0.99071,     0.99071,     0.99071,     0.99071,     0.99071,     0.99071,     0.99071,     0.99071,     0.99071,     0.99071,     0.99071,     0.99071,     0.99071,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,\n",
      "            0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,\n",
      "            0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98895,     0.98626,     0.98396,     0.98396,     0.98396,     0.98396,     0.98396,     0.98396,     0.98396,     0.98396,     0.98396,     0.98396,     0.98223,     0.98223,     0.98223,     0.98223,     0.98223,\n",
      "            0.98223,     0.98223,     0.98223,     0.98223,     0.98223,     0.98223,     0.98223,     0.98223,     0.98223,     0.98223,     0.98223,     0.98223,     0.98223,     0.98223,     0.98223,     0.98039,     0.98039,     0.98039,     0.98039,     0.98039,     0.98039,     0.98039,     0.98039,\n",
      "            0.98039,     0.98039,     0.98039,     0.98039,     0.98039,     0.97805,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,\n",
      "            0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97768,     0.97624,\n",
      "            0.97624,     0.97624,     0.97624,     0.97624,     0.97624,     0.97624,     0.97624,     0.97624,     0.97624,     0.97624,     0.97624,     0.97624,     0.97624,     0.97624,     0.97463,     0.97463,     0.97463,     0.97463,     0.97463,     0.97463,     0.97463,     0.97463,     0.97463,\n",
      "            0.97352,     0.97352,     0.97352,     0.97352,     0.97352,     0.97352,     0.97352,     0.97352,     0.97352,     0.97352,     0.97352,     0.97352,     0.97352,     0.97352,     0.97352,     0.97352,     0.97352,     0.97352,     0.97189,     0.97189,     0.97189,     0.97189,     0.97189,\n",
      "            0.97189,     0.97126,     0.97126,     0.97126,     0.97126,     0.97126,     0.97126,     0.97126,     0.97126,     0.97126,     0.97126,     0.97126,     0.97126,     0.97126,     0.97126,     0.97126,     0.97126,     0.97126,     0.97126,     0.97126,     0.97126,     0.97126,     0.97126,\n",
      "            0.97126,     0.97126,     0.97059,     0.97059,     0.97059,     0.97059,     0.97059,     0.97059,     0.97059,     0.97059,     0.97059,     0.97059,     0.97059,     0.97059,     0.97059,     0.97059,     0.97059,     0.97059,     0.97059,     0.97059,     0.97059,     0.97059,     0.97059,\n",
      "            0.97059,     0.96931,     0.96931,     0.96931,     0.96931,     0.96931,     0.96931,     0.96931,     0.96931,     0.96931,     0.96931,     0.96768,     0.96768,     0.96649,     0.96649,     0.96649,     0.96649,     0.96649,     0.96649,     0.96649,     0.96649,     0.96649,     0.96503,\n",
      "            0.96503,     0.96503,     0.96503,     0.96503,     0.96435,     0.96435,     0.96435,     0.96435,     0.96435,     0.96435,     0.96435,     0.96435,     0.96435,     0.96435,     0.96435,     0.96435,     0.96435,     0.96435,     0.96435,     0.96435,     0.96315,     0.96315,     0.96315,\n",
      "            0.96315,     0.96315,     0.96315,     0.96315,     0.96315,     0.96179,     0.96179,     0.96179,     0.96179,     0.96026,        0.96,        0.96,        0.96,        0.96,        0.96,        0.96,        0.96,        0.96,        0.96,        0.96,        0.96,        0.96,        0.96,\n",
      "               0.96,        0.96,        0.96,        0.96,        0.96,        0.96,        0.96,        0.96,     0.95937,     0.95937,     0.95937,     0.95937,     0.95937,     0.95937,     0.95937,     0.95937,     0.95937,     0.95937,     0.95937,     0.95937,     0.95937,     0.95937,     0.95878,\n",
      "            0.95878,     0.95878,     0.95878,     0.95878,     0.95878,     0.95878,     0.95878,     0.95878,     0.95878,     0.95878,     0.95878,     0.95878,     0.95878,     0.95878,     0.95808,     0.95808,     0.95808,     0.95808,     0.95808,     0.95808,     0.95808,     0.95808,     0.95808,\n",
      "            0.95808,     0.95808,     0.95808,     0.95808,     0.95536,     0.95536,     0.95328,     0.95328,     0.95328,     0.95328,     0.95328,     0.95328,     0.95328,     0.95328,     0.95328,     0.95328,     0.95328,     0.95224,     0.95224,     0.95224,     0.95224,     0.95224,     0.95224,\n",
      "            0.95101,     0.95101,     0.94857,     0.94857,     0.94857,     0.94857,     0.94752,     0.94752,     0.94752,     0.94752,     0.94648,     0.94648,     0.94648,     0.94648,     0.94421,     0.94421,     0.94421,     0.94421,     0.94421,     0.94321,     0.94321,     0.94321,     0.94321,\n",
      "            0.94321,     0.94199,     0.94102,     0.94102,     0.94102,     0.94102,     0.93989,     0.93989,     0.93902,     0.93902,     0.93902,     0.93902,     0.93902,     0.93875,     0.93875,     0.93875,     0.93875,     0.93875,     0.93875,     0.93875,     0.93875,     0.93875,     0.93875,\n",
      "            0.93875,     0.93875,     0.93875,     0.93799,     0.93799,     0.93799,     0.93799,     0.93799,     0.93799,     0.93782,     0.93782,     0.93782,     0.93782,     0.93782,     0.93782,     0.93782,     0.93782,     0.93782,     0.93782,     0.93782,     0.93782,     0.93782,     0.93782,\n",
      "            0.93308,      0.9272,     0.92494,     0.92395,     0.92395,     0.92172,     0.92075,     0.92075,      0.9197,     0.91875,     0.91875,     0.91781,     0.91781,     0.91337,     0.91288,     0.91288,     0.91288,     0.91288,     0.91288,     0.91288,     0.91288,     0.91076,     0.91009,\n",
      "            0.91009,     0.91009,     0.91009,      0.9092,      0.9092,     0.90614,     0.90614,     0.90516,     0.90203,         0.9,     0.89858,     0.89858,     0.89858,     0.89858,     0.89858,     0.89858,     0.89765,     0.89671,     0.89671,     0.89591,     0.89591,     0.89498,     0.89419,\n",
      "            0.89419,     0.89364,     0.89364,     0.89364,     0.89364,     0.89171,     0.89143,     0.89143,     0.89143,     0.89143,     0.89143,     0.89143,     0.89054,     0.88977,     0.88977,     0.88889,     0.88713,     0.88713,     0.88713,     0.88255,     0.88255,     0.88255,     0.88071,\n",
      "            0.87987,     0.86842,     0.86696,     0.86696,     0.86696,     0.85684,     0.85423,     0.85256,     0.84974,     0.84974,     0.84974,     0.84974,     0.84974,     0.84205,     0.84205,      0.8361,      0.8361,      0.8354,     0.83213,      0.8272,     0.81736,     0.81672,     0.81363,\n",
      "            0.81219,     0.81095,     0.81095,     0.80315,     0.77958,     0.77905,     0.77119,     0.76564,     0.76301,     0.75137,     0.73548,     0.71927,     0.71927,     0.71826,     0.71788,     0.71788,     0.67813,     0.67784,     0.67425,     0.66533,     0.64624,     0.64624,     0.64551,\n",
      "            0.59714,     0.59714,     0.56592,     0.55607,     0.54765,     0.54759,     0.54646,      0.5349,      0.5349,     0.52651,     0.52413,     0.52413,     0.52413,     0.45567,     0.39251,     0.39251,     0.36888,     0.36888,     0.36236,     0.36236,     0.33742,     0.29937,      0.2887,\n",
      "            0.27556,     0.27178,      0.2676,     0.25346,     0.20485,     0.20075,     0.20046,      0.1965,     0.18949,     0.18826,     0.17763,     0.17511,     0.16347,     0.15633,     0.15633,     0.15421,     0.15312,     0.12661,     0.12367,     0.11194,     0.10553,    0.099186,    0.088915,\n",
      "           0.087804,    0.086692,    0.085581,     0.08447,    0.083358,    0.082247,    0.081135,    0.080024,    0.078912,    0.077801,    0.076689,    0.075578,    0.074467,    0.073355,    0.072244,    0.071132,    0.070021,    0.068909,    0.067798,    0.066686,    0.065575,    0.064464,    0.063352,\n",
      "           0.062241,    0.061129,    0.060018,    0.058906,    0.057795,    0.056684,    0.055572,    0.054461,    0.053349,    0.052238,    0.051126,    0.050015,    0.048903,    0.047792,    0.046681,    0.045569,    0.044458,    0.043346,    0.042235,    0.041123,    0.040012,      0.0389,    0.037789,\n",
      "           0.036678,    0.035566,    0.034455,    0.033343,    0.032232,     0.03112,    0.030009,    0.028897,    0.027786,    0.026675,    0.025563,    0.024452,     0.02334,    0.022229,    0.021117,    0.020006,    0.018895,    0.017783,    0.016672,     0.01556,    0.014449,    0.013337,    0.012226,\n",
      "           0.011114,    0.010003,   0.0088915,   0.0077801,   0.0066686,   0.0055572,   0.0044458,   0.0033343,   0.0022229,   0.0011114,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.16331,     0.16342,     0.26063,     0.32844,     0.37856,     0.41795,     0.44788,     0.47779,     0.50182,     0.52141,     0.53443,     0.55222,     0.56369,     0.57562,     0.58608,     0.59752,     0.60438,     0.61586,     0.62499,     0.63205,     0.63802,     0.64372,     0.65006,\n",
      "            0.65787,     0.66162,     0.66581,     0.66915,     0.67422,     0.67546,     0.68023,     0.68403,     0.68747,     0.69009,     0.69398,     0.69689,     0.69914,     0.70227,      0.7061,     0.70959,     0.70956,     0.71415,     0.71627,     0.71787,      0.7212,     0.72315,     0.72663,\n",
      "             0.7281,        0.73,     0.73142,     0.73286,     0.73436,     0.73562,     0.73774,     0.74103,     0.74192,     0.74242,     0.74181,     0.74346,     0.74426,     0.74625,     0.74817,     0.74986,     0.75104,     0.75283,     0.75435,     0.75621,       0.757,     0.75818,     0.75927,\n",
      "             0.7607,     0.76123,     0.76132,      0.7623,      0.7641,     0.76638,      0.7669,     0.76764,     0.76907,     0.76972,     0.76999,     0.77126,     0.77292,     0.77525,     0.77599,     0.77667,     0.77777,     0.77996,     0.78057,     0.78142,     0.78225,     0.78293,     0.78502,\n",
      "            0.78505,     0.78485,     0.78443,      0.7854,      0.7861,      0.7874,      0.7885,     0.78898,     0.79071,     0.79255,     0.79316,     0.79373,     0.79384,     0.79394,     0.79394,     0.79368,      0.7935,     0.79427,     0.79447,     0.79475,     0.79557,     0.79634,     0.79703,\n",
      "            0.79761,     0.79844,     0.79874,     0.80039,      0.8006,     0.80082,     0.80116,     0.80142,     0.80205,     0.80249,     0.80245,     0.80414,     0.80459,     0.80539,     0.80696,     0.80741,     0.80782,     0.80803,      0.8087,     0.80897,     0.80961,     0.81068,     0.81124,\n",
      "             0.8125,     0.81204,     0.81237,     0.81261,      0.8138,     0.81454,      0.8152,     0.81546,     0.81615,     0.81628,     0.81613,     0.81598,     0.81584,     0.81583,     0.81614,     0.81653,      0.8168,     0.81704,     0.81765,     0.81796,      0.8186,     0.81956,     0.82031,\n",
      "            0.82106,     0.82223,     0.82251,     0.82343,     0.82372,     0.82396,     0.82417,     0.82433,     0.82446,     0.82459,     0.82537,     0.82576,     0.82607,     0.82638,     0.82682,     0.82697,     0.82712,     0.82756,     0.82788,     0.82831,     0.82836,     0.82853,     0.82874,\n",
      "            0.82916,     0.82994,     0.83012,     0.83029,     0.83086,     0.83103,      0.8312,     0.83145,      0.8318,     0.83091,     0.83106,     0.83121,     0.83144,     0.83129,     0.83167,     0.83183,     0.83195,     0.83183,     0.83171,     0.83159,     0.83147,     0.83162,     0.83218,\n",
      "            0.83307,     0.83273,     0.83283,     0.83244,     0.83303,     0.83492,     0.83597,     0.83687,     0.83728,     0.83743,     0.83764,     0.83788,     0.83813,     0.83914,     0.83946,     0.83938,     0.83955,     0.83973,     0.84018,     0.84045,     0.84045,     0.84006,     0.84032,\n",
      "             0.8404,     0.84026,     0.84013,     0.83999,     0.83986,     0.84035,     0.84042,     0.84049,     0.84057,     0.84064,     0.84071,     0.84082,     0.84095,     0.84108,     0.84126,     0.84156,     0.84283,     0.84193,     0.84228,      0.8426,     0.84289,     0.84326,     0.84377,\n",
      "            0.84457,     0.84511,     0.84542,     0.84503,     0.84466,     0.84431,     0.84396,     0.84376,     0.84359,     0.84343,     0.84327,     0.84277,     0.84329,     0.84353,     0.84369,     0.84384,       0.844,     0.84417,     0.84435,      0.8443,       0.844,     0.84398,     0.84472,\n",
      "            0.84447,     0.84422,     0.84469,     0.84492,     0.84513,     0.84533,     0.84539,     0.84518,     0.84497,     0.84496,     0.84526,      0.8454,     0.84551,     0.84563,     0.84574,     0.84716,     0.84751,     0.84809,     0.84852,     0.84864,     0.84877,      0.8489,     0.84904,\n",
      "            0.84919,     0.84933,     0.84949,     0.84965,     0.84981,     0.84906,     0.84889,     0.84871,     0.84848,     0.84812,     0.84837,     0.84877,     0.84892,     0.84907,     0.84951,     0.84991,     0.85012,      0.8502,     0.85028,     0.85037,     0.85045,     0.85053,     0.85126,\n",
      "            0.85154,     0.85173,     0.85196,     0.85247,     0.85303,     0.85343,     0.85364,     0.85358,     0.85322,     0.85304,     0.85318,     0.85333,     0.85351,     0.85376,     0.85317,     0.85295,     0.85272,     0.85224,      0.8526,     0.85277,     0.85294,     0.85389,     0.85425,\n",
      "            0.85337,     0.85357,     0.85372,     0.85387,     0.85402,     0.85384,     0.85362,     0.85341,     0.85361,     0.85383,      0.8535,     0.85315,     0.85294,     0.85274,     0.85271,      0.8529,     0.85305,     0.85263,     0.85279,     0.85252,     0.85136,     0.84953,     0.84917,\n",
      "            0.84926,     0.84934,     0.84942,     0.84951,     0.84959,     0.84952,     0.84965,     0.84977,      0.8499,     0.84904,     0.84864,     0.84849,     0.84834,     0.84819,     0.84804,     0.84784,     0.84763,     0.84741,     0.84747,     0.84756,     0.84766,     0.84775,     0.84784,\n",
      "              0.847,      0.8467,     0.84685,     0.84647,     0.84677,      0.8462,     0.84604,     0.84589,     0.84574,     0.84573,     0.84545,      0.8456,     0.84575,     0.84589,      0.8454,     0.84557,     0.84528,     0.84491,     0.84442,     0.84425,     0.84408,     0.84391,     0.84296,\n",
      "            0.84211,     0.84258,     0.84222,     0.84234,     0.84245,     0.84256,     0.84308,     0.84297,     0.84287,     0.84276,     0.84265,     0.84255,     0.84251,      0.8428,     0.84301,     0.84315,     0.84329,     0.84385,     0.84355,     0.84323,     0.84359,     0.84313,     0.84306,\n",
      "            0.84342,     0.84351,      0.8436,     0.84368,     0.84377,     0.84307,     0.84282,     0.84256,     0.84262,     0.84273,     0.84285,     0.84296,     0.84287,     0.84271,     0.84255,     0.84239,     0.84199,     0.84165,     0.84154,     0.84143,     0.84132,     0.84121,      0.8411,\n",
      "            0.84096,      0.8408,     0.84063,     0.84047,     0.84084,     0.84036,     0.84065,     0.84112,     0.84092,     0.84073,     0.84053,     0.83957,     0.83816,     0.83756,     0.83759,     0.83706,      0.8372,     0.83733,     0.83746,     0.83806,     0.83822,     0.83839,     0.83855,\n",
      "             0.8387,     0.83886,     0.83884,     0.83866,     0.83849,     0.83832,      0.8379,     0.83766,     0.83777,     0.83789,     0.83801,     0.83801,     0.83778,     0.83755,      0.8374,     0.83734,     0.83729,     0.83723,     0.83718,     0.83712,     0.83707,     0.83702,     0.83696,\n",
      "            0.83691,     0.83685,      0.8368,     0.83678,     0.83688,     0.83699,     0.83709,     0.83719,     0.83698,     0.83686,     0.83666,     0.83647,     0.83579,      0.8361,     0.83555,     0.83566,     0.83577,     0.83588,     0.83599,     0.83621,     0.83644,     0.83626,     0.83596,\n",
      "            0.83525,     0.83546,     0.83557,     0.83527,     0.83497,     0.83534,      0.8359,     0.83549,     0.83586,     0.83629,     0.83668,     0.83682,     0.83695,     0.83709,     0.83723,     0.83742,      0.8376,       0.837,     0.83709,     0.83718,     0.83727,     0.83736,     0.83745,\n",
      "            0.83798,     0.83816,     0.83833,     0.83856,     0.83892,     0.83831,     0.83793,     0.83691,      0.8368,     0.83669,     0.83659,     0.83648,     0.83637,     0.83627,     0.83598,     0.83562,     0.83457,     0.83376,      0.8331,     0.83243,     0.83205,     0.83184,     0.83163,\n",
      "            0.83145,     0.83135,     0.83125,     0.83115,     0.83105,     0.83095,     0.83085,     0.83071,     0.83053,     0.83034,     0.83016,     0.83053,     0.83044,     0.83025,     0.83005,     0.82921,     0.82889,     0.82857,     0.82698,      0.8266,     0.82663,     0.82683,     0.82678,\n",
      "            0.82603,     0.82488,     0.82475,     0.82462,     0.82449,     0.82437,     0.82424,     0.82354,     0.82291,     0.82252,     0.82217,     0.82136,     0.82123,     0.82111,     0.82098,     0.82086,      0.8207,     0.82041,     0.82013,     0.81932,     0.81889,     0.81877,       0.819,\n",
      "            0.81893,     0.81698,     0.81599,     0.81603,     0.81473,     0.81509,     0.81422,     0.81352,     0.81297,     0.81269,     0.81241,     0.81262,      0.8123,     0.81199,     0.81031,     0.80988,     0.81011,     0.81018,     0.80937,     0.80769,     0.80716,     0.80674,     0.80689,\n",
      "            0.80705,     0.80721,     0.80718,     0.80645,     0.80604,     0.80574,     0.80493,     0.80527,     0.80425,     0.80389,     0.80247,     0.80282,     0.80101,     0.80051,     0.80018,     0.80041,     0.80027,     0.79983,     0.79961,     0.79996,     0.79779,     0.79726,     0.79661,\n",
      "            0.79453,     0.79428,     0.79403,     0.79321,     0.79072,     0.78876,     0.78907,     0.78858,     0.78821,     0.78884,     0.78912,     0.78796,       0.786,     0.78541,     0.78356,     0.78188,     0.78089,     0.78025,     0.78042,     0.78059,     0.77888,     0.77844,     0.77591,\n",
      "            0.77431,     0.77371,     0.77309,      0.7722,     0.77189,     0.77158,     0.77056,     0.77037,     0.76813,     0.76763,      0.7664,     0.76336,     0.76227,     0.76056,     0.75996,     0.75956,     0.75856,     0.75605,     0.75189,     0.74986,      0.7473,     0.74612,     0.74493,\n",
      "            0.74448,     0.74409,     0.74377,     0.74283,     0.74158,     0.74118,     0.73952,     0.73853,      0.7362,     0.73494,     0.73267,     0.73208,     0.73173,     0.73138,     0.72786,     0.72591,     0.72451,     0.72412,     0.72332,     0.72245,     0.72192,     0.72115,      0.7205,\n",
      "            0.72029,     0.72035,      0.7199,     0.71745,     0.71483,     0.71385,     0.71309,       0.712,     0.71207,     0.70972,     0.70812,     0.70642,     0.70494,     0.70407,     0.70316,     0.70237,      0.6985,     0.69744,     0.69539,     0.69443,     0.69385,     0.69288,     0.68979,\n",
      "            0.68815,     0.68694,     0.68495,     0.68149,     0.67855,     0.67707,      0.6755,     0.67039,     0.66639,       0.663,     0.66135,     0.65783,      0.6548,     0.65065,     0.64575,     0.64576,     0.64069,     0.63833,     0.63772,     0.63391,     0.63238,     0.62619,     0.62452,\n",
      "            0.62226,     0.61717,     0.61459,     0.61281,     0.61156,     0.60826,     0.60312,     0.59951,     0.59218,     0.58793,     0.58589,     0.58235,     0.58034,     0.57613,      0.5738,     0.57137,     0.56869,     0.56115,     0.55973,     0.55584,     0.54984,     0.54502,     0.54462,\n",
      "            0.54387,     0.54254,      0.5409,     0.53973,     0.53436,     0.52772,     0.52244,     0.51622,     0.50783,      0.5045,     0.49875,     0.49459,     0.48599,     0.47989,     0.47541,     0.46598,     0.45602,     0.44864,     0.43941,     0.43306,     0.43026,     0.41532,     0.40832,\n",
      "            0.39682,     0.38988,     0.38365,     0.37911,     0.37281,     0.36337,     0.35546,     0.35178,     0.34188,     0.32842,     0.31755,     0.31001,     0.29734,     0.28571,     0.27764,     0.27368,      0.2654,     0.25008,     0.23762,     0.23216,     0.21836,     0.20763,     0.20005,\n",
      "            0.19254,     0.18353,     0.17278,     0.16531,     0.15225,     0.14438,     0.13246,     0.12827,     0.12095,     0.10791,     0.10209,    0.090083,    0.079806,    0.077055,    0.067418,    0.063896,    0.055963,    0.051611,    0.043675,    0.036622,    0.033868,    0.028916,    0.023978,\n",
      "           0.019155,    0.015808,    0.014155,    0.013257,    0.010353,   0.0098686,   0.0093843,   0.0088998,   0.0084151,   0.0071697,   0.0059457,    0.004992,   0.0040552,   0.0032157,   0.0023755,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.089618,    0.089681,     0.15199,     0.20071,     0.23959,     0.27233,     0.29854,     0.32586,     0.34879,     0.36836,     0.38188,     0.40052,     0.41271,     0.42562,     0.43716,     0.45001,     0.45811,     0.47144,     0.48223,     0.49068,     0.49791,     0.50489,     0.51274,\n",
      "            0.52252,     0.52876,     0.53414,     0.53922,     0.54625,     0.54869,     0.55502,     0.56052,     0.56515,     0.56914,     0.57445,     0.57845,     0.58156,      0.5859,     0.59126,     0.59617,      0.5971,     0.60363,     0.60667,     0.60897,     0.61377,      0.6166,     0.62167,\n",
      "            0.62383,     0.62663,     0.62872,     0.63086,     0.63308,     0.63496,     0.63813,     0.64306,      0.6444,     0.64539,     0.64596,     0.64846,     0.64969,     0.65272,     0.65567,     0.65826,     0.66008,     0.66285,     0.66582,     0.66873,     0.66996,     0.67181,     0.67416,\n",
      "            0.67642,     0.67725,     0.67804,     0.68024,     0.68311,     0.68677,     0.68761,      0.6888,      0.6911,     0.69215,     0.69258,     0.69464,     0.69735,     0.70114,     0.70236,     0.70346,     0.70528,     0.70889,      0.7099,     0.71129,     0.71267,     0.71381,     0.71729,\n",
      "            0.71775,     0.71845,      0.7192,     0.72082,     0.72201,     0.72421,     0.72606,     0.72687,     0.72981,     0.73297,       0.734,     0.73498,     0.73517,     0.73535,     0.73545,     0.73534,     0.73535,     0.73668,     0.73702,      0.7375,     0.73892,     0.74024,     0.74144,\n",
      "            0.74245,     0.74389,     0.74441,     0.74728,     0.74763,     0.74802,     0.74862,     0.74908,     0.75017,     0.75094,     0.75167,     0.75464,     0.75544,     0.75684,     0.75962,     0.76042,     0.76114,     0.76152,     0.76272,     0.76402,     0.76599,     0.76791,     0.76892,\n",
      "            0.77118,      0.7712,     0.77179,     0.77224,     0.77438,     0.77572,     0.77693,     0.77739,     0.77865,     0.77902,     0.77896,     0.77891,     0.77886,     0.77946,     0.78036,     0.78109,     0.78158,     0.78202,     0.78314,     0.78371,     0.78489,     0.78664,     0.78803,\n",
      "            0.78943,     0.79158,      0.7921,     0.79381,     0.79435,      0.7948,      0.7952,     0.79549,     0.79573,     0.79597,     0.79742,     0.79815,     0.79873,     0.79932,     0.80013,     0.80042,      0.8007,     0.80153,     0.80212,     0.80311,     0.80396,     0.80428,     0.80467,\n",
      "            0.80546,     0.80694,     0.80727,      0.8076,     0.80867,     0.80899,     0.80932,     0.80979,     0.81046,     0.81066,     0.81094,     0.81122,     0.81166,     0.81205,     0.81306,     0.81337,     0.81362,     0.81358,     0.81355,     0.81351,     0.81347,     0.81392,       0.815,\n",
      "            0.81672,     0.81661,     0.81733,     0.81721,     0.81857,     0.82223,     0.82427,     0.82602,     0.82714,      0.8281,     0.82853,     0.82899,     0.82947,     0.83145,     0.83213,     0.83295,     0.83329,     0.83364,     0.83452,     0.83505,     0.83536,     0.83532,     0.83583,\n",
      "            0.83608,     0.83604,     0.83601,     0.83597,     0.83593,     0.83691,     0.83705,      0.8372,     0.83734,     0.83748,     0.83763,     0.83785,     0.83811,     0.83836,     0.83873,     0.83932,     0.84185,     0.84213,     0.84284,     0.84349,     0.84407,     0.84481,     0.84584,\n",
      "            0.84744,     0.84853,     0.84966,     0.84956,     0.84946,     0.84937,     0.84928,     0.84922,     0.84918,     0.84914,      0.8491,     0.84896,     0.85022,     0.85071,     0.85103,     0.85134,     0.85165,     0.85201,     0.85237,     0.85253,     0.85245,     0.85272,     0.85423,\n",
      "            0.85417,      0.8541,     0.85526,     0.85573,     0.85616,     0.85656,     0.85682,     0.85677,     0.85671,     0.85692,     0.85753,     0.85781,     0.85805,     0.85829,     0.85853,     0.86145,     0.86219,     0.86339,     0.86427,     0.86453,     0.86479,     0.86505,     0.86535,\n",
      "            0.86566,     0.86596,     0.86629,     0.86662,     0.86695,     0.86678,     0.86674,      0.8667,     0.86664,     0.86656,      0.8685,     0.86935,     0.86967,     0.86998,      0.8709,     0.87174,     0.87219,     0.87236,     0.87253,     0.87271,     0.87288,     0.87306,      0.8746,\n",
      "            0.87518,     0.87559,     0.87606,     0.87714,     0.87834,     0.87918,     0.87963,     0.87983,     0.87975,     0.88071,     0.88103,     0.88134,     0.88172,     0.88226,      0.8824,     0.88235,      0.8823,      0.8826,     0.88337,     0.88374,     0.88411,     0.88615,     0.88692,\n",
      "            0.88693,      0.8879,     0.88822,     0.88854,     0.88886,     0.88885,     0.88881,     0.88876,     0.88922,     0.88968,      0.8897,     0.88963,     0.88959,     0.88955,     0.88971,     0.89014,     0.89053,     0.89044,      0.8914,     0.89135,     0.89112,     0.89076,     0.89073,\n",
      "            0.89091,      0.8911,     0.89128,     0.89147,     0.89165,     0.89277,     0.89305,     0.89332,      0.8936,     0.89347,     0.89339,     0.89336,     0.89333,     0.89331,     0.89328,     0.89324,     0.89319,     0.89315,     0.89333,     0.89355,     0.89376,     0.89397,     0.89418,\n",
      "            0.89402,     0.89396,     0.89494,     0.89487,     0.89567,     0.89578,     0.89575,     0.89572,     0.89569,     0.89596,     0.89664,     0.89697,      0.8973,     0.89764,     0.89785,     0.89856,      0.8985,     0.89843,     0.89834,     0.89831,     0.89828,     0.89824,     0.89807,\n",
      "            0.89791,     0.89948,     0.90001,     0.90027,     0.90053,     0.90079,     0.90203,     0.90201,     0.90199,     0.90197,     0.90195,     0.90193,     0.90203,     0.90271,     0.90318,      0.9035,     0.90382,     0.90511,     0.90511,     0.90505,     0.90612,     0.90604,     0.90744,\n",
      "            0.90828,     0.90848,     0.90868,     0.90888,     0.90908,     0.90907,     0.90903,     0.90899,      0.9092,     0.90947,     0.90974,     0.91002,     0.91006,     0.91004,     0.91001,     0.90998,     0.90992,     0.90986,     0.90984,     0.90982,      0.9098,     0.90978,     0.90977,\n",
      "            0.90974,     0.90972,     0.90969,     0.90966,     0.91072,     0.91103,      0.9117,     0.91288,     0.91285,     0.91281,     0.91278,     0.91262,     0.91239,     0.91229,     0.91335,     0.91335,     0.91367,     0.91398,     0.91429,     0.91572,     0.91612,     0.91651,     0.91689,\n",
      "            0.91727,     0.91764,     0.91779,     0.91777,     0.91774,     0.91771,     0.91765,     0.91773,     0.91801,     0.91828,     0.91856,     0.91874,      0.9187,     0.91867,     0.91864,     0.91864,     0.91863,     0.91862,     0.91861,      0.9186,     0.91859,     0.91859,     0.91858,\n",
      "            0.91857,     0.91856,     0.91855,     0.91861,     0.91885,     0.91909,     0.91934,     0.91958,     0.92058,     0.92073,      0.9207,     0.92067,     0.92073,     0.92148,     0.92166,     0.92193,      0.9222,     0.92247,     0.92275,     0.92327,     0.92383,     0.92392,     0.92388,\n",
      "            0.92399,     0.92452,     0.92493,     0.92489,     0.92484,     0.92576,      0.9272,     0.92714,     0.92859,     0.92966,     0.93063,     0.93099,     0.93131,     0.93164,       0.932,     0.93245,     0.93291,       0.933,     0.93323,     0.93345,     0.93368,     0.93391,     0.93414,\n",
      "            0.93545,     0.93588,     0.93631,     0.93688,     0.93782,     0.93775,      0.9377,     0.93758,     0.93757,     0.93756,     0.93754,     0.93753,     0.93752,      0.9375,     0.93747,     0.93743,      0.9373,      0.9372,     0.93712,     0.93704,     0.93699,     0.93697,     0.93694,\n",
      "            0.93692,     0.93691,      0.9369,     0.93688,     0.93687,     0.93686,     0.93685,     0.93683,     0.93681,     0.93679,     0.93676,      0.9378,     0.93797,     0.93795,     0.93793,     0.93783,     0.93779,     0.93775,     0.93756,     0.93751,     0.93786,     0.93836,     0.93872,\n",
      "            0.93864,      0.9385,     0.93848,     0.93847,     0.93845,     0.93844,     0.93842,     0.93834,     0.93826,     0.93822,     0.93818,     0.93808,     0.93806,     0.93805,     0.93803,     0.93802,       0.938,     0.93796,     0.93793,     0.93783,     0.93778,     0.93804,     0.93863,\n",
      "              0.939,     0.93877,     0.93865,     0.93961,     0.93975,     0.94071,      0.9409,     0.94082,     0.94076,     0.94072,     0.94069,     0.94321,     0.94318,     0.94314,     0.94295,     0.94309,      0.9437,      0.9442,     0.94411,     0.94393,     0.94387,     0.94386,     0.94429,\n",
      "            0.94472,     0.94515,     0.94643,     0.94635,      0.9463,     0.94627,     0.94619,     0.94741,     0.94741,     0.94737,     0.94722,     0.94833,     0.94838,     0.94833,     0.94829,     0.94944,     0.95094,      0.9509,     0.95115,     0.95224,     0.95203,     0.95198,     0.95326,\n",
      "            0.95307,     0.95305,     0.95302,     0.95295,     0.95271,     0.95253,      0.9553,     0.95526,     0.95523,     0.95717,     0.95799,     0.95798,     0.95782,     0.95777,     0.95761,     0.95747,     0.95738,     0.95771,     0.95821,     0.95871,     0.95863,      0.9586,     0.95839,\n",
      "            0.95825,      0.9582,     0.95815,     0.95807,     0.95805,     0.95802,     0.95793,     0.95932,     0.95919,     0.95915,     0.95904,     0.95879,      0.9587,     0.95856,     0.95851,     0.95847,     0.95988,     0.95968,     0.95933,     0.95917,     0.95895,     0.95885,     0.95875,\n",
      "            0.96005,     0.96137,     0.96176,     0.96169,     0.96159,     0.96221,       0.963,     0.96293,     0.96275,     0.96425,     0.96408,     0.96404,     0.96401,     0.96399,     0.96372,     0.96357,     0.96347,     0.96344,     0.96338,     0.96496,     0.96492,     0.96487,     0.96482,\n",
      "            0.96546,     0.96647,     0.96644,     0.96627,     0.96608,     0.96601,     0.96766,     0.96759,     0.96883,     0.96915,     0.96905,     0.96894,     0.97059,     0.97053,     0.97048,     0.97043,     0.97018,     0.97011,     0.96998,     0.96992,     0.96988,     0.96982,     0.96962,\n",
      "            0.96951,     0.97105,     0.97114,     0.97092,     0.97074,     0.97064,     0.97054,     0.97021,     0.97073,     0.97166,     0.97156,      0.9733,     0.97312,     0.97288,     0.97258,     0.97461,     0.97432,     0.97419,     0.97509,     0.97602,     0.97594,      0.9756,     0.97619,\n",
      "            0.97755,     0.97729,     0.97715,     0.97706,     0.97699,     0.97682,     0.97654,     0.97634,     0.97593,     0.97805,     0.98032,     0.98016,     0.98006,     0.97986,     0.98222,     0.98211,       0.982,     0.98167,      0.9816,     0.98143,     0.98378,     0.98359,     0.98529,\n",
      "            0.98758,     0.98891,     0.98886,     0.98883,     0.98868,     0.98849,     0.98833,     0.98814,     0.98788,     0.98777,     0.99065,     0.99055,     0.99033,     0.99342,     0.99334,     0.99316,     0.99297,     0.99282,     0.99262,     0.99249,     0.99243,     0.99208,     0.99191,\n",
      "            0.99162,     0.99143,     0.99126,     0.99113,     0.99095,     0.99067,     0.99041,     0.99029,     0.99495,      0.9947,     0.99448,     0.99432,     0.99404,     0.99376,     0.99355,     0.99344,      0.9932,     0.99273,     0.99229,     0.99209,     0.99153,     0.99104,     0.99067,\n",
      "            0.99027,     0.98974,     0.98905,     0.98851,     0.98745,     0.98672,     0.98545,     0.98495,     0.98399,     0.98197,      0.9809,     0.97828,     0.97542,     0.97453,     0.97085,     0.96924,     0.96489,     0.96196,     0.95518,     0.94682,     0.94265,     0.97692,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.91929,     0.91929,     0.91405,     0.90334,     0.90147,     0.89832,     0.89623,     0.89518,     0.89413,     0.89203,     0.88994,     0.88889,     0.88889,     0.88889,     0.88889,     0.88889,     0.88784,     0.88784,     0.88784,     0.88784,     0.88784,     0.88784,     0.88784,\n",
      "            0.88784,     0.88365,     0.88365,     0.88155,      0.8805,     0.87841,     0.87841,     0.87736,     0.87736,     0.87631,     0.87631,     0.87631,     0.87631,     0.87631,     0.87631,     0.87631,     0.87421,     0.87421,     0.87421,     0.87421,     0.87421,     0.87421,     0.87421,\n",
      "            0.87421,     0.87421,     0.87421,     0.87421,     0.87421,     0.87421,     0.87421,     0.87421,     0.87421,     0.87377,     0.87107,     0.87107,     0.87107,     0.87107,     0.87107,     0.87107,     0.87107,     0.87107,     0.87002,     0.87002,     0.87002,     0.87002,     0.86897,\n",
      "            0.86897,     0.86897,     0.86792,     0.86688,     0.86688,     0.86688,     0.86688,     0.86688,     0.86688,     0.86688,     0.86688,     0.86688,     0.86688,     0.86688,     0.86688,     0.86688,     0.86688,     0.86688,     0.86688,     0.86688,     0.86688,     0.86688,     0.86688,\n",
      "            0.86629,     0.86478,     0.86268,     0.86268,     0.86268,     0.86268,     0.86268,     0.86268,     0.86268,     0.86268,     0.86268,     0.86268,     0.86268,     0.86268,     0.86254,     0.86207,     0.86164,     0.86164,     0.86164,     0.86164,     0.86164,     0.86164,     0.86164,\n",
      "            0.86164,     0.86164,     0.86164,     0.86164,     0.86164,     0.86164,     0.86164,     0.86164,     0.86164,     0.86164,     0.86059,     0.86059,     0.86059,     0.86059,     0.86059,     0.86059,     0.86059,     0.86059,     0.86059,     0.85954,     0.85849,     0.85849,     0.85849,\n",
      "            0.85849,     0.85744,     0.85744,     0.85744,     0.85744,     0.85744,     0.85744,     0.85744,     0.85744,     0.85729,     0.85703,     0.85676,      0.8565,     0.85577,     0.85535,     0.85535,     0.85535,     0.85535,     0.85535,     0.85535,     0.85535,     0.85535,     0.85535,\n",
      "            0.85535,     0.85535,     0.85535,     0.85535,     0.85535,     0.85535,     0.85535,     0.85535,     0.85535,     0.85535,     0.85535,     0.85535,     0.85535,     0.85535,     0.85535,     0.85535,     0.85535,     0.85535,     0.85535,     0.85514,      0.8543,      0.8543,      0.8543,\n",
      "             0.8543,      0.8543,      0.8543,      0.8543,      0.8543,      0.8543,      0.8543,      0.8543,      0.8543,      0.8522,      0.8522,      0.8522,      0.8522,     0.85145,     0.85115,     0.85115,     0.85112,     0.85091,      0.8507,     0.85049,     0.85028,      0.8501,      0.8501,\n",
      "             0.8501,      0.8495,     0.84892,     0.84824,     0.84801,     0.84801,     0.84801,     0.84801,     0.84767,     0.84696,     0.84696,     0.84696,     0.84696,     0.84696,     0.84693,     0.84591,     0.84591,     0.84591,     0.84591,     0.84591,     0.84561,     0.84486,     0.84486,\n",
      "            0.84476,     0.84452,     0.84429,     0.84406,     0.84383,     0.84382,     0.84382,     0.84382,     0.84382,     0.84382,     0.84382,     0.84382,     0.84382,     0.84382,     0.84382,     0.84382,     0.84382,     0.84172,     0.84172,     0.84172,     0.84172,     0.84172,     0.84172,\n",
      "            0.84172,     0.84172,     0.84123,     0.84055,     0.83991,      0.8393,     0.83871,     0.83836,     0.83808,      0.8378,     0.83753,     0.83666,     0.83648,     0.83648,     0.83648,     0.83648,     0.83648,     0.83648,     0.83648,     0.83624,     0.83572,     0.83543,     0.83542,\n",
      "            0.83499,     0.83456,     0.83438,     0.83438,     0.83438,     0.83438,     0.83426,     0.83391,     0.83355,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83333,\n",
      "            0.83333,     0.83333,     0.83333,     0.83333,     0.83333,     0.83206,     0.83176,     0.83145,     0.83107,     0.83046,     0.82914,     0.82914,     0.82914,     0.82914,     0.82914,     0.82914,     0.82914,     0.82914,     0.82914,     0.82914,     0.82914,     0.82914,     0.82914,\n",
      "            0.82914,     0.82914,     0.82914,     0.82914,     0.82914,     0.82914,     0.82914,     0.82885,     0.82825,     0.82704,     0.82704,     0.82704,     0.82704,     0.82704,     0.82582,     0.82544,     0.82506,      0.8239,      0.8239,      0.8239,      0.8239,      0.8239,      0.8239,\n",
      "            0.82225,      0.8218,      0.8218,      0.8218,      0.8218,     0.82148,     0.82112,     0.82076,     0.82075,     0.82075,     0.82013,     0.81955,      0.8192,     0.81885,     0.81866,     0.81866,      0.8186,     0.81789,     0.81739,     0.81694,       0.815,     0.81196,     0.81132,\n",
      "            0.81132,     0.81132,     0.81132,     0.81132,     0.81132,     0.81027,     0.81027,     0.81027,     0.81027,     0.80881,     0.80816,     0.80791,     0.80766,     0.80741,     0.80716,     0.80683,     0.80648,     0.80613,     0.80608,     0.80608,     0.80608,     0.80608,     0.80606,\n",
      "            0.80468,     0.80418,     0.80365,     0.80303,     0.80294,     0.80182,     0.80157,     0.80132,     0.80107,     0.80084,     0.79979,     0.79979,     0.79979,     0.79979,     0.79874,     0.79849,     0.79802,     0.79741,      0.7966,     0.79633,     0.79605,     0.79577,     0.79422,\n",
      "            0.79285,     0.79245,      0.7914,      0.7914,      0.7914,      0.7914,     0.79137,     0.79119,     0.79102,     0.79085,     0.79067,      0.7905,     0.79036,     0.79036,     0.79036,     0.79036,     0.79036,     0.79036,     0.78984,     0.78932,     0.78913,      0.7884,     0.78721,\n",
      "            0.78721,     0.78721,     0.78721,     0.78721,     0.78721,     0.78601,      0.7856,     0.78519,     0.78512,     0.78512,     0.78512,     0.78512,     0.78492,     0.78466,      0.7844,     0.78414,      0.7835,     0.78295,     0.78278,      0.7826,     0.78242,     0.78224,     0.78207,\n",
      "            0.78185,     0.78159,     0.78132,     0.78106,     0.78092,     0.77987,     0.77987,     0.77983,     0.77951,     0.77919,     0.77887,     0.77735,      0.7751,     0.77414,     0.77343,     0.77254,     0.77254,     0.77254,     0.77254,     0.77254,     0.77254,     0.77254,     0.77254,\n",
      "            0.77254,     0.77254,     0.77239,     0.77212,     0.77184,     0.77157,      0.7709,     0.77044,     0.77044,     0.77044,     0.77044,     0.77032,     0.76996,     0.76959,     0.76935,     0.76927,     0.76918,     0.76909,     0.76901,     0.76892,     0.76884,     0.76875,     0.76867,\n",
      "            0.76858,     0.76849,     0.76841,     0.76834,     0.76834,     0.76834,     0.76834,     0.76834,      0.7673,       0.767,     0.76669,     0.76638,      0.7652,      0.7652,     0.76415,     0.76415,     0.76415,     0.76415,     0.76415,     0.76415,     0.76415,     0.76378,     0.76332,\n",
      "            0.76205,     0.76205,     0.76196,     0.76149,     0.76103,     0.76101,     0.76097,     0.76033,     0.75996,     0.75996,     0.75996,     0.75996,     0.75996,     0.75996,     0.75996,     0.75996,     0.75996,     0.75891,     0.75891,     0.75891,     0.75891,     0.75891,     0.75891,\n",
      "            0.75891,     0.75891,     0.75891,     0.75891,     0.75888,     0.75793,     0.75734,     0.75576,     0.75559,     0.75543,     0.75526,      0.7551,     0.75493,     0.75476,     0.75432,     0.75377,     0.75214,     0.75088,     0.74986,     0.74883,     0.74824,     0.74792,      0.7476,\n",
      "            0.74733,     0.74718,     0.74702,     0.74687,     0.74671,     0.74656,     0.74641,     0.74619,      0.7459,     0.74562,     0.74534,     0.74528,     0.74503,     0.74473,     0.74443,     0.74315,     0.74265,     0.74216,     0.73974,     0.73916,     0.73899,     0.73899,     0.73869,\n",
      "            0.73755,      0.7358,      0.7356,     0.73541,     0.73522,     0.73502,     0.73483,     0.73377,     0.73282,     0.73223,      0.7317,     0.73047,     0.73028,     0.73009,      0.7299,     0.72971,     0.72948,     0.72904,     0.72861,      0.7274,     0.72675,     0.72642,     0.72642,\n",
      "            0.72609,     0.72316,     0.72168,     0.72117,     0.71908,     0.71908,      0.7176,     0.71657,     0.71574,     0.71533,     0.71491,     0.71379,     0.71333,     0.71286,     0.71038,     0.70964,     0.70964,     0.70948,     0.70828,     0.70581,     0.70505,      0.7044,      0.7044,\n",
      "             0.7044,      0.7044,     0.70366,     0.70259,     0.70199,     0.70155,     0.70037,     0.70021,     0.69868,     0.69815,     0.69609,     0.69602,     0.69328,     0.69256,     0.69208,     0.69182,     0.69081,     0.69018,     0.68973,     0.68966,     0.68656,      0.6858,     0.68418,\n",
      "            0.68122,     0.68086,      0.6805,     0.67933,     0.67581,     0.67304,     0.67212,     0.67143,      0.6709,     0.67086,     0.67086,     0.66919,     0.66645,     0.66563,     0.66305,     0.66071,     0.65934,     0.65828,     0.65828,     0.65828,     0.65589,     0.65529,     0.65181,\n",
      "            0.64961,      0.6488,     0.64794,     0.64673,     0.64631,     0.64589,      0.6445,     0.64361,     0.64054,     0.63987,      0.6382,      0.6341,     0.63264,     0.63035,     0.62955,     0.62902,     0.62704,     0.62371,      0.6182,     0.61554,     0.61218,     0.61064,     0.60909,\n",
      "            0.60797,     0.60692,     0.60634,     0.60512,      0.6035,     0.60273,     0.60023,     0.59896,     0.59596,     0.59375,     0.59085,      0.5901,     0.58966,     0.58921,     0.58475,     0.58229,     0.58053,     0.58004,     0.57904,     0.57735,     0.57668,     0.57572,     0.57492,\n",
      "            0.57442,     0.57414,     0.57358,     0.57054,      0.5673,     0.56609,     0.56457,     0.56322,     0.56289,     0.55986,      0.5579,     0.55583,     0.55345,     0.55241,     0.55131,     0.55035,     0.54569,     0.54442,     0.54197,     0.54082,     0.54013,     0.53897,     0.53531,\n",
      "            0.53336,     0.53145,     0.52904,     0.52499,     0.52157,     0.51985,     0.51803,     0.51213,     0.50734,     0.50316,     0.50129,      0.4968,      0.4934,     0.48877,     0.48333,     0.48284,     0.47726,     0.47469,     0.47379,     0.46939,     0.46773,     0.46106,     0.45912,\n",
      "            0.45639,     0.45099,     0.44826,     0.44639,     0.44509,     0.44163,     0.43628,     0.43256,     0.42505,     0.42029,     0.41779,     0.41423,     0.41221,     0.40801,     0.40528,     0.40288,     0.40023,     0.39286,     0.39148,     0.38771,     0.38154,     0.37695,     0.37631,\n",
      "            0.37526,     0.37381,     0.37226,     0.37116,     0.36612,     0.35994,     0.35506,     0.34937,     0.34175,     0.33876,     0.33327,     0.32957,     0.32201,     0.31635,     0.31248,      0.3044,     0.29598,      0.2898,     0.28215,     0.27695,     0.27467,     0.26263,     0.25708,\n",
      "            0.24804,     0.24265,     0.23786,     0.23438,     0.22959,     0.22249,      0.2166,     0.21388,      0.2064,     0.19668,     0.18894,     0.18363,     0.17482,     0.16684,     0.16137,      0.1587,     0.15317,     0.14306,     0.13497,     0.13146,     0.12269,     0.11596,     0.11126,\n",
      "            0.10664,     0.10114,     0.09466,    0.090199,    0.082485,     0.07789,    0.071002,      0.0686,    0.064437,    0.057091,    0.053845,    0.047215,    0.041605,    0.040113,    0.034921,    0.033037,    0.028817,    0.026517,    0.022349,    0.018672,    0.017244,    0.014675,    0.012134,\n",
      "            0.00967,   0.0079668,   0.0071278,   0.0066726,   0.0052032,   0.0049588,   0.0047143,   0.0044698,   0.0042253,   0.0035977,   0.0029817,   0.0025022,   0.0020317,   0.0016104,   0.0011891,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
      "fitness: 1.3114398301596126\n",
      "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)', 'metrics/precision(M)', 'metrics/recall(M)', 'metrics/mAP50(M)', 'metrics/mAP50-95(M)']\n",
      "maps: array([     1.2637])\n",
      "names: {0: 'contaminated'}\n",
      "plot: True\n",
      "results_dict: {'metrics/precision(B)': 0.8934863417187102, 'metrics/recall(B)': 0.8333333333333334, 'metrics/mAP50(B)': 0.8810718489013599, 'metrics/mAP50-95(B)': 0.6862074103027365, 'metrics/precision(M)': 0.8837387569269474, 'metrics/recall(M)': 0.8238993710691824, 'metrics/mAP50(M)': 0.8599014115199177, 'metrics/mAP50-95(M)': 0.5775064831611354, 'fitness': 1.3114398301596126}\n",
      "save_dir: PosixPath('runs/segment/train102')\n",
      "seg: ultralytics.utils.metrics.Metric object\n",
      "speed: {'preprocess': 0.22123066278604359, 'inference': 2.0344647077413702, 'loss': 0.0005146631827721229, 'postprocess': 1.18101560152494}\n",
      "task: 'segment'\n"
     ]
    }
   ],
   "source": [
    "results = model.val(data=\"./data.yaml\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59855c0397ebdf83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T20:45:56.619711200Z",
     "start_time": "2024-12-05T20:45:56.612160500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def infer_image(image_path):\n",
    "    # Загрузка изображения\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Инференс\n",
    "    return model(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bafd993999b6432d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T20:51:52.367492200Z",
     "start_time": "2024-12-05T20:51:52.357938500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Функция для создания маски с черным фоном\n",
    "def create_mask(image_path, results):\n",
    "    # Загружаем изображение и переводим в градации серого\n",
    "    image = cv2.imread(image_path)\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Создаем пустую маску с черным фоном\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # Проходим по результатам и создаем маску\n",
    "    for result in results:\n",
    "        masks = result.masks  # Получаем маски из результатов\n",
    "        if masks is not None:\n",
    "            for mask_array in masks.data:  # Получаем маски как массивы\n",
    "                mask_i = mask_array.numpy()  # Преобразуем маску в numpy массив\n",
    "                \n",
    "                # Изменяем размер маски под размер оригинального изображения\n",
    "                mask_i_resized = cv2.resize(mask_i, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "                \n",
    "                # Накладываем маску на пустую маску (255 для белого)\n",
    "                mask[mask_i_resized > 0] = 255\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb1b125643cab2da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T20:52:49.360392200Z",
     "start_time": "2024-12-05T20:52:49.205486700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ 'source' is missing. Using 'source=/home/jovyan/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/ultralytics/assets'.\n",
      "\n",
      "image 1/2 /home/jovyan/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/ultralytics/assets/bus.jpg: 640x480 (no detections), 66.9ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@702.858] global loadsave.cpp:241 findDecoder imread_('./cv_test_dataset/test_img/_08.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 2/2 /home/jovyan/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/ultralytics/assets/zidane.jpg: 384x640 3 contaminateds, 70.6ms\n",
      "Speed: 2.1ms preprocess, 68.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@703.063] global loadsave.cpp:241 findDecoder imread_('./cv_test_dataset/test_img/_08.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m results \u001b[38;5;241m=\u001b[39m infer_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./cv_test_dataset/test_img/_08.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m mask_image \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./cv_test_dataset/test_img/_08.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Сохраняем маску в формате PNG\u001b[39;00m\n\u001b[1;32m      5\u001b[0m mask_output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./mask_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Укажите путь для сохранения маски\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m, in \u001b[0;36mcreate_mask\u001b[0;34m(image_path, results)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_mask\u001b[39m(image_path, results):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Загружаем изображение и переводим в градации серого\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m----> 5\u001b[0m     height, width \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Создаем пустую маску с черным фоном\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((height, width), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "results = infer_image(\"./cv_test_dataset/test_img/_08.jpg\")\n",
    "mask_image = create_mask(\"./cv_test_dataset/test_img/_08.jpg\", results)\n",
    "\n",
    "# Сохраняем маску в формате PNG\n",
    "mask_output_path = './mask_image.png'  # Укажите путь для сохранения маски\n",
    "cv2.imwrite(mask_output_path, mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "35bf1867b33cd589",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.save('/home/jovyan/nazar/123/123/misis_chill/nornikel_dockerfile/baseline4n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bba70711-a52b-448b-bc72-489ed9167a54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): SegmentationModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): C2PSA(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): Sequential(\n",
       "          (0): PSABlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): Conv(\n",
       "                (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (proj): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (pe): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (ffn): Sequential(\n",
       "              (0): Conv(\n",
       "                (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (12): Concat()\n",
       "      (13): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (15): Concat()\n",
       "      (16): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (18): Concat()\n",
       "      (19): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (21): Concat()\n",
       "      (22): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): Segment(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (proto): Proto(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (upsample): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (cv4): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(512, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_n = YOLO('/home/jovyan/nazar/123/123/misis_chill/nornikel_dockerfile/baseline4n.pt')\n",
    "model_s = YOLO('/home/jovyan/nazar/123/123/misis_chill/nornikel_dockerfile/baseline2.pt')\n",
    "model_n.to('cpu')\n",
    "model_s.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4eebee65-549d-4110-853e-4b80975855df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model16 = model_n.to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "14352518-a1a7-4262-a0f4-ae0b2fa78448",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 6.4258e+00, -2.4766e+00,  2.6973e+00],\n",
      "          [ 2.9336e+00, -1.9578e+01, -6.4404e-01],\n",
      "          [ 5.9531e+00, -1.8320e+00,  5.4219e+00]],\n",
      "\n",
      "         [[ 7.5898e+00, -3.4043e+00,  5.2656e+00],\n",
      "          [ 8.5645e-01, -2.4359e+01,  1.3779e+00],\n",
      "          [ 5.9336e+00, -1.3379e+00,  8.2656e+00]],\n",
      "\n",
      "         [[ 1.8193e+00, -3.2617e-01,  2.2734e+00],\n",
      "          [-3.2324e-01, -8.5391e+00,  1.2676e+00],\n",
      "          [ 7.6562e-01, -2.8418e-01,  2.8613e+00]]],\n",
      "\n",
      "\n",
      "        [[[-4.4141e+00, -5.3594e+00, -1.1846e+00],\n",
      "          [-1.2844e+01, -1.3219e+01, -4.8984e+00],\n",
      "          [-1.6219e+01, -1.1250e+01, -4.8008e+00]],\n",
      "\n",
      "         [[ 1.7080e+00,  6.2500e-01, -1.7188e+00],\n",
      "          [ 4.8398e+00,  2.9902e+00, -2.4585e-01],\n",
      "          [ 6.2305e+00,  2.3594e+00, -1.8860e-02]],\n",
      "\n",
      "         [[ 1.4053e+00,  3.7676e+00,  1.7725e+00],\n",
      "          [ 8.1875e+00,  1.0406e+01,  5.7461e+00],\n",
      "          [ 8.5000e+00,  7.1055e+00,  3.8672e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.9844e+01,  3.6113e+00,  2.1125e+01],\n",
      "          [-3.9781e+01,  3.5332e+00,  3.5156e+01],\n",
      "          [-2.4219e+01,  6.0498e-01,  2.0719e+01]],\n",
      "\n",
      "         [[ 8.9219e+00, -4.2891e+00, -4.3359e+00],\n",
      "          [ 1.3023e+01, -2.0234e+00, -1.0859e+01],\n",
      "          [ 6.3398e+00, -7.2705e-01, -6.2656e+00]],\n",
      "\n",
      "         [[ 1.3633e+01, -3.8613e+00, -1.5125e+01],\n",
      "          [ 2.9312e+01, -1.1162e+00, -2.7750e+01],\n",
      "          [ 1.6906e+01,  2.2852e+00, -1.5547e+01]]],\n",
      "\n",
      "\n",
      "        [[[ 7.3203e+00, -1.0439e+00, -5.7188e+00],\n",
      "          [ 1.3367e+01, -4.8047e-01, -1.4320e+01],\n",
      "          [ 8.3672e+00,  1.7249e-01, -7.6953e+00]],\n",
      "\n",
      "         [[ 6.6133e+00, -4.0479e-01, -5.4258e+00],\n",
      "          [ 1.3688e+01,  1.6998e-02, -1.5961e+01],\n",
      "          [ 6.8281e+00,  9.6875e-01, -7.0469e+00]],\n",
      "\n",
      "         [[ 9.4482e-01, -1.9348e-01, -3.3252e-01],\n",
      "          [ 3.5781e+00,  4.2017e-01, -5.5078e+00],\n",
      "          [ 8.4570e-01,  7.6270e-01, -1.3154e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8975e+00, -2.4258e+00, -3.9336e+00],\n",
      "          [ 1.6670e+00, -2.0918e+00, -2.4023e+00],\n",
      "          [-1.7761e-01,  1.4150e+00,  3.4355e+00]],\n",
      "\n",
      "         [[ 2.2598e+00,  1.0034e-01, -2.5117e+00],\n",
      "          [ 6.5381e-01, -4.3896e-01, -4.5581e-01],\n",
      "          [-2.4043e+00,  1.0762e+00,  6.0645e-01]],\n",
      "\n",
      "         [[-1.8867e+00,  8.0643e-03, -1.9873e-01],\n",
      "          [-6.0645e-01, -5.1562e-01, -1.0488e+00],\n",
      "          [-6.6846e-01, -1.7896e-01, -3.0176e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 5.1875e+00,  3.8281e+00,  3.1602e+00],\n",
      "          [ 6.5781e+00,  6.9688e+00,  6.1875e+00],\n",
      "          [ 4.3008e+00,  3.1621e+00,  4.5078e+00]],\n",
      "\n",
      "         [[-1.1922e+01, -1.3070e+01, -1.0906e+01],\n",
      "          [-1.6844e+01, -1.9781e+01, -1.7625e+01],\n",
      "          [-1.3117e+01, -1.3828e+01, -1.3328e+01]],\n",
      "\n",
      "         [[ 6.1680e+00,  8.7500e+00,  7.2852e+00],\n",
      "          [ 9.8047e+00,  1.3156e+01,  1.0227e+01],\n",
      "          [ 9.5156e+00,  1.0016e+01,  9.3203e+00]]],\n",
      "\n",
      "\n",
      "        [[[-3.5410e+00, -1.3418e+00,  5.2461e+00],\n",
      "          [-9.9453e+00, -1.7793e+00,  1.1727e+01],\n",
      "          [-4.7344e+00, -1.7500e+00,  6.1641e+00]],\n",
      "\n",
      "         [[-5.6211e+00, -1.6279e+00,  7.2070e+00],\n",
      "          [-1.4219e+01, -1.8633e+00,  1.5391e+01],\n",
      "          [-6.1016e+00, -1.4355e+00,  7.5625e+00]],\n",
      "\n",
      "         [[-2.0117e+00, -8.9502e-01,  2.9180e+00],\n",
      "          [-7.0898e+00, -6.3916e-01,  6.9570e+00],\n",
      "          [-3.3750e+00, -3.1274e-01,  3.9492e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 6.5820e+00,  1.1969e+01,  6.2734e+00],\n",
      "          [ 3.4375e-01,  3.2373e-01, -4.1870e-01],\n",
      "          [-6.8281e+00, -1.2297e+01, -5.9531e+00]],\n",
      "\n",
      "         [[ 6.3906e+00,  1.3391e+01,  6.9336e+00],\n",
      "          [ 3.1403e-02, -7.9248e-01, -1.0078e+00],\n",
      "          [-6.6602e+00, -1.3664e+01, -5.3516e+00]],\n",
      "\n",
      "         [[ 1.0088e+00,  4.4336e+00,  1.6943e+00],\n",
      "          [ 2.4573e-01,  3.4302e-01, -6.1572e-01],\n",
      "          [-1.7461e+00, -4.8477e+00, -1.0488e+00]]],\n",
      "\n",
      "\n",
      "        [[[-5.6094e+00, -1.1164e+01, -4.3906e+00],\n",
      "          [-1.3242e+00, -7.9932e-01, -2.8247e-01],\n",
      "          [ 6.8047e+00,  1.1445e+01,  5.4062e+00]],\n",
      "\n",
      "         [[-5.3086e+00, -1.2523e+01, -4.4219e+00],\n",
      "          [-7.0654e-01, -1.6113e-01,  8.4863e-01],\n",
      "          [ 6.1055e+00,  1.0758e+01,  4.5625e+00]],\n",
      "\n",
      "         [[-2.2148e+00, -6.2617e+00, -1.9199e+00],\n",
      "          [-9.6533e-01, -3.0981e-01,  5.1123e-01],\n",
      "          [ 2.6934e+00,  5.3438e+00,  2.4121e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 6.0742e+00, -9.0078e+00, -2.6738e+00],\n",
      "          [ 1.3109e+01, -4.0469e+00,  4.9062e+00],\n",
      "          [ 3.5957e+00, -8.5078e+00,  4.6094e-01]],\n",
      "\n",
      "         [[ 3.2617e+00, -1.1484e+01, -1.3513e-01],\n",
      "          [ 1.0133e+01, -5.0273e+00,  1.1039e+01],\n",
      "          [-9.9854e-01, -1.0359e+01,  4.0938e+00]],\n",
      "\n",
      "         [[-3.1006e-01, -3.4980e+00,  1.3887e+00],\n",
      "          [ 1.4463e+00, -1.9092e+00,  5.9727e+00],\n",
      "          [-2.3945e+00, -3.8340e+00,  2.1816e+00]]],\n",
      "\n",
      "\n",
      "        [[[-5.5000e+00,  1.3609e+01, -7.7461e+00],\n",
      "          [-8.3750e+00,  2.1125e+01, -1.3641e+01],\n",
      "          [-4.2031e+00,  1.1555e+01, -7.1719e+00]],\n",
      "\n",
      "         [[-9.4219e+00,  1.6438e+01, -7.0000e+00],\n",
      "          [-1.5180e+01,  2.5891e+01, -1.0648e+01],\n",
      "          [-7.3281e+00,  1.2383e+01, -5.3203e+00]],\n",
      "\n",
      "         [[-2.3516e+00,  4.1211e+00, -1.5283e+00],\n",
      "          [-6.7773e+00,  8.6797e+00, -2.5781e+00],\n",
      "          [-2.2852e+00,  3.4238e+00, -1.1104e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 2.6348e+00, -1.0008e+01, -1.7547e+01],\n",
      "          [ 5.5742e+00, -5.4453e+00, -9.6172e+00],\n",
      "          [ 6.5469e+00,  4.9170e-01,  8.2471e-01]],\n",
      "\n",
      "         [[ 8.0371e-01,  8.3984e+00,  1.1836e+01],\n",
      "          [-2.2871e+00,  3.8574e+00,  5.7969e+00],\n",
      "          [-2.1406e+00,  4.9390e-01,  1.5127e+00]],\n",
      "\n",
      "         [[-3.0703e+00,  2.0078e+00,  7.2109e+00],\n",
      "          [-3.6855e+00,  1.3516e+00,  2.8809e+00],\n",
      "          [-3.1465e+00,  6.3965e-01,  4.4604e-01]]],\n",
      "\n",
      "\n",
      "        [[[-7.7500e+00, -7.7969e+00, -3.8828e+00],\n",
      "          [-6.9727e+00, -6.1602e+00, -1.9277e+00],\n",
      "          [-6.6797e+00, -5.0156e+00, -2.6367e+00]],\n",
      "\n",
      "         [[ 1.5000e+01,  1.4219e+01,  1.5562e+01],\n",
      "          [ 1.2523e+01,  1.3562e+01,  1.2422e+01],\n",
      "          [ 9.4375e+00,  8.0312e+00,  8.8203e+00]],\n",
      "\n",
      "         [[-7.4219e+00, -7.6719e+00, -1.1055e+01],\n",
      "          [-5.1602e+00, -8.1484e+00, -9.8594e+00],\n",
      "          [-3.4941e+00, -4.7930e+00, -6.7227e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 3.8984e+00, -1.4131e+00,  5.1328e+00],\n",
      "          [-4.2422e+00, -2.5172e+01, -4.5547e+00],\n",
      "          [ 3.2363e+00, -5.4766e+00,  2.2832e+00]],\n",
      "\n",
      "         [[ 3.3418e+00,  2.0215e+00,  5.6875e+00],\n",
      "          [-3.0469e-01, -2.0891e+01, -4.1772e-01],\n",
      "          [ 4.1289e+00, -1.5889e+00,  4.6328e+00]],\n",
      "\n",
      "         [[-4.5264e-01, -2.7129e+00, -8.3191e-02],\n",
      "          [-1.8457e+00, -1.1242e+01, -2.7539e-01],\n",
      "          [ 7.9834e-01, -3.6426e+00,  2.2969e+00]]],\n",
      "\n",
      "\n",
      "        [[[-8.7812e+00,  3.0273e+00,  7.1211e+00],\n",
      "          [ 2.2773e+00, -6.5273e+00,  5.5713e-01],\n",
      "          [ 1.0344e+01,  1.3594e+00, -9.4453e+00]],\n",
      "\n",
      "         [[-1.3375e+01,  2.2148e+00,  1.3477e+01],\n",
      "          [ 6.0254e-01, -8.7812e+00,  2.1953e+00],\n",
      "          [ 1.5352e+01,  7.6953e-01, -1.2203e+01]],\n",
      "\n",
      "         [[-6.2070e+00,  3.2695e+00,  5.1641e+00],\n",
      "          [-1.6777e+00, -4.9961e+00,  5.2227e+00],\n",
      "          [ 4.7305e+00,  1.0242e-01, -5.4414e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 3.8340e+00,  4.4922e+00,  5.1992e+00],\n",
      "          [ 6.3867e+00,  9.5938e+00,  7.6094e+00],\n",
      "          [ 6.5000e+00,  6.7578e+00,  5.7891e+00]],\n",
      "\n",
      "         [[-1.2900e+00, -1.9551e+00, -2.4883e+00],\n",
      "          [-2.9707e+00, -4.9180e+00, -4.8555e+00],\n",
      "          [-2.1738e+00, -3.8652e+00, -3.5527e+00]],\n",
      "\n",
      "         [[-2.5371e+00, -1.5527e+00, -1.1992e+00],\n",
      "          [-3.7988e+00, -4.1836e+00, -3.0566e+00],\n",
      "          [-4.6406e+00, -3.5547e+00, -2.5703e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "for i in (model_n.parameters()):\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5176ba2c-1b22-4deb-9e28-207787056e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7f1e173f-d85c-4a75-8ab9-64db55321d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 6.4258e+00, -2.4766e+00,  2.6973e+00],\n",
      "          [ 2.9336e+00, -1.9578e+01, -6.4404e-01],\n",
      "          [ 5.9531e+00, -1.8320e+00,  5.4219e+00]],\n",
      "\n",
      "         [[ 7.5898e+00, -3.4043e+00,  5.2656e+00],\n",
      "          [ 8.5645e-01, -2.4359e+01,  1.3779e+00],\n",
      "          [ 5.9336e+00, -1.3379e+00,  8.2656e+00]],\n",
      "\n",
      "         [[ 1.8193e+00, -3.2617e-01,  2.2734e+00],\n",
      "          [-3.2324e-01, -8.5391e+00,  1.2676e+00],\n",
      "          [ 7.6562e-01, -2.8418e-01,  2.8613e+00]]],\n",
      "\n",
      "\n",
      "        [[[-4.4141e+00, -5.3594e+00, -1.1846e+00],\n",
      "          [-1.2844e+01, -1.3219e+01, -4.8984e+00],\n",
      "          [-1.6219e+01, -1.1250e+01, -4.8008e+00]],\n",
      "\n",
      "         [[ 1.7080e+00,  6.2500e-01, -1.7188e+00],\n",
      "          [ 4.8398e+00,  2.9902e+00, -2.4585e-01],\n",
      "          [ 6.2305e+00,  2.3594e+00, -1.8860e-02]],\n",
      "\n",
      "         [[ 1.4053e+00,  3.7676e+00,  1.7725e+00],\n",
      "          [ 8.1875e+00,  1.0406e+01,  5.7461e+00],\n",
      "          [ 8.5000e+00,  7.1055e+00,  3.8672e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.9844e+01,  3.6113e+00,  2.1125e+01],\n",
      "          [-3.9781e+01,  3.5332e+00,  3.5156e+01],\n",
      "          [-2.4219e+01,  6.0498e-01,  2.0719e+01]],\n",
      "\n",
      "         [[ 8.9219e+00, -4.2891e+00, -4.3359e+00],\n",
      "          [ 1.3023e+01, -2.0234e+00, -1.0859e+01],\n",
      "          [ 6.3398e+00, -7.2705e-01, -6.2656e+00]],\n",
      "\n",
      "         [[ 1.3633e+01, -3.8613e+00, -1.5125e+01],\n",
      "          [ 2.9312e+01, -1.1162e+00, -2.7750e+01],\n",
      "          [ 1.6906e+01,  2.2852e+00, -1.5547e+01]]],\n",
      "\n",
      "\n",
      "        [[[ 7.3203e+00, -1.0439e+00, -5.7188e+00],\n",
      "          [ 1.3367e+01, -4.8047e-01, -1.4320e+01],\n",
      "          [ 8.3672e+00,  1.7249e-01, -7.6953e+00]],\n",
      "\n",
      "         [[ 6.6133e+00, -4.0479e-01, -5.4258e+00],\n",
      "          [ 1.3688e+01,  1.6998e-02, -1.5961e+01],\n",
      "          [ 6.8281e+00,  9.6875e-01, -7.0469e+00]],\n",
      "\n",
      "         [[ 9.4482e-01, -1.9348e-01, -3.3252e-01],\n",
      "          [ 3.5781e+00,  4.2017e-01, -5.5078e+00],\n",
      "          [ 8.4570e-01,  7.6270e-01, -1.3154e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8975e+00, -2.4258e+00, -3.9336e+00],\n",
      "          [ 1.6670e+00, -2.0918e+00, -2.4023e+00],\n",
      "          [-1.7761e-01,  1.4150e+00,  3.4355e+00]],\n",
      "\n",
      "         [[ 2.2598e+00,  1.0034e-01, -2.5117e+00],\n",
      "          [ 6.5381e-01, -4.3896e-01, -4.5581e-01],\n",
      "          [-2.4043e+00,  1.0762e+00,  6.0645e-01]],\n",
      "\n",
      "         [[-1.8867e+00,  8.0643e-03, -1.9873e-01],\n",
      "          [-6.0645e-01, -5.1562e-01, -1.0488e+00],\n",
      "          [-6.6846e-01, -1.7896e-01, -3.0176e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 5.1875e+00,  3.8281e+00,  3.1602e+00],\n",
      "          [ 6.5781e+00,  6.9688e+00,  6.1875e+00],\n",
      "          [ 4.3008e+00,  3.1621e+00,  4.5078e+00]],\n",
      "\n",
      "         [[-1.1922e+01, -1.3070e+01, -1.0906e+01],\n",
      "          [-1.6844e+01, -1.9781e+01, -1.7625e+01],\n",
      "          [-1.3117e+01, -1.3828e+01, -1.3328e+01]],\n",
      "\n",
      "         [[ 6.1680e+00,  8.7500e+00,  7.2852e+00],\n",
      "          [ 9.8047e+00,  1.3156e+01,  1.0227e+01],\n",
      "          [ 9.5156e+00,  1.0016e+01,  9.3203e+00]]],\n",
      "\n",
      "\n",
      "        [[[-3.5410e+00, -1.3418e+00,  5.2461e+00],\n",
      "          [-9.9453e+00, -1.7793e+00,  1.1727e+01],\n",
      "          [-4.7344e+00, -1.7500e+00,  6.1641e+00]],\n",
      "\n",
      "         [[-5.6211e+00, -1.6279e+00,  7.2070e+00],\n",
      "          [-1.4219e+01, -1.8633e+00,  1.5391e+01],\n",
      "          [-6.1016e+00, -1.4355e+00,  7.5625e+00]],\n",
      "\n",
      "         [[-2.0117e+00, -8.9502e-01,  2.9180e+00],\n",
      "          [-7.0898e+00, -6.3916e-01,  6.9570e+00],\n",
      "          [-3.3750e+00, -3.1274e-01,  3.9492e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 6.5820e+00,  1.1969e+01,  6.2734e+00],\n",
      "          [ 3.4375e-01,  3.2373e-01, -4.1870e-01],\n",
      "          [-6.8281e+00, -1.2297e+01, -5.9531e+00]],\n",
      "\n",
      "         [[ 6.3906e+00,  1.3391e+01,  6.9336e+00],\n",
      "          [ 3.1403e-02, -7.9248e-01, -1.0078e+00],\n",
      "          [-6.6602e+00, -1.3664e+01, -5.3516e+00]],\n",
      "\n",
      "         [[ 1.0088e+00,  4.4336e+00,  1.6943e+00],\n",
      "          [ 2.4573e-01,  3.4302e-01, -6.1572e-01],\n",
      "          [-1.7461e+00, -4.8477e+00, -1.0488e+00]]],\n",
      "\n",
      "\n",
      "        [[[-5.6094e+00, -1.1164e+01, -4.3906e+00],\n",
      "          [-1.3242e+00, -7.9932e-01, -2.8247e-01],\n",
      "          [ 6.8047e+00,  1.1445e+01,  5.4062e+00]],\n",
      "\n",
      "         [[-5.3086e+00, -1.2523e+01, -4.4219e+00],\n",
      "          [-7.0654e-01, -1.6113e-01,  8.4863e-01],\n",
      "          [ 6.1055e+00,  1.0758e+01,  4.5625e+00]],\n",
      "\n",
      "         [[-2.2148e+00, -6.2617e+00, -1.9199e+00],\n",
      "          [-9.6533e-01, -3.0981e-01,  5.1123e-01],\n",
      "          [ 2.6934e+00,  5.3438e+00,  2.4121e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 6.0742e+00, -9.0078e+00, -2.6738e+00],\n",
      "          [ 1.3109e+01, -4.0469e+00,  4.9062e+00],\n",
      "          [ 3.5957e+00, -8.5078e+00,  4.6094e-01]],\n",
      "\n",
      "         [[ 3.2617e+00, -1.1484e+01, -1.3513e-01],\n",
      "          [ 1.0133e+01, -5.0273e+00,  1.1039e+01],\n",
      "          [-9.9854e-01, -1.0359e+01,  4.0938e+00]],\n",
      "\n",
      "         [[-3.1006e-01, -3.4980e+00,  1.3887e+00],\n",
      "          [ 1.4463e+00, -1.9092e+00,  5.9727e+00],\n",
      "          [-2.3945e+00, -3.8340e+00,  2.1816e+00]]],\n",
      "\n",
      "\n",
      "        [[[-5.5000e+00,  1.3609e+01, -7.7461e+00],\n",
      "          [-8.3750e+00,  2.1125e+01, -1.3641e+01],\n",
      "          [-4.2031e+00,  1.1555e+01, -7.1719e+00]],\n",
      "\n",
      "         [[-9.4219e+00,  1.6438e+01, -7.0000e+00],\n",
      "          [-1.5180e+01,  2.5891e+01, -1.0648e+01],\n",
      "          [-7.3281e+00,  1.2383e+01, -5.3203e+00]],\n",
      "\n",
      "         [[-2.3516e+00,  4.1211e+00, -1.5283e+00],\n",
      "          [-6.7773e+00,  8.6797e+00, -2.5781e+00],\n",
      "          [-2.2852e+00,  3.4238e+00, -1.1104e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 2.6348e+00, -1.0008e+01, -1.7547e+01],\n",
      "          [ 5.5742e+00, -5.4453e+00, -9.6172e+00],\n",
      "          [ 6.5469e+00,  4.9170e-01,  8.2471e-01]],\n",
      "\n",
      "         [[ 8.0371e-01,  8.3984e+00,  1.1836e+01],\n",
      "          [-2.2871e+00,  3.8574e+00,  5.7969e+00],\n",
      "          [-2.1406e+00,  4.9390e-01,  1.5127e+00]],\n",
      "\n",
      "         [[-3.0703e+00,  2.0078e+00,  7.2109e+00],\n",
      "          [-3.6855e+00,  1.3516e+00,  2.8809e+00],\n",
      "          [-3.1465e+00,  6.3965e-01,  4.4604e-01]]],\n",
      "\n",
      "\n",
      "        [[[-7.7500e+00, -7.7969e+00, -3.8828e+00],\n",
      "          [-6.9727e+00, -6.1602e+00, -1.9277e+00],\n",
      "          [-6.6797e+00, -5.0156e+00, -2.6367e+00]],\n",
      "\n",
      "         [[ 1.5000e+01,  1.4219e+01,  1.5562e+01],\n",
      "          [ 1.2523e+01,  1.3562e+01,  1.2422e+01],\n",
      "          [ 9.4375e+00,  8.0312e+00,  8.8203e+00]],\n",
      "\n",
      "         [[-7.4219e+00, -7.6719e+00, -1.1055e+01],\n",
      "          [-5.1602e+00, -8.1484e+00, -9.8594e+00],\n",
      "          [-3.4941e+00, -4.7930e+00, -6.7227e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 3.8984e+00, -1.4131e+00,  5.1328e+00],\n",
      "          [-4.2422e+00, -2.5172e+01, -4.5547e+00],\n",
      "          [ 3.2363e+00, -5.4766e+00,  2.2832e+00]],\n",
      "\n",
      "         [[ 3.3418e+00,  2.0215e+00,  5.6875e+00],\n",
      "          [-3.0469e-01, -2.0891e+01, -4.1772e-01],\n",
      "          [ 4.1289e+00, -1.5889e+00,  4.6328e+00]],\n",
      "\n",
      "         [[-4.5264e-01, -2.7129e+00, -8.3191e-02],\n",
      "          [-1.8457e+00, -1.1242e+01, -2.7539e-01],\n",
      "          [ 7.9834e-01, -3.6426e+00,  2.2969e+00]]],\n",
      "\n",
      "\n",
      "        [[[-8.7812e+00,  3.0273e+00,  7.1211e+00],\n",
      "          [ 2.2773e+00, -6.5273e+00,  5.5713e-01],\n",
      "          [ 1.0344e+01,  1.3594e+00, -9.4453e+00]],\n",
      "\n",
      "         [[-1.3375e+01,  2.2148e+00,  1.3477e+01],\n",
      "          [ 6.0254e-01, -8.7812e+00,  2.1953e+00],\n",
      "          [ 1.5352e+01,  7.6953e-01, -1.2203e+01]],\n",
      "\n",
      "         [[-6.2070e+00,  3.2695e+00,  5.1641e+00],\n",
      "          [-1.6777e+00, -4.9961e+00,  5.2227e+00],\n",
      "          [ 4.7305e+00,  1.0242e-01, -5.4414e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 3.8340e+00,  4.4922e+00,  5.1992e+00],\n",
      "          [ 6.3867e+00,  9.5938e+00,  7.6094e+00],\n",
      "          [ 6.5000e+00,  6.7578e+00,  5.7891e+00]],\n",
      "\n",
      "         [[-1.2900e+00, -1.9551e+00, -2.4883e+00],\n",
      "          [-2.9707e+00, -4.9180e+00, -4.8555e+00],\n",
      "          [-2.1738e+00, -3.8652e+00, -3.5527e+00]],\n",
      "\n",
      "         [[-2.5371e+00, -1.5527e+00, -1.1992e+00],\n",
      "          [-3.7988e+00, -4.1836e+00, -3.0566e+00],\n",
      "          [-4.6406e+00, -3.5547e+00, -2.5703e+00]]]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "for i in model.to(torch.float16).parameters():\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "69ced134-3c27-4859-93f5-a9cf3ea4ccaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): SegmentationModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): C2PSA(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): Sequential(\n",
       "          (0): PSABlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): Conv(\n",
       "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (proj): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (pe): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (ffn): Sequential(\n",
       "              (0): Conv(\n",
       "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (12): Concat()\n",
       "      (13): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (15): Concat()\n",
       "      (16): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (18): Concat()\n",
       "      (19): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (21): Concat()\n",
       "      (22): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): Segment(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (proto): Proto(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (upsample): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (cv4): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model16.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cf4e43ce-f953-4be7-90f9-b2aa7b163830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): SegmentationModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): C2PSA(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): Sequential(\n",
       "          (0): PSABlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): Conv(\n",
       "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (proj): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (pe): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (ffn): Sequential(\n",
       "              (0): Conv(\n",
       "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (12): Concat()\n",
       "      (13): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (15): Concat()\n",
       "      (16): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (18): Concat()\n",
       "      (19): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (21): Concat()\n",
       "      (22): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): Segment(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (proto): Proto(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (upsample): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (cv4): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_n.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a14c648e-7800-4bff-997e-d99733fb4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "model16 = model.to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7b3068af-02d5-4189-9235-2705378e5314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): SegmentationModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): C2PSA(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): Sequential(\n",
       "          (0): PSABlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): Conv(\n",
       "                (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (proj): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (pe): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (ffn): Sequential(\n",
       "              (0): Conv(\n",
       "                (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (12): Concat()\n",
       "      (13): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (15): Concat()\n",
       "      (16): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (18): Concat()\n",
       "      (19): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (21): Concat()\n",
       "      (22): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): Segment(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (proto): Proto(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (upsample): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (cv4): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(512, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5ce39f9c-5286-4e65-bef9-ce246a522bf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): SegmentationModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): C2PSA(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): Sequential(\n",
       "          (0): PSABlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): Conv(\n",
       "                (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (proj): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (pe): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (ffn): Sequential(\n",
       "              (0): Conv(\n",
       "                (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (12): Concat()\n",
       "      (13): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (15): Concat()\n",
       "      (16): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (18): Concat()\n",
       "      (19): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (21): Concat()\n",
       "      (22): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): Segment(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (proto): Proto(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (upsample): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (cv4): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(512, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model16.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b57e36-6d58-4458-8342-a1dc565a389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "98ee4f2d-5a9c-4323-9068-ff0d5b46ed9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected m1 and m2 to have the same dtype, but got: c10::Half != float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/images/train/1710275253_0.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/ultralytics/engine/model.py:180\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    153\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    154\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/ultralytics/engine/model.py:551\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor:\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor \u001b[38;5;241m=\u001b[39m (predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictor\u001b[39m\u001b[38;5;124m\"\u001b[39m))(overrides\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[0;32m--> 551\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_cli\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# only update args if predictor is already setup\u001b[39;00m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m get_cfg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs, args)\n",
      "File \u001b[0;32m~/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/ultralytics/engine/predictor.py:308\u001b[0m, in \u001b[0;36mBasePredictor.setup_model\u001b[0;34m(self, model, verbose)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    307\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize YOLO model with given parameters and set it to evaluation mode.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoBackend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice  \u001b[38;5;66;03m# update device\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfp16  \u001b[38;5;66;03m# update half\u001b[39;00m\n",
      "File \u001b[0;32m~/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/ultralytics/nn/autobackend.py:148\u001b[0m, in \u001b[0;36mAutoBackend.__init__\u001b[0;34m(self, weights, device, dnn, data, fp16, batch, fuse, verbose)\u001b[0m\n\u001b[1;32m    146\u001b[0m model \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fuse:\n\u001b[0;32m--> 148\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkpt_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    150\u001b[0m     kpt_shape \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mkpt_shape  \u001b[38;5;66;03m# pose-only\u001b[39;00m\n",
      "File \u001b[0;32m~/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/ultralytics/nn/tasks.py:207\u001b[0m, in \u001b[0;36mBaseModel.fuse\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, Conv2):\n\u001b[1;32m    206\u001b[0m     m\u001b[38;5;241m.\u001b[39mfuse_convs()\n\u001b[0;32m--> 207\u001b[0m m\u001b[38;5;241m.\u001b[39mconv \u001b[38;5;241m=\u001b[39m \u001b[43mfuse_conv_and_bn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# update conv\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mdelattr\u001b[39m(m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbn\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# remove batchnorm\u001b[39;00m\n\u001b[1;32m    209\u001b[0m m\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mforward_fuse  \u001b[38;5;66;03m# update forward\u001b[39;00m\n",
      "File \u001b[0;32m~/.mlspace/envs/nmkarpov/lib/python3.10/site-packages/ultralytics/utils/torch_utils.py:267\u001b[0m, in \u001b[0;36mfuse_conv_and_bn\u001b[0;34m(conv, bn)\u001b[0m\n\u001b[1;32m    265\u001b[0m b_conv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(conv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39mconv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mif\u001b[39;00m conv\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m conv\u001b[38;5;241m.\u001b[39mbias\n\u001b[1;32m    266\u001b[0m b_bn \u001b[38;5;241m=\u001b[39m bn\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m-\u001b[39m bn\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mmul(bn\u001b[38;5;241m.\u001b[39mrunning_mean)\u001b[38;5;241m.\u001b[39mdiv(torch\u001b[38;5;241m.\u001b[39msqrt(bn\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;241m+\u001b[39m bn\u001b[38;5;241m.\u001b[39meps))\n\u001b[0;32m--> 267\u001b[0m fusedconv\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mcopy_(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw_bn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_conv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m b_bn)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fusedconv\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected m1 and m2 to have the same dtype, but got: c10::Half != float"
     ]
    }
   ],
   "source": [
    "model('/home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/images/train/1710275253_0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "123810f7-15c4-4b29-8b1d-f51430236484",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('/home/jovyan/nazar/123/123/misis_chill/train_dataset/cv_open_dataset/open_img/1710275253_0.jpg')\n",
    "image = add_sobel_as_fourth_channel(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "bf63e37e-bef8-4b46-82e1-d40b0793328f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1920, 4)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "08d14b06-9946-4828-8445-5d44ded0e3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): SegmentationModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): C2PSA(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): Sequential(\n",
       "          (0): PSABlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): Conv(\n",
       "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (proj): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (pe): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (ffn): Sequential(\n",
       "              (0): Conv(\n",
       "                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (12): Concat()\n",
       "      (13): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (15): Concat()\n",
       "      (16): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (18): Concat()\n",
       "      (19): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (21): Concat()\n",
       "      (22): C3k2(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): C3k(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv3): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (m): Sequential(\n",
       "              (0): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): Bottleneck(\n",
       "                (cv1): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "                (cv2): Conv(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (act): SiLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): Segment(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): DWConv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (proto): Proto(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (upsample): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (cv4): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3f1461eb-95d2-4ccc-830e-6db092d0a6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/images/train/1710275253_0.jpg: 384x640 (no detections), 225.3ms\n",
      "Speed: 2.4ms preprocess, 225.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       " obb: None\n",
       " orig_img: array([[[ 21,  24,  32],\n",
       "         [ 21,  24,  32],\n",
       "         [ 22,  24,  32],\n",
       "         ...,\n",
       "         [ 94,  82,  64],\n",
       "         [ 91,  79,  61],\n",
       "         [ 97,  85,  67]],\n",
       " \n",
       "        [[ 21,  24,  32],\n",
       "         [ 21,  24,  32],\n",
       "         [ 22,  24,  32],\n",
       "         ...,\n",
       "         [ 96,  84,  66],\n",
       "         [ 92,  80,  62],\n",
       "         [ 93,  81,  63]],\n",
       " \n",
       "        [[ 22,  24,  32],\n",
       "         [ 22,  24,  32],\n",
       "         [ 22,  24,  32],\n",
       "         ...,\n",
       "         [ 97,  85,  67],\n",
       "         [ 94,  82,  64],\n",
       "         [ 95,  83,  65]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[140, 123, 102],\n",
       "         [139, 123, 100],\n",
       "         [133, 116,  95],\n",
       "         ...,\n",
       "         [ 67,  65,  47],\n",
       "         [ 67,  64,  49],\n",
       "         [ 67,  65,  47]],\n",
       " \n",
       "        [[137, 122, 103],\n",
       "         [136, 122, 100],\n",
       "         [131, 116,  97],\n",
       "         ...,\n",
       "         [ 56,  56,  40],\n",
       "         [ 58,  55,  41],\n",
       "         [ 58,  55,  40]],\n",
       " \n",
       "        [[137, 122, 103],\n",
       "         [137, 122, 103],\n",
       "         [141, 127, 108],\n",
       "         ...,\n",
       "         [ 51,  50,  36],\n",
       "         [ 53,  50,  36],\n",
       "         [ 53,  50,  36]]], dtype=uint8)\n",
       " orig_shape: (1080, 1920)\n",
       " path: '/home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/images/train/1710275253_0.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs/segment/train62'\n",
       " speed: {'preprocess': 2.419710159301758, 'inference': 225.2960205078125, 'postprocess': 0.4367828369140625}]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model16('/home/jovyan/nazar/123/123/misis_chill/train_dataset/baseline/datasets/train_data/images/train/1710275253_0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc25c72-b65b-408a-b387-f148d92918a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-nmkarpov]",
   "language": "python",
   "name": "conda-env-.mlspace-nmkarpov-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
